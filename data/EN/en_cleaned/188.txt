Speech synthesis (text-to-speech, TT
S) is a process of transforming the character sequence of any text to a sequence of speech samples . There are several approaches to doing this. The basic approaches are the following: rule-based speech synthesis (formant synthesis), articulatory speech synthesis, concatenative speech synthesis, and speech synthesis based on statistical models .
Currently the most popular approaches are the following: the Unit Selection algorithm (speech element selection) and statistical models (HM
M TT
S). The first one makes it possible to synthesize speech with maximum naturalness, given an accurately segmented voice database of a large size (10 hours and more). On the other hand, the second approach, which produces synthesized speech that is less natural, has the advantages presented below.
1. The HM
M-based method provides an easy way to modify voice characteristics
by using speaker adaptation/interpolation techniques. The Unit Selection algorithm generates speech with a constant style that is the same as the style of the speech in the database.
2. Speech generated by the HM
M method is less natural for listeners. However,
it is smoother, without detectable phone boundaries (pitch or energy leaps) which are usual for concatenative synthesis. In addition, the quality of Unit Selection TT
S can be strongly reduced when some of the necessary speech elements are absent in the database. When voice models are used, absent speech elements are synthesized based on mean values which are closest to the required ones. It is possible due to tree-based context clustering, and the method provides good intelligibility when the amount of contexts is insufficient.
3. Applying the HM
M-based speech synthesis method makes it possible to create
a new TT
S voice in much less time and to reduce the memory size required for storing the voice data.
We propose a hybrid TT
S system that combines both approaches: looking for a matching sequence of speech elements in the speaker’s speech database by means of the classic Unit selection algorithm, and employing a statistical intonation model which was trained on the same database. Experiments show that the naturalness of synthesized speech is increased compared to systems based only on Unit Selection or hidden Markov models.
2. System description
Structurally, the system is divided in two parts (
Figure 1): the training part (the preparation stage) and the synthesis part. A speech database is created based on the speech corpus containing a set of sound files (each file contains a single recorded sentence) and a set of corresponding label files (these contain information about the speech elements in each sound file) . Then the speech database is indexed to provide fast search for target elements by the following features: phone name, names of phones before and after the current phone, mel-frequency cepstral coefficients (MFC
C) at phone boundaries, energy, pitch, and phone duration.
Chistikov P. G., Korolkov E. A., Talanov A. O.

Voice model
Database of speech elements
Acoustic feature extraction
Linguistic and prosodic feature extraction
Speech element database creationHM
Ms training
Training partText
Linguistic and prosodic feature extraction
Acoustic feature extraction Looking for matching speech element sequence Combining speech elementsSpeech
Synthesis partHM
M Unit Selection
Labelsby the speech synthesis engine224
The procedure of voice parameter modeling begins with the extraction of the feature set for all sound files . Each member of the set represents a short part of the signal (frame) with the length of 25 ms. The features contain the following parameters:•	 Sequence {
C1, …, C
K} of MFC
C vectors , where each vector consists of 25 coefficients and characterizes the spectrum envelope of the signal for the frame; K is the total number of frames.
•	 Sequence {
F01, …, F0
K} of pitch values.
Combining HM
M and unit selection technologies After that, linguistic and prosodic features for each allophone of all the sentences of the training database are calculated. The description of the linguistic and prosodic features is presented in In the next step, the HM
M prototypes for each allophone are created. Each HM
M corresponds to a no-skip N-state left-to-right model with N = 5. Each output observation vector io1 2 3 4i i
T i
T i
T i
T To o o o o=
Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,.
Allophone features
Phone before previous Phone after next
Previous phone Phone position from the beginning of the syllable
Current phone Phone position from the end of the syllable
Next phone
Syllable features
Previous syllable Syllable position from the end of the word
Current syllable Syllable position from the beginning of the sentence
Next syllable Syllable position from the end of the sentence
Number of phones in the previous syllable Number of stressed syllables before current syllable in the sentence
Number of phones in the current syllable Number of stressed syllables after current syllable in the sentence
Number of phones in the next syllable Vowel name in the current syllable
Syllable position from the beginning of the word
Word features
Part of speech of the previous word Number of syllables in the current word
Part of speech of the current word Number of syllables in the next word
Part of speech of the next word Word position from the beginning of the sentence
Number of syllables in the previous word Word position from the end of the sentence
Sentence features
Number of syllables in the current sentence End punctuation type (comma, full stop, etc.)
Number of words in the current sentence
Chistikov P. G., Korolkov E. A., Talanov A. O.
the voice model building, a tree-based clustering technique is applied to the HM
M-states of MFC
C and their delta and delta-delta components, F0 values and their delta and delta-delta components, as well as to the state duration models. In the end of the process, 4
N + 1 different acoustic decision trees are generated: N trees for MFC
C and their delta and delta-delta components, 3
N trees for F0 features, and one tree for state duration (
Figure 3). Performing this stage makes it possible to generate speech parameters for elements absent in the database, which provides intelligible output even under conditions of insufficient training data.
S1 S5...
...
Decision Tree for State Duration Model
State Duration ModelHM
M for Spectrum and Pitch
Decision Trees for MFC
C, ΔMFC
C and Δ2MFCC
Decision Trees for F0 ...
...
...
...
...
...
...
...
...
Decision Trees for ΔF0
Decision Trees for Δ2F0
Text-to-speech system input is a raw text without any manual preprocessing. Based on the input text, the target allophone sequence is formed, and linguistic and prosodic features are calculated for each allophone. The type and structure of features are the same as those used at the stage of the speech database building. Using this information and the voice model, acoustic features are calculated for each allophone: MFC
C, pitch, energy and duration. Then the most appropriate speech elements are selected from the database, based on the calculated acoustic features. Special metrics (target cost and concatenation cost) are used to estimate the suitability of each selected allophone .
Target cost estimation is given in equation (1):io1 2 3 4i i
T i
T i
T i
T To o o o o=
Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,i i
T i
T i
T i
T To o o o o=Tk
Nkk ddd ],...,[ 1=knd( ) ( )1
pt t ti i k k i ik=
C u ,t = w C u ,t∑iuittk
Ctkw( ) ( )1 12
qc c ci i k k i ik=
C u ,u = w C u ,u− −∑1iu −
iuck
Cckw( ) ( ) ( )11 2
n nt ci i i ii= i=
C u,t = C u ,t C u ,u−+∑ ∑ (3).
The purpose of the Unit Selection algorithm is to select a sequence of elements that minimizes the final cost equation (3).
In the final step, the selected sequence of elements is concatenated to form the speech signal which is the result of TT
S system work.
3. Experiments
Figures 4–6 present the results of the system’s work. They are oscillograms, spectrograms, and pitch envelopes for the utterance “это очень важно!” (“eto očen’ važno”, Russian for “it is very important!”). A natural phrase is at the top of each figure, and its synthesized equivalent is at the bottom. It should be mentioned that this phrase had been excluded from the training data set.
(“it is very important”) (top) and its synthesized version (bottom)
Chistikov P. G., Korolkov E. A., Talanov A. O.
is very important”) (top) and its synthesized version (bottom)(“it is very important”) (top) and its synthesized version (bottom)
From the figures above you can notice that the synthesized utterance has almost the same tempo and spectrum characteristics as the natural equivalent uttered by a real speaker. It is due to the modeling of parameters based on hidden Markov models.
We conducted a MO
S (mean opinion score) evaluation to estimate the naturalness of the synthesized speech. Table 2 presents the results of the comparison for two systems: the proposed hybrid system and the system based on Unit Selection only. The comparison was performed by five experts for two voices (one male and one female); the results in the table have been averaged. The values ranged from 0 (unnatural, “mechanical” speech) to 5 (completely natural speech). The synthesized sentences were also compared to the same sentences pronounced by the speaker (they were not included in the training data set). The results show that the hybrid TT
S approach increases the naturalness of synthesized speech.
Type of TTS
Natural speech
Unit Selection Hybrid4,0 4,3 4,8
Combining HM
M and unit selection technologies 4. Conclusions
This paper describes an approach for building a Russian TT
S system based on the integration of hidden Markov models and Unit Selection. The TT
S engine is based on a method where the speech parameters are obtained from HM
Ms whose observation vectors consist of MFC
C, pitch and duration features; the speech signal is generated by a Unit Selection algorithm using the obtained speech parameters. We developed a voice model creation method for constructing a natural intonation contour. The experimental results confirm the improved quality of synthesized speech. It is also worth noting that the final speech quality can be improved by tuning Unit Selection weights and optimizing the training feature set.
