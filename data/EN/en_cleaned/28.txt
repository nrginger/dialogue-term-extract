With the rapid development of A
I technologies, a growing number of methods the ability to generate realistic artifacts. For instance, amount of texts generated by recent text generation methods the transformer*
These authors contribute equally to this work.
Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference â€œ
Dialogue 2022â€
Moscow, June 15â€“18, 2022encoder-decoder framework are very close to the text written by humans, including lots of security issues(de Rosa and Papa, 2021; Topal et al., 2021). Extensive Transformer-based text generation models, suchas Bert-stytle (
Devlin et al., 2018a), GP
T-stytle (
Radford et al., 2019), have achieved excellent resultson a large number of NL
P tasks. (
Keskar et al., 2019) proposed the conditional transformer languagemodel (CTR
L) with 1.63 billion parameters to control the text generation. The model is trained withdifferent codes that control task-specific behavior, entities, specify style and content. (
Zellers et al.,2019) introduced a controllable text generation model named Grover, which can overwrite propaganda
papers. For example, given a headline "
Discovering the link between vaccines and autism," Grover couldgenerate a description article for this title. Humans rated this generated text as more trustworthy thanhuman-written text. However, the success of natural language generation has drawn dual-use concerns.
On the one hand, applications such as summary generation and machine translation are positive. Onthe other hand, related techniques may also enable adversaries to generate neural fake news, targetedpropaganda and even fake political content. Therefore, several researchers have made many attemptsto develop artificial text detectors (
Jawahar et al., 2020). (
Solaiman et al., 2019) used the pre-trainedlanguage model RoBER
Ta for the downstream text detection task and achieved the best performancein recognizing web pages generated by the GP
T-2 model. (
Kushnareva et al., 2021) proposed a novelmethod based on Topological Data Analysis (TD
A). The interpretable topological features that can bederived from the attention map of any transformer-based language model are introduced for the task ofartificial text detection. (
Shamardina et al., 2022) originated two tracks on the RuAT
D 2022 Dialogue
Shared task to solve the problem of automatic recognition of generated texts. In this paper, we adopt DeBER
Ta method with multiple training strategies for the Russian artificial text detection in the Dialogueshared task 2022 (Multi
Class). More details about our system are introduced in the following sections.
2 Main Method
This section will elaborate on the main method for the Russian artificial text detection dialogue sharedtask, where we adopt the pre-trained model with multiple training strategies, such as adversarial training,child tuning, and intrust loss function.
CategoryLabelargmaxtt1t2tN-1T
Ex1
x2
xN
Ex1
x2
x
NĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°, ÑÑ€Ğ¾ĞºĞ¸ Ğ¸ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸DeBER
Taâ€¦ â€¦â€¦ â€¦â€¦ â€¦â€¦ â€¦M2
M
-100
Averaged Poolingâ€¦ â€¦
Fully ConnectedLayerï¼ˆ
With Dropoutï¼‰PoolingLayer
Encoder Context
Input Final Predicton2.1 Overview of Pre-trained Model
It is noted that the pre-trained model has a solid ability to differentiate the results generated by differentmodels (
Qiu et al., 2020; Naseem et al., 2021), we resort to the state-of-the-art (SOT
A) pre-trained
Li B., Weng Y., Song Q., Deng H.language model for better prediction. As shown in Figure 1, we present the main model architecturefor the shared task. Specifically, we adopt the DeBER
Ta family, i.e., mDeBER
Ta (
He et al., 2020) andDeBER
Ta (
He et al., 2021a), for the category classification. The pooling and fully connected layersare at the top of the pre-trained language model for leveraging global semantics. Finally, the argmax isperformed after the 14 categories classification to obtain the final results.
2.2 Multiple Training Strategies
2.2.1 Adversarial Training
The common method in adversarial training is the Fast Gradient Method (
Nesterov, 2013; Dong et al.,2018). The idea of FG
M is straightforward. Increasing the loss is to increase the gradient so that we can
takeâˆ†ğ‘¥ğ‘¥ = ğœ–ğœ–âˆ‡ğ‘¥ğ‘¥ğ¿ğ¿(ğ‘¥ğ‘¥ğ‘¥ ğ‘¥ğ‘¥; ğœƒğœƒ) (1)where ğ‘¥ğ‘¥ represents the input, ğ‘¥ğ‘¥ represents the label, ğœƒğœƒ is the model parameter, ğ¿ğ¿(ğ‘¥ğ‘¥ğ‘¥ ğ‘¥ğ‘¥; ğœƒğœƒ) is the loss of asingle sample, âˆ†ğ‘¥ğ‘¥ is the anti-disturbance.
Of course, to prevent âˆ†ğ‘¥ğ‘¥ from being too large, it is usually necessary to standardize âˆ‡ğ‘¥ğ‘¥ğ¿ğ¿(ğ‘¥ğ‘¥ğ‘¥ ğ‘¥ğ‘¥; ğœƒğœƒ).
The more common way isâˆ†ğ‘¥ğ‘¥ = ğœ–ğœ–âˆ‡ğ‘¥ğ‘¥ğ¿ğ¿(ğ‘¥ğ‘¥ğ‘¥ ğ‘¥ğ‘¥; ğœƒğœƒ)â€–âˆ‡ğ‘¥ğ‘¥ğ¿ğ¿(ğ‘¥ğ‘¥ğ‘¥ ğ‘¥ğ‘¥; ğœƒğœƒ)â€–(2)2.2.2 Child-tuning Training
The efficient Child-tuning (
Xu et al., 2021) method is used to fine-tuning the backbone model in ourmethod, where the parameters of the Child network are updated with the gradients mask. For this sharedtask, the task-independent algorithm is used for child-tuning. When fine-tuning, the gradient masks areobtained by Bernoulli Distribution (
Chen and Liu, 1997) sampling from in each step of iterative update,which is equivalent to randomly dividing a part of the network parameters when updating. The equationof the above steps is shown as followswğ‘¡ğ‘¡+1 = wğ‘¡ğ‘¡ âˆ’ ğœ‚ğœ‚ğœ•ğœ•â„’ (wğ‘¡ğ‘¡)ğœ•ğœ•wğ‘¡ğ‘¡âŠ™ğµğµğ‘¡ğ‘¡
Bğ‘¡ğ‘¡ âˆ¼ Bernoulli (ğ‘ğ‘ğ¹ğ¹ )(3)where the notation âŠ™ represents the dot production, ğ‘ğ‘ğ¹ğ¹ is the partial network parameter.
2.2.3 Ensemble Method
Once obtaining the pre-trained model, we need to maximize the advantages of each model. So, weuse ensemble each model with the Bagging Algorithm (
Skurichina and Duin, 2002) via voting on thepredicted results of the trained models. The Bagging algorithm is used during the prediction, wherethis method can effectively reduce the variance of the final prediction by bridging the prediction bias ofdifferent models, enhancing the overall generalization ability of the system.
3 Experiment
We will introduce the RuAT
D dataset, evaluation indicators, implementation details and method description.
3.1 RuAT
D
However, some people may use these models with malicious intent to generate false news, automaticproduct reviews, and false political content. RuAT
D 2022 proposes a new task, which requires judgingwhether a sentence is generated by the model (binary classification) or even which model it is generatedby (multi-class classification). More task details can be found in the website1.
1https://www.kaggle.com/c/ruatd-2022-multi-task
Artificial Text Detection with Multiple Training Strategies
Model FG
M Child
Tune In-trust loss BackboneDeBER
Ta-large 61.27 61.23 61.54 61.42mDeBER
Ta-base 61.89 61.68 62.21 62.06
Methods Accuracy
Random sample 19.927
Tf-idf 44.280BER
T fine-tuning 59.813
Ours 64.7313.2 Evaluation
In the multi-classification task, accuracy is used as the evaluation index. The task requires the model tojudge whether a sample is written by humans or generated by other generation models.
ğ´ğ´ğ´ğ´ğ´ğ´ =(ï¸€ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ´ğ´ğ´ğ´ğ´ğ´)3.3 Baseline Introduction
Tf-idf: With the help of sklearn(
Pedregosa et al., 2018), the organizers connect TF-ID
F, SV
D, standardscaler and logistic regression in turn for training.
BER
T Fine-tuning: BERT(
Devlin et al., 2018b) is a model designed for natural language understanding task. It uses ML
M pre-training method and has strong semantic feature understanding ability.
Theorganizer added a 14 category linear layer after the output layer of BER
T-base, and Cross
Entropy lossis used to fit.
3.4 Implementation Details
We train the model using the Pytorch 2 (
Paszke et al., 2019) on the NVIDI
A RT
X3090 GP
U and use thehugging-face3 (
Wolf et al., 2020) framework. For all uninitialized layers, We set the dimension of allthe hidden layers in the model as 1024. The AdamW(
Loshchilov and Hutter, 2018) optimizer which isa fixed version of Adam (
Kingma and Ba, 2014) with weight decay, and set ğ›½ğ›½1 to 0.9, ğ›½ğ›½2 to 0.99 for theoptimizer. We set the learning rate to 1ğ‘’ğ‘’ âˆ’ 5 with the warm-up (
He et al., 2016). The batch size is 32.
We set the maximum length of 280, and delete the excess. Linear decay of learning rate and gradientclipping is set to 1ğ‘’ğ‘’ âˆ’ 4. Dropout (
Srivastava et al., 2014) of 0.1 is applied to prevent over-fitting. Allexperiments select the best parameters in the valid set. Finally, we report the score of the best model(valid set) in the test set.
We use the mDeBER
Ta-base (
He et al., 2021c; He et al., 2021b) as our pretrained model, and fine-tunethe model 4. The mDeBER
Ta5 model comes with 12 layers and a hidden size of 768. And it was trainedwith the C
C100 (
Conneau et al., 2020) multilingual data .
4 Case Study
We counted and analyzed the mispredicted samples, and the distribution of error types is shown in Figures2. We chose the top 100 samples with the most significant difference from the ground truth as the analysis
2https://pytorch.org
3https://github.com/huggingface/transformers
4
You can reproduce the baseline code from here https://github.com/dialogue-evaluation/RuAT
D/blob/main/
Baseline.ipynb5microsoft/mdeberta-v3-base
Li B., Weng Y., Song Q., Deng H.44%13%
10%
7%
6%
6%
5%
3%
2%
2% 2% 0.2%
HumanM2M-100OPUS-MTruGPT3-LargeM-BART50ruGPT3-MediumruGPT3-smallmT5-LargeruT5-BaseM-BARTruGPT2-LargeruT5
Largeobject. As we can see from Figure 2, the most mispredicted type in the classification task was â€œ
Human",with 44%, followed byâ€œM2
M-100" with 13%, etc, the â€œruT5
Largeâ€ obtains the least error with 0.2%.
Further conclusions can be that sorting from high to low actually shows the capability performance of themodel. The higher the error rate, the better the performance of the model, and the effects like M2M/GP
T3are better. Then the bigger the model, the harder the target is to distinguish.
5 Result and Discussion
As shown in Table 1, we implement the DeBER
Ta-large and mDeBER
Ta-large with multiple trainingstrategies. It can be further concluded that the in trust loss method with the pre-trained model can achievethe best results in artificial text detection. It may be the reason that the model is trained through In-trusttraining. can be more robust. Moreover, we found that the mDeBER
Ta outperforms the original version,which indicates that the multi-lingual can provide differentiated knowledge for this text detection. Table2 also presents the comparison between ours and baselines, where our method outperforms the BER
T
baseline by 2.397 in accuracy score on the official test set.
6 Conclusion
This paper illustrates our contributions for Russian Artificial Text Detection Dialogue Shared task (Multi
Class). We use the DeBER
Ta pre-trained language model with multiple traning strategies to distinguishwhich model from the list was used to generate this text. In the evaluation phase, our submission achievessecond place.
