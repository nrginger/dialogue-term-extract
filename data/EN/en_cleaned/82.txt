1.1. The task of computational metaphor identification
Metaphor occupies a prominent place in contemporary linguistic theory: it is recognized to be one of the most powerful cognitive tools with which humans conceptualize . Metaphor is truly ubiquitous in everyday discourse, forming a fundamental part of the language system; not surprisingly, metaphor identification and interpretation pose a serious challenge to a wide range of real-world NL
P applications.
Different features have been proposed to train machine learning algorithms to identify metaphor, such as lexical, morphological and syntactic, distributional semantic, and topic modelling features; features extracted from lexical thesauri and ontologies Badryzlova Yu. G.
Net, Frame
Net, Verb
Net, Concept
Net, etc.), and psycholinguistic features including concreteness and abstractness, imageability, affect, and force; for a comprehensive overview of systems for metaphor identification, see .
Much of the computational metaphor identification work for Russian follows the top-down design, i.e. is aimed at identifying conceptual metaphors ; ; ; . The experiments for identification of linguistic metaphor in Russian include ; ; ; ; . The first four of these projects explore different sets of features for metaphor classification, including lexical and morphosyntactic co-occurrence, distributional semantic embeddings, and concreteness-abstractness indexes (the present paper is an extension of the preliminary concreteness-abstractness studies from ). The two latter of the aforementioned studies are based on crosslingual model transfer, when the model is trained on English data using English lexical resources, and then the classification features are translated into Russian and other languages by means of an electronic dictionary (for more detail, see Section 1.2).
1.2. Concreteness
Abstractness feature in metaphor
identification: previous research
Implementation of concreteness and abstractness in metaphor identification experiments builds on the groundwork of the theory of embodied and grounded cognition, and primary and conceptual metaphor ; . These theories claim that human thinking is intrinsically metaphoric, since the conceptual representations underlying knowledge are grounded in sensory and motor systems, and conceptual metaphor is the primary mechanism for transferring conventional mental imagery from sensorimotor domains to the domains of subjective experience.
Since concreteness and abstractness represent two dialectically related facets of meaning, they will be discussed under the dual term â€˜Ğ¡oncreteness
Abstractnessâ€™.
The established method to compute indexes of concreteness and abstractness of a word is to collect two sets of lexemes (â€˜seed listsâ€™) consisting of abstract and concrete words (which together constitute the â€˜paradigmâ€™ of concreteness-abstractness)â€”and to measure the lexical similarity between each word in the lexicon and each of the paradigm words ; . In cases when researchers are unwilling to resort to the translational method, computation of concreteness and abstractness indexes becomes a nontrivial language-specific task requiring seed lists that are compiled purposefully for a given language.
the Concreteness
Abstractness indexes of a word by comparing its distributional semantic embedding to the vector representations of twenty abstract and twenty concrete words. The paradigm words are automatically selected from the MR
C Psycholinguistic Database Machine Usable Dictionary , a collection of 4,295 English words rated with degrees of abstractness by human subjects in psycholinguistic experiments. We are replicating the Turney et al. experiments in this work, therefore more detail on their experimental dataset and the obtained results will be reported in Section 3.
Exploring Semantic Concreteness and Abstractness for Metaphor Identification and Beyond also compute the Concreteness
Abstractness indexes of English words by using a distributional semantic model and the MR
C database: they train a logistic regression classifier on 1,225 most abstract and 1,225 most concrete words from MR
C; the degree of Concreteness
Abstractness of a word is the posterior probability produced by the classifier.
This paper presents two Russian paradigms which are used to compute indexes of Concreteness and Abstractness for a large Russian vocabulary. We also introduce a similar English paradigm and compute Concreteness and Abstractness indexes for a large English vocabulary. Both the Russian and the English resources are made available to the community. We use the obtained indexes in machine learning experiments for linguistic metaphor identification on representative datasets in the two languages and compare the performance with previous research and the baselines. Besides, we look into the Concreteness
Abstractness rankings to see how they align with the intuitions about the semantic domains of abstractness and concreteness. To the best of our knowledge, this is the first research of this kind on Russian data.
2. Concreteness
Abstractness indexes
2.1. Russian Concreteness
Abstractness paradigms
We use two paradigms to compute the indexes of Russian words; each paradigm consists of a Concrete (
Con) and an Abstract (
Abs) seed list.
The first paradigm is a subset of the psycholinguistic database collected at the Kazan Federal University , therefore it will be referred to as the Kazan Paradigm in this paper. The psycholinguistic database consists of 500 most frequent Russian nouns which have been ranked by human judges to indicate the perceived degree of concreteness of each noun. We took nouns with the highest and the lowest rankings to populate the Con and the Abs seed lists of the Kazan paradigm, respectively; both animate and inanimate nouns were included in the Con list.
The second paradigm was selected from the Open Semantics of the Russian Language, the semantically annotated dataset of the Karta
Slov database . Karta
Slov contains about 71,000 words with human annotations assigning them to one of the semantic categories of the ontology. The Abs seed list was selected from the category ABSTRAC
T; the nouns for the Con seed list were drawn from the class PHYSICA
L ENTIT
Y, namely, the subclasses INORGANI
C, THIN
G, and ORGANI
C (with the exception of HUMA
N and ANIMA
L). The decision to discard the categories HUMA
N and ANIMA
L was prompted by the consideration that HUMA
N lexemes mostly denote abstract social roles rather than concrete human beings (e.g. Ğ°Ğ´Ğ²Ğ¾ĞºĞ°Ñ‚ â€˜lawyerâ€™, Ğ´Ğ¾Ğ±Ñ€Ğ¾Ğ²Ğ¾Ğ»ĞµÑ† â€˜volunteerâ€™, Ğ¼Ğ°Ñ‡ĞµÑ…Ğ° â€˜stepmotherâ€™, etc.), and much of the ANIMA
L lexicon is intrinsically metaphoric (e.g. Ğ²Ğ¾Ñ€Ğ¾Ğ½Ğ° â€˜crowâ€™, Ğ¾Ñ€ĞµĞ» â€˜eagleâ€™, Ğ»Ğ¸ÑĞ° â€˜foxâ€™, Ğ¾ÑĞµĞ» â€˜donkeyâ€™, ĞºĞ¾Ñ€Ğ¾Ğ²Ğ° â€˜cowâ€™, etc.). This second paradigm will be addressed to as the Moscow Paradigm throughout this paper.
Badryzlova Yu. G. for the size of the paradigms, in the metaphor identification experiments described in Section 3, we experimented with Con
Abs indexes computed with seed lists ranging from 360 to 40 words. No substantial difference in the quality of classification was observed in relation to the size of the paradigm. Thus, we decided to follow recommend 40 as the optimal size of a seed list, since they indicated a problematic amount of overfitting when increasing the number of words. Examples from the Kazan and the Moscow Paradigms are presented in Abs seed list (excerpt) Con seed list (excerpt)
Moscow ParadigmĞ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¾ÑÑ‚ÑŒ â€˜domesticityâ€™ Ğ°Ñ€Ñ„Ğ° â€˜harpâ€™Ğ¼ĞµÑ‡Ñ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ â€˜reverieâ€™ Ñ‚ĞµĞ¿Ğ»Ğ¾Ñ…Ğ¾Ğ´ â€˜motorshipâ€™Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¼ â€˜automatismâ€™ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒÑ†Ğµ â€˜compact mirrorâ€™Ñ€Ğ°ÑĞ¿ÑƒÑ‚ÑÑ‚Ğ²Ğ¾ â€˜promiscuity â€™ Ğ¿ĞµĞ»ÑŒĞ¼ĞµĞ½Ğ¸ â€˜dumplingsâ€™Ñ‡Ñ€ĞµĞ·Ğ²Ñ‹Ñ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ â€˜extraordinarinessâ€™ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ñ€Ğ° â€˜electric guitarâ€™Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ â€˜party affiliationâ€™ Ğ³Ñ€Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼ĞµÑ‚ â€˜grenade launcherâ€™Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ â€˜activationâ€™ Ğ»Ğ¸Ğ½ĞµĞ¹ĞºĞ° â€˜rulerâ€™Ğ±ÑƒĞ´Ğ´Ğ¸Ğ·Ğ¼ â€˜
Buddhism â€™ Ñ„ÑƒĞ³Ğ°Ğ½Ğ¾Ğº â€˜jointer planeâ€™Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ â€˜correctnessâ€™ ÑƒĞ·ĞµĞ»Ğ¾Ğº â€˜knotâ€™Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¼ â€˜minimalismâ€™ Ğ¿Ğ¾Ğ»ÑƒĞ¿Ğ°Ğ»ÑŒÑ‚Ğ¾ â€˜short coatâ€™
Kazan ParadigmĞ¶Ğ¸Ğ·Ğ½ÑŒ â€˜lifeâ€™ ÑÑ‚Ğ¾Ğ» table'Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ â€˜processâ€™ ÑĞ°Ğ¼Ğ¾Ğ»ĞµÑ‚ â€˜aircraftâ€™Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ â€˜representation / showâ€™ Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº â€˜boyâ€™ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ â€˜abilityâ€™ ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ° â€˜apartmentâ€™Ñ€Ğ°Ğ´Ğ¾ÑÑ‚ÑŒ â€˜joyâ€™ Ğ²Ñ€Ğ°Ñ‡ â€˜doctorâ€™ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ â€˜correspondenceâ€™ Ğ´ĞµĞ²Ğ¾Ñ‡ĞºĞ° â€˜girlâ€™ÑÑƒÑ‚ÑŒ â€˜essenceâ€™ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½ â€˜phoneâ€™Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ â€˜situation / locationâ€™ Ğ¿Ğ°Ğ»ĞµÑ† â€˜fingerâ€™Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ğµ â€˜desireâ€™ Ñ€ÑƒĞ±Ğ»ÑŒ â€˜roubleâ€™Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€˜responsibilityâ€™ ÑÑ‚ĞµĞºĞ»Ğ¾ â€˜glassâ€™2.2. English Concreteness
Abstractness paradigm
To compile the English Paradigm, we used the MR
C Psycholinguistic Database (similarly to the previous research presented in Section 1.2). Nouns from the top and from the end of the MR
C concreteness rating were drawn to populate the paradigm. We use three English Concreteness seed lists: one consisting of animate (
Anim), and another of inanimate (
Inan) nouns, while the third is the composite concreteness (
Con) list composed of animate and inanimate nouns in equal proportions. The English seed lists are presented in of animate nouns in the top of the concreteness ranking in the MR
C database, where animal nouns rank higher than those denoting humans. This may be due to the consideration discussed above in Section 2.1, namely, that lexemes denoting humans mainly indicate abstract social roles rather than physical human beings.
Exploring Semantic Concreteness and Abstractness for Metaphor Identification and Beyond
Anim ape, adder, albatross, beetle, carp, cat, catfish, chicken, clown, cow, crab, deer, eagle, frog, goat, gorilla, grasshopper, hare, hen, horse, lion, mackerel, mussel, nightingale, otter, owl, ox, pig, puppy, rabbit, rat, sheep, shrimp, skunk, skylark, sparrow, stoat, stork, turtle, walrus
Inan balloon, banana, barn, bed, bench, bluebell, bra, bridge, camera, car, carnation, casket, cauliflower, clarinet, collar, corkscrew, cucumber, daisy, egg, garlic, harpsichord, jacket, lamp, lantern, mattress, nightgown, olive, pants, pea, peach, pencil, piano, plum, potato, quilt, saxophone, ship, skyscraper, sofa, tulip
Abs affirmation, animosity, demeanour, derivation, determination, detestation, devotion, enunciation, etiquette, fallacy, forethought, gratitude, harm, hatred, illiteracy, impatience, independence, indolence, inefficiency, insufficiency, integrity, intellect, interposition, justification, malice, mediocrity, obedience, oblivion, optimism, prestige, pretence, reputation, resentment, tendency, unanimity, uneasiness, unhappiness, unreality, value2.3. Computing the Concreteness
Abstractness indexes
We computed Con
Abs indexes for a Russian vocabulary with a total of about 18,000 words, and for an English vocabulary of about 17,000 words.
The Russian Con
Abs indexes were computed using a pre-trained Continuous Skip
Gram distributional semantic model on the Araneum corpus . The English indexes were computed with a Continuous Skip
Gram model had been pre-trained on the Gigaword 5th Edition corpus .
As shown in Equation 1, we measured semantic similarity (cosine distance, ğ‘†ğ‘–ğ‘š) between the vectors of each word in the vocabulary and each word in a seed list, and took the mean of the ten nearest semantic neighbors (ğ‘ğ‘, Equations 2â€“3) in order to obtain the indexes.
	 âˆ€	ğ‘£ğ‘–,	âˆ€	ğ‘ ğ‘—	âˆƒ	ğ·ğ‘–	=	{ğ‘†ğ‘–ğ‘š(ğ‘£ğ‘–,	ğ‘ â‚), ğ‘†ğ‘–ğ‘š(ğ‘£ğ‘–,	ğ‘ â‚‚), ..., ğ‘†ğ‘–ğ‘š(ğ‘£ğ‘–,	ğ‘ ğ‘—),	...,	ğ‘†ğ‘–ğ‘š(ğ‘£ğ‘–,	ğ‘ ğ‘˜)}, (1)where ğ‘‰ is the set of words in the vocabulary, ğ‘† is the set of words in the seed list, ğ‘˜ is the number of elements in ğ‘†	 	 	 	 									ğ‘ğ‘ =	{ğ‘‘â€²ğ‘–â‚,	ğ‘‘â€²ğ‘–â‚‚,	...,	ğ‘‘â€²ğ‘–â‚â‚€}, (2)where ğ·ğ‘–â€² is a linearly ordered set of ğ·ğ‘– (in ascending order)	 	 	 	 													ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ =ğ‘€ğ‘’ğ‘ğ‘›{ğ‘ğ‘} (3)
Since we had two Con and two Abs seed lists in the Russian part of the experiment, and three Con and one Abs seed list in the English part, the following rankings were generated: Moscow Con, Moscow Abs, Kazan Con, and Kazan Abs for Russian; and English Anim, English Inan, English Con, and English Abs, for English1.
1 The full English and Russian rankings computed with the described method, as well
as the Rus
Met corpus are available at https://github.com/yubadryzlova/
ConcretenessAbstractness-in- Abstractness-in
MetaphorIdentification.
https://github.com/yubadryzlova/Concreteness-Abstractness-in-Metaphor-Identificationhttps://github.com/yubadryzlova/Concreteness-Abstractness-in-Metaphor-Identification
Badryzlova Yu. G. Metaphor identification experiments
We conduct two experiments with English data and two with Russian data.
We begin by replicating the experiments with the Tro
Fi (
Trope Finder) Example Base in . Tro
Fi is a collection of verbal metaphor: it is built around 50 polysemous target verbs (e.g. absorb, assault, die, drag, drown, smooth, step, stick, strike, touch, etc.). Each target verb has from 1 to 115 sentences which are annotated as either literal or non-literal. For example,
see the sentences with the target verb besiege:â€¢ (
Literal) In 1347, Mongols < besieging > the Black Sea port of Caffa began to sicken and die from the plague.
â€¢ (
Non-literal) â€¦ Powelson began to < besiege > me with letters asking for an invitation.
In the first of the English experiments following Turney et al. we use a subset of 25 target verbs. Each group of sentences for a given target verb is treated as a separate learning problem, by learning and testing a separate model with ten-fold crossvalidation. The performance is measured as macro-averaged Accuracy and F1-score (this experiment will be referred to as Tro
Fi-1 below).
In our second English experiment after Turney et al., one model is trained on the entire subset of the 25 target verbs from Tro
Fi-1 (1,965 sentences), which is then tested on the new unseen sentences (numbering 1,772) with the other 25 verbs (this experiment will be called Tro
Fi-2 in the rest of this paper).
The two Russian experiments are performed on Rus
Met, the Russian corpus of metaphor-annotated sentences presented in . The corpus consists of 7,020 sentences; each of them contains one of the 20 polysemous target verbs (e.g. Ğ±Ğ¾Ğ¼Ğ±Ğ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ â€˜to bombardâ€™, Ğ½Ğ°Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ â€˜to attackâ€™, ÑƒÑ‚ÑĞ¶Ğ¸Ñ‚ÑŒ â€˜to iron (about clothes)â€™, Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ â€˜to explode (smth)â€™, Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ â€˜to weighâ€™, etc.) which are used either metaphorically or non-metaphorically. The number of sentences per target verb ranges from 225 to 693; each of these subsets is balanced by class. The following sentences demonstrate examples of the metaphoric and non-metaphoric classes with the target verb ÑƒĞºĞ°Ğ»Ñ‹Ğ²Ğ°Ñ‚ÑŒ â€˜to prickâ€™:â€¢ (
Metaphorical) Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾ < ÑƒĞºĞ¾Ğ»Ğ¾Ğ»Ğ¾ > Ğ¼ĞµĞ½Ñ. This defeat seriously piqued (lit. < â€˜prickedâ€™ >) me.
â€¢ (
Metaphorical) Ğ¡Ğ°Ğ¼Ğ¾Ğ»ÑĞ±Ğ¸Ğµâ€”ÑÑ‚Ğ¾ Ğ½Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ‚Ñ€Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ñ‹Ğ¹ ÑˆĞ°Ñ€, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ±ÑƒÑ€Ñ, Ğ»Ğ¸ÑˆÑŒ < ÑƒĞºĞ¾Ğ»ĞµÑˆÑŒ > ĞµĞ³Ğ¾. â€˜
Vanity is a balloon filled with the wind; once you < prick > it, you release a stormâ€™.
â€¢ (
Non-metaphorical) Ğ Ğ¼Ğ°Ğ½Ğ¸ĞºÑÑ€ÑˆĞ°, Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑÑ, ĞºĞ°Ğº-Ñ‚Ğ¾ Ğ´Ğ¾ ĞºÑ€Ğ¾Ğ²Ğ¸ < ÑƒĞºĞ¾Ğ»Ğ¾Ğ»Ğ° > Ğ¼Ğ½Ğµ Ğ¿Ğ°Ğ»ĞµÑ†. â€˜
I recall that a manicurist has once < pricked > my finger so that it bledâ€™.
Here, we start by training and testing a separate model with ten-fold crossvalidation for each of the 20 subsets with individual target verbs; the overall performance is measured in terms of macro-averaged Accuracy and F1-score. Then, we split the dataset into two approximately equal parts: one containing the first ten target verbs (3,617 sentences in total), and the other containing the remaining ten target verbs (3,504 sentences). We train the classifier on the first subset and test it on the Exploring Semantic Concreteness and Abstractness for Metaphor Identification and Beyondsecond. These two experiments will be respectively referred to as Rus
Met-1 and Rus
Met-2 in the discussion below.
In all the four experiments, the metaphor identification task was formulated as sentence-level binary classification. The Support Vector Machine (SV
M) classifier with linear kernel was used to learn and test the models.
For each sentence, we computed the mean of the scores of its constituent content words. The decision to use entire sentences rather than syntactic dependencies of the target verbs was motivated, firstly, by the design of the Turney et al. experiments replicated in Tro
Fi-1 and Tro
Fi-2 and, secondly, following the observations by accuracy of metaphor identification tends to grow with the increase of the window size, suggesting that discourse-level features consistently enhance performance of conventional, non-neural classifiers making them competitive with sophisticated neural models.
In each of the experiments we compare the performance of the Con
Abs model with the baseline lexical model in which the frequencies of lemmas are represented by the Î”
P indexes of co-occurrence . The choice of the baseline was prompted by the findings of reported that simple lexical unigram baselines achieve surprisingly good results for some of the datasets
Since in both of the pre-trained distributional semantic models which were used to compute the Con
Abs indexes (see Section 2.3) the tokens are lemmatized, the Tro
Fi and the Rus
Met datasets were also PO
S-tagged. Tro
Fi was preprocessed with Stanford Log-linear Part-Of
Speech Tagger , and Rus
Met with the Ru-syntax parser .
4. Results
The results of the classification experiments are presented in Tables 3 and 4 (along with the lemma baselines).
In the Tro
Fi experiments, our Con
Abs models either outperform the Turney et al. models, both in Accuracy and F1-score (sometimes by a sizeable margin, as in the case of Tro
Fi-1), or they fall behind, yet by a slim margin.
When comparing the Con
Abs results with the lemma baseline, we see that in all the cases the Con
Abs models either outstrip the baseline both in Accuracy and F1score (sometimes by a substantial margin, e.g. the Accuracy in both of the Tro
Fi experiments), or they fall behind, but by a very narrow margin, as in Rus
Met-1.
The overall lower results of the Tro
Fi experiments, as compared to Rus
Met, are presumably due to the unbalanced setup of the former dataset.
When comparing the performance of the models learned and tested separately for each target verb (Tro
Fi-1 and Rus
Met-1) with the models that were trained on one set of target verbs and tested on another set of new verbs (Tro
Fi-2 and Rus
Met-2), the former models, quite expectedly, produce higher results. This is not surprising considering that the second task is more challenging and ambitious, since the model has to capture semantic regularities that are common for all the verbs irrespective of the idiosyncratic patterns of their combinability.
Badryzlova Yu. G. Con-Abs
Acc F1 Acc F1Tro
Fi-1, Turney et al. N
A N
A 0.73 0.64Tro
Fi-1, this work 0.7 0.76 0.75 0.77Tro
Fi-2, Turney et al. N
A N
A 0.68 0.68Tro
Fi-2, this work 0.63 0.67 0.7 0.67lemma Con-Abs
Acc F1 Acc F1Rus
Met-1 0.83 0.83 0.82 0.82Rus
Met-2 0.73 0.76 0.75 0.77
Table 5 compares models based on different types of Con
Abs indexes and their combinations in the Tro
Fi-2 and the Rus
Met-2 experiments. The uni-feature Con models outperform the Abs models twice (Mos
Con and Eng
Con vs. Mos
Abs and Eng
Abs, respectively); besides, the combination of two Con features (Mos
Con and Kaz
Con) outstrips the combination of the two Abs features (Mos
Abs and Kaz
Abs) by a safe marginâ€”thus suggesting that indexes of Concreteness may serve as more reliable predictors of metaphoricity.
Russian rankings Acc F1 English rankings Acc F1Mos
Con 0.72 0.75 Anim 0.66 0.66Mos
Abs 0.67 0.68 Inan 0.67 0.67Kaz
Con 0.62 0.63 Con 0.68 0.68Kaz
Abs 0.68 0.68 Abs 0.59 0.57Mos
Con, Mos
Abs 0.74 0.77 An, Inan 0.67 0.67Kaz
Con, Kaz
Abs 0.74 0.74 Con, Abs 0.7 0.67Mos
Con, Kaz
Con 0.72 0.75 An, Inan, Abs 0.69 0.67Mos
Abs, Kaz
Abs 0.69 0.69 An, Inan, Con, Abs 0.69 0.66Mos
Con, Mos
Abs, Kaz
Con, Kaz
Abs0.75 0.77
The Mos
Con model substantially outperforms Kaz
Con, while both of the Russian Abs models perform on the par with each other, which may indicate that the two Abstractness rankings resemble each other. There is not much difference between Eng
Anim and Eng
Inan; besides, the combination of Eng
Anim and Eng
Inan fares Exploring Semantic Concreteness and Abstractness for Metaphor Identification and Beyondrelatively similarly to the composite Eng
Con feature; this may suggest that both animate and inanimate nouns may be equally representative of the semantic category of concreteness. In the English experiments, the best result is achieved by combining only two features, Eng
Con and En
Abs, while with the Russian models the top performance is produced by combination of all the four types of indexes.
5. Concreteness
Abstractness indexes: a reliability test
In order to put our Con
Abs indexes to test, we took 451 known abstract nouns and 571 known concrete nouns from the Karta
Slov thesaurus; we computed their Moscow Con
Abs indexes as described in Section 2 and clustered them using k-means algorithm with N clusters = 2.
The resulting clustering Accuracy was 0.99: indexes of known concrete and known abstract words form two distinct clusters with little overlap. The misclassified concrete nouns are Ğ°Ñ„Ğ¸ÑˆĞ° â€˜posterâ€™, Ğ±Ğ°Ğ½Ğ´ĞµÑ€Ğ¾Ğ»ÑŒ â€˜parcelâ€™, Ğ±Ğ»Ğ°Ğ½Ğº â€˜form (document)â€™, and Ğ·Ğ°Ğ¿Ğ¸ÑĞºĞ° â€˜noteâ€™; the misclassified abstract nouns are Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ» â€˜nominal valueâ€™ and Ğ¿Ñ€Ğ¸ÑĞ»Ğ¾Ğ²ÑŒĞµ â€˜proverbâ€™.
6. A closer look at Concreteness
Abstractness indexes
The overall encouraging results of the metaphor identification experiments and the reliability test motivate us to take a more in-depth look at the computationally generated indexes of Concreteness and Abstractness to see whether their behavior aligns with the intuitive expectations about the corresponding semantic categories.
Badryzlova Yu. G. one could presume that Con and Abs indexes should stand in negative correlation to each other: the higher the Concreteness of a word, the lower its Abstractness should be, and vice versa. Yet, no statistical correlation between our Con and Abs rankings has been found. Obviously, the linguistic reality is not so straightforward, especially when taking into account the issue of polysemy, when different meanings of one word may vary in Concreteness and Abstractness. A glance at visualizations of the pairs of index rankings (
Con vs. Abs) reveals interesting insights.
Russian and the English vocabulary. The lexemes are ordered by the Con indexes (in descending order) which thus form a smooth sigmoid curve; the small dots are the Abs indexes which correspond to each of the Con indexes in the sigmoid.
In plots A and C (depicting the Moscow and the English indexes), in the beginning of the sigmoid curve, where the Con indexes are the highest, all the Abs indexes are located below the curve, with the dots forming areas of high density. This distribution indicates that there is a well-defined group of concrete words characterized by very high concreteness and very low abstractness. At the opposite end of the curve, where the Con indexes are the lowest, all the Abs indexes are above the curve, although there is no such pronounced gap between them as in the beginning: we see that, although abstract vocabulary does form a semantically uniform group, it is not as numerous as the concrete vocabulary, and the Concreteness and Abstractness are not so distinctly contrasted in it. The long steep part of the curve shows that the Abs indexes are scattered both below and above itâ€”yet, as the curve goes down, the number of the Abs dots above the curve increases. Thus, there is a broadly defined trend for Abstractness to grow with the decrease of the Concreteness, which conforms to the linguistic intuitions; this trend is profiled in Abs indexes located above and below the Con line in the English vocabulary.
Table 5 demonstrates English and Russian examples of Con and Abs indexes from the beginning, the middle, and the end of the ranking. These examples are also intuitively feasible: the nouns from the top are concrete, the words in the middle are a semantic mix of the concrete and the abstract, and the words from the end of the ranking are highly abstract.
Plot B in Figure 2, which depicts the Con
Abs indexes computed using the Kazan paradigm, shows that the Abs indexes have much lower variance than in the Moscow and the English rankings. This may be a result of the fact that the lexemes in the Kazan paradigm are high-frequency words: thus, the mean frequency (ipm, according to ) of the Moscow paradigm is 17.8, while the mean frequency of the Kazan paradigm is 244.23. Words with high frequency tend to cooccur with a wide range of vocabulary, so their distributional semantic vectors may be less indicative of the concreteness and abstractness expected of a paradigm word. Still, comparison of the Mos
Abs and the Kaz
Abs rankings provides an interesting observation: there is a strong positive Pearson correlation (0.73) between themâ€”which is not the case between Mos
Con and Kaz
Con (corr = 0.5). The presence of correlation between the rankings computed with two different Abs paradigms seems to confirm the intuition that the category of Abstractness is semantically more homogeneous than the category of Concreteness. The Anim and Inan indexes of the English ranking Exploring Semantic Concreteness and Abstractness for Metaphor Identification and Beyondalso show strong positive correlation (0.8), which may indicate that animate and inanimate nouns are equally representative of the semantic category of concreteness.
AbsConAbs
Con Abs
Badryzlova Yu. G. below the Con line (
English)from the top, middle, and end of the ranking
Russian word Translation PO
S Con Abs English word PO
S Con Abs
TopĞ½Ğ°Ğ²Ğ¾Ğ»Ğ¾Ñ‡ĞºĞ° â€˜pillow-caseâ€™ noun 0.45 0.13 shrimp noun 0.71 0.17ÑĞ°Ğ»Ñ„ĞµÑ‚ĞºĞ° â€˜napkinâ€™ noun 0.44 0.11 crab noun 0.68 0.18Ğ¿Ğ»ĞµĞ´ â€˜bed throwâ€™ noun 0.43 0.13 tomato noun 0.68 0.2Ğ¾Ğ¼Ğ»ĞµÑ‚ â€˜omeletteâ€™ noun 0.43 0.14 scallop noun 0.67 0.18Ğ¾Ğ´ĞµÑĞ»Ğ¾ â€˜blanketâ€™ noun 0.42 0.13 chicken noun 0.66 0.14ÑÑƒĞ¼Ğ¾Ñ‡ĞºĞ° â€˜purseâ€™ noun 0.41 0.13 mussel noun 0.66 0.18ÑĞºĞ°Ñ‚ĞµÑ€Ñ‚ÑŒ â€˜tableclothâ€™ noun 0.41 0.13 lobster noun 0.66 0.17Ğ²Ğ¸Ğ½Ñ‚Ğ¾Ğ²ĞºĞ° â€˜rifleâ€™ noun 0.41 0.13 oyster noun 0.65 0.18ÑˆĞ°Ñ€Ñ„ â€˜scarfâ€™ noun 0.4 0.13 onion noun 0.65 0.19ÑĞ¾ÑƒÑ â€˜sauceâ€™ noun 0.4 0.15 potato noun 0.64 0.2
MiddleÑ‚Ğ¾Ñ€Ñ€ĞµĞ½Ñ‚ â€˜torrentâ€™ noun 0.16 0.14 healey noun 0.2 0.17Ğ½Ğ°Ğ³Ğ»Ğ¾ â€˜brazenlyâ€™ adv 0.16 0.19 swing verb 0.2 0.16Ğ±Ğ¾Ğ´Ñ€Ğ¾ â€˜cheerfullyâ€™ adv 0.16 0.15 carrier noun 0.2 0.15Ñ€Ğ°Ğ¶ â€˜ zealâ€™ noun 0.16 0.21 smash verb 0.2 0.11Ğ²Ğ°Ğ»ÑŒĞ´Ğ¾Ñ€Ñ„ÑĞºĞ¸Ğ¹ â€˜waldorfâ€™ adj 0.16 0.14 unload verb 0.2 0.14Ğ²Ğ¿ĞµÑ€ĞµĞ´Ğ¸ â€˜in front ofâ€™ adv 0.16 0.14 stanford noun 0.2 0.16Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ â€˜sellâ€™ verb 0.16 0.09 north adv 0.2 0.15Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑâ€˜practiceâ€™ verb 0.16 0.12 perilous adj 0.2 0.21Ğ²ĞµĞºĞ¾Ğ²Ğ¾Ğ¹ â€˜centennialâ€™ adj 0.16 0.18 enumeration noun 0.2 0.29
Exploring Semantic Concreteness and Abstractness for Metaphor Identification and Beyond
Russian word Translation PO
S Con Abs English word PO
S Con Abs
EndÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ñ‚ÑŒÑÑ â€˜correspondâ€™ verb 0.07 0.19 vitro propn 0.2 0.12Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ñ†Ğ¸Ñ â€˜differentiationâ€™ noun 0.07 0.27 open-ended adj 0.11 0.24Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ â€˜involvementâ€™ noun 0.07 0.26 inter-departmentaladj 0.11 0.18Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ â€˜proliferationâ€™ noun 0.07 0.2 rejection noun 0.11 0.36Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ â€˜separationâ€™ noun 0.07 0.27 discredit verb 0.11 0.31Ñ‚Ğ¾Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ â€˜totalâ€™ adj 0.07 0.25 hold verb 0.11 0.13Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒÑÑ â€˜be regarded asâ€™ verb 0.07 0.22 legitimate adj 0.11 0.28Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ â€˜comprehensivelyâ€™adv 0.07 0.16 unsuccessfullyadv 0.11 0.15Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ â€˜detectâ€™ verb 0.07 0.14 long-awaited adj 0.11 0.19Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ â€˜re-subordinationâ€™noun 0.07 0.23 abolition noun 0.11 0.257. Conclusions
We have presented a method for computing indexes of semantic Abstractness and Concreteness of words in two languagesâ€”
Russian and English, and applied them to the task of linguistic metaphor identification in these two languages.
The results of the classification are either comparable with previous research or surpass it, the same holds for the baseline model. The efficiency of metaphor identification with Concreteness
Abstractness indexes suggests that metaphoric and nonmetaphoric contexts are semantically different in terms of Concreteness and Abstractness, which conforms to the Conceptual Metaphor Theory. Models based on Concreteness indexes are more efficient than the models based on Abstractness, suggesting that Concreteness may be a more reliable predictor of metaphoricity.
Analysis of the Concreteness
Abstractness indexes reveals that there is a general trend for Abstractness indexes to increase as the corresponding Concreteness indexes decrease, which is in accordance with linguistic intuitions. There is a distinct group of highly concrete words in the lexicon which have very high Concreteness and very low Abstractness indexes; similarly, there is a group of distinctly abstract vocabulary, with low Concreteness and high Abstractness scores.
Presence of statistical correlation between two Russian Abstractness ratings (and absence of correlation between the respective Concreteness ratings) may indicate that the category of Abstractness is more semantically homogeneous than the category of Concreteness. Statistical correlation between the English concrete Animate and Inanimate rankings suggests that both animate and non-animate nouns may be equally representative of the semantic category of Concreteness.
Badryzlova Yu. G.