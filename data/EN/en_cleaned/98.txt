Spelling correction is one of the oldest and most important problems of computational linguistics. It has attracted many researchers since the pioneer works of Levenshtein and Damerau in the 60-s the studies on isolated word correction in the beginning of modern NL
P era, such as early context-based methods sophisticated machine-learning based techniques of last decade used in . Automatic spellchecking is a problem of high practical importance, especially concerning actual technical requirements of big corpora . The most straightforward application of it is query correction and completion, used in search engines, as well as orthography correctors which form a part of any modern text editor. More marginal applications include second language learning grammatical error correction .
Automatic spelling correction for Russian social media texts In the next section, we describe present-day situation in the field of spelling correction. Section 3 contains a detailed overview of the system developed. Section 4 describes the problems we faced during the development of our system and some open questions left, while Section 5 discusses the results attained. Finally, in Section 6, we offer a brief conclusion of our research and propose some directions for its future application.
2. Past and present of spellchecking: methods and problems
Different researchers focus on different aspects of spelling correction in their studies. Most of the early works dealt with effective search of candidates for typo correction, addressing the problem of fast dictionary lookup and approximate string matching, which is especially important for agglutinative and polysynthetic languages. The task of candidate search is alleviated by the fact that 80% of time a correct word can be obtained from the mistyped word by one primitive edit operation1 . However, orthographic mistakes usually happen on phonetic level, while spelling correction is performed on the graphic one, which complicates the search when phonetic and graphic representations of do not exactly map to each other: the . Moreover, not all primitive edit operations and orthographic changes are equiprobable which means that a variant of weighted Levenshtein distance should be used instead of the basic one to achieve better performance .
Correction systems used in text editors, such as Notepad++ or M
S Word, usually suggest several candidates, compelling the user to select between them. However, the number of variants even for a medium-length sentence is too high, which implies that ideal spellchecker should be able not only to generate corrections, but also to select the best one in the given context. Another difficulty concerns the typos producing another dictionary word (such as piece/peace or компания/кампания). Such problems are a subject of context-sensitive spelling correction . Most studies address this task in a narrow fashion, trying to correct realword spelling errors only in the groups of several predefined confusion sets . Then for every confusion set the task becomes a usual classification problem which can be solved using standard machine-learning techniques. However, this method cannot be straightforwardly generalized to real-world spelling correction since the dimension of feature space becomes too high (the most standard features for this task are adjacent words, which means that every pair of dictionary words is a separate feature). Another approach which we pursued in our work is to learn a low-dimensional classifier which uses the scores given by error and language models as its features.
The difficulty of spelling correction also depends from its domain of application and the source language. Indeed, the more fine-grained is the morphology system, 1 Primitive edit operations include a) deletion of a single character, b) insertion of a single character, c) substitution of one character for another, d) permutation of a pair of adjacent characters.
Sorokin A. A., Shavrina T. O.
larger is the dictionary, which complicates candidate search and selection. This also makes the data more sparse implying that larger corpora are necessary to learn the language model. Using World Wide Web as a huge unannotated corpora partially solves the problem, but in this case the training data already contains typos which can deteriorate the performance of correction system. Moreover, the percenttage of outof-vocabulary words, such as proper names, slang and neologisms, is very high for the Web creating another obstacle for the dictionary-based approach. That’s why some authors refuse to use the dictionary basing their algorithms only on corpora frequency.
We are especially interested in spelling correction when applied to social media texts, such as Live Journal, V
Kontakte and other blogs and social networks. The percentage of misspelled words in such texts is rather high both due to typos and orthography errors and effective correction of such errors is a necessary preliminary condition for further processing such as morphological and syntactical parsing. The percentage of out-of-vocabulary words is also rather high. Our work is a part of General Internet Corpora of Russian (GIC
R) Project . There are very few works on spelling correction for Russian , , ; moreover, the first two are concerned primarily with correction of mistyped search queries, while the latter addresses only to isolated word correction.
3. Our system
Our system participated in the first competition of spellcheckers for Russian SpellRu
Eval-2016 and won the first place by all the measures, including precision, recall, F1-measure and percentage of correct sentences. We decided to follow the scheme described in collecting scores from different levels including dictionary model, n-gram language model, a weighted error model and morphological error model and combining them in a single linear classifier. We used the reranking approach applied in machine translation : the algorithm first created n-best lists of candidate sentences according to the simplest of the models and then reranked these hypotheses using logistic regression classifier. We observed that reranking indeed leads to a consistent gain in performance. Other results were quite surprising for us: we observed that morphological and semantic features does not give any further improvement after applying the error model, which either implies that the features we used are too weak to distinguish good hypotheses from the best ones in comparison with other features or that our model of morphology and semantics was not adequate for this task.
3.1. Multi-level spelling correction
In this section we describe our algorithm of spelling errors correction. The algorithm processes one sentence at a time and consists of two stages. On the first stage we rank the candidate hypotheses according to a baseline model. Then for every Automatic spelling correction for Russian social media texts candidate correction several scores are calculated. These scores include the number of words corrected, the logarithmic probability of the sentence according to the language model, the logarithmic probability of the source sentence to be obtained from the correction by the error model and several other scores characterizing the adequacy and quality of the corrections. These scores were given to a linear classifier as features and the candidate sentence with the highest score was selected. The weights of the classifier were trained on the development set. We describe all these operations further in the article.
3.2. Candidate generation
When generating the candidate sentences, the first step is to find possible corrections for every word in this sentence. The first part of this list consists of all the words on the edit distance 1 from the source as well as the source word itself, no matter whether it appears in the dictionary or not. We used the list of words from ABBY
Y Compreno which includes approx. 3.7 million words. We store the dictionary as a prefix tree which allows us to effectively search for the words on the distance d or less. We selected d=1 since larger values lead to a drastic expansion of candidate list. The search procedure follows the algorithm described in the heuristics used in , for a more detailed description we refer the reader to . We transformed all the words to lower case but preserve the information about the capitalization of source word since the abbreviations written by all capitals and lowercase common words have different probabilities to be mistyped. All the words which contain non-alphabetic characters such as Latin letters or digits were copied to the output without candidate search except certain special cases like (в4ера → вчера).
However, this error model does not capture several frequent patterns, such as цца → тся/ться transformation in the verb flexion (появляцца → появляться, появицца → появится), which is very popular in Russian Internet slang. We deal with this problem by using an analogue of Metaphone algorithm , mapping the sounds to their phonetic classes. We used the following table of classes:code Russian letters1 а, о, ы, у, я
3 и, е, ё, ю, я, э
5 б, п
6 в, ф
7 д, т
code Russian letters8 г, к, х
9 л
10 р
11 м
12 н
code Russian letters13 з, с
14 й
15 щ, ч
16 ж, ш
17 ц
The symbols also affect the code of consequent symbols: all the vowels after the ь, ъ, щ, ч, й letters were mapped to class 3, as well as to class 1 after ш, ж and ц. The Sorokin A. A., Shavrina T. O.
ь, ъ were omitted. To deal with multisymbol sequences we mapped the тс sequence possibly with sign letters between them to the class 17, we also omitted т after a sibilant and before other consonant (for example, in the word грустный and compressed consecutive occurrences of the same code to a single one. To use this algorithm in spelling correction, we extended the list of candidates by all the dictionary words having the same phonetic code as the source word. However, these transformation does not capture all irregular patterns, so we handcoded a list of about 50 transformations such as ваще → вообще and грит → говорит.
Another frequent error pattern is insertion/deletion of space symbol. When the space is inserted in the middle of the word, it is straightforward to model it by allowing the algorithm to traverse from the terminal node of the dictionary tree to its root and paying the special cost for space insertion on this edge. This modification allows us to recognize all the tuples of dictionary word separated by one or more spaces. An analogous problem arises when considering the deletion/insertion of hyphen (-) symbol in composite words, we solved it by adding ‘-’ to the alphabet. This method cannot handle space deletion since we process the sentence word by word. Therefore we performed the candidate search not only for every single word, but also for the groups of two consecutive words.
Given a sentence of 10 words and 3 dictionary candidates for every word in average, which is more probably an underestimate than an overestimate, we obtain approximately 60,000 candidate sentences for every source sentence, which is obviously impossible to handle. Therefore the procedures of candidate generation and baseline candidate ranking cannot be separated. We rank the candidates by the sum Clm+
Cerr of language model score Clm and basic error model score Cerr (the least score is the best). We describe language model in the next subsection and now we explain the basic error model.
Given a source sentence u1, ..., un, we generate the candidate hypothesis v1, ..., vm by groups, for example (1):(1) кто то ищо сделал тоже предположение кто-то ещё сделал то же предположение
Here we have 5 groups (кто то, кто-то), (ищо ещё), (сделал, сделал), (тоже, то же), (предположение, предположение) and denote them by g1, ..., g5. If the partition of the source-candidate pair of sentences include r groups g1, ..., gr, each group including the source word group si and correction word group ti, then the overall cost of this transformation is −∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 C (si → ti ) is the cost of transforming si to ti. This cost is calculated by the following euristics table. The row of a table correspond to the property of si, while the column describes the way to obtain ti from si. When a source group is fixed, the weights of different hypotheses are taken from the same row, therefore the weights do not need to sum to 1 since the normalizing coefficient after calculating the logarithm yields a constant summand, which is the same form all candidate word groups. The weights were obtained empirically from the development set by calculating the frequencies of different typo-correction transformations.
Automatic spelling correction for Russian social media texts si = ti Levenshtein
Phonetic code2 words
from 12 words
from 1no capitals, dictionary word1 0.005 0.0005 0.001 0.005
initial capital,dictionary word1 0.001 0.0001 0.0001 0.0001
all capitals,dictionary word1 0.001 0.0001 0.0001 0.0001
no capitals, dictionary word0.15 0.6 0.09 0.15 0.01
initial capital,dictionary word1 0.05 0.01 0.005 0.005
all capitals,dictionary word1 0.1 0.01 0.01 0.01
3.3. Language model and generation of candidate sentences
Since basic error model score cannot distinguish between different dictionary words on the same Levenshtein distance, we also took into account the language model to obtain the baseline candidate score Clm+
Cerr. Provided k is the order of the language model, the language model score of the candidate sentence t1, ..., tm equals −∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 The logarithmic cost in the language model appears to be additive as well as the error model cost. It permits us to apply beam search for pruning partial hypotheses space. Agenda consists of n+1 hypotheses lists, where i-th list store partial hypotheses after processing i words of the sentence and n is the length of the source sentence. Initial agenda item consists of empty hypotheses with initial cost 0. On i-th step we generate all the candidates for the word si to expand the hypotheses on the step i−1, as well as the candidates for the group (si−1, si ) to expand the hypotheses from the (i−2)-th item of the agenda. For every partial hypothesis we store its current score and the state of the language model (roughly speaking, last (k−1) words). Using this information, we are able to recalculate the score and the state for the expanded hypotheses. We arrange the list items by the states of language models, storing all the partial hypotheses with the same state together. To prune the hypotheses space we preserve only such hypotheses whose score is not greater than the score of the best hypothesis times some constant α.
Sorokin A. A., Shavrina T. O.
Learning of the reranking model
After the previous step we have a list of candidates together with their baseline scores. Now our task is to rerank these candidates using algorithms of machine learning. For this goal we use a linear classifier and determine the best correction using −∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 the rule−∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 where C is the set of candidate sentences and fi are features which we specify below. To learn the weights of the classifier we observe that maximum does not depend on the additive term w0 therefore only the linear coefficients wi should be learnt. In machine translation literature the usual approach is to learn these coefficients from the ranking of train hypotheses, however, in our disposal are only the corrections for the training sentences. However, it is sufficient to learn the weights: a good decision function should rank the correct hypothesis higher than the incorrect ones, therefore we have −∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 any other hypothesis c. Equivalently, we have −∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 . Then our task is to find a linear classifier such that all the vectors of the form −∑ log𝐶𝐶(𝑠𝑠𝑖𝑖 → 𝑡𝑡𝑖𝑖)𝑟𝑟𝑖𝑖=1𝐶𝐶𝑙𝑙𝑙𝑙 = − log 𝑝𝑝(𝑡𝑡1, … , 𝑡𝑡𝑙𝑙) = − log(𝑝𝑝(𝑡𝑡1) 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) … 𝑝𝑝(𝑡𝑡𝑘𝑘|𝑡𝑡1 … 𝑡𝑡𝑘𝑘−1) … 𝑝𝑝(𝑡𝑡𝑙𝑙| 𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1) )= −(log 𝑝𝑝(𝑡𝑡1) + log 𝑝𝑝(𝑡𝑡2|𝑡𝑡1) + ⋯+ 𝑙𝑙𝑙𝑙𝑙𝑙 𝑝𝑝(𝑡𝑡𝑙𝑙|𝑡𝑡𝑙𝑙−𝑘𝑘+1 … 𝑡𝑡𝑙𝑙−1)= ∑ − log𝑝𝑝(𝑡𝑡𝑖𝑖|𝑡𝑡𝑖𝑖−𝑘𝑘+1𝑖𝑖=𝑙𝑙𝑖𝑖=1 … 𝑡𝑡𝑖𝑖−1)�̂�𝑐 = 𝑎𝑎𝑎𝑎𝑙𝑙𝑎𝑎𝑎𝑎𝑎𝑎𝑐𝑐∈𝐶𝐶 �𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) + 𝑤𝑤0𝑖𝑖 ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(�̂�𝑐) ≥ ∑𝑤𝑤𝑖𝑖𝑓𝑓𝑖𝑖(𝑐𝑐) ∑𝑤𝑤𝑖𝑖(𝑓𝑓𝑖𝑖(�̂�𝑐) − 𝑓𝑓𝑖𝑖(𝑐𝑐)) ≥ 0 belong to the positive class and the opposite vectors—to negative. Then our problem is reformulated as usual linear classification problem and can be solved by any of standard algorithms, such as SV
M or logistic regression.
In our experiments we used the following list of features (
Table 3). When a feature is defined for a single word (say, its capitalization), it means that we sum its values for all the words in the. For example, the unit feature for a single word yields the number of words for the whole sentence.
Feature name Description
F1 Sentence length in words
F2 Error score Cerr
F3 Language model score Clm
F4 Number of corrected words
F5 Number of OO
V words
F6 Number of corrections in OO
V words
F7 Number of corrections in dictionary words
F8 Number of corrections in capitalized words
F9 Number of corrections on edit distance 1
F10 Number of corrections by phonetic similarity
F11 Number of corrections by word lists
F12 Number of 1 → 2 corrections (space insertions)
F13 Number of 2 → 1 corrections (space deletions)
F14 Number of OO
V words having dictionary partitions
F15 Morphological model score
F16 Weighted edit distance score
F17, F18 Semantic model score
F19, F20 Prepositional model score
Automatic spelling correction for Russian social media texts The calculation of features F1–
F14 is straightforward: we just memorize their values for single words in the candidate generation phase and sum these values to obtain the aggregate score. Morphological model score is calculated just as usual language model score, except the n-gram model is built on PO
S tags instead of words. To learn the weights in the edit distance we use the algorithm of : we align each word in the development set with its correction and extract all the groups of up to 3 alignment tokens. The only refinement we made is that a token containing a space symbol on either of its sides is not joined to any longer group.
The features F17–
F20 were not used in the system we submitted for evaluation since they did not improve performance but we describe them for future research. We tried to use cooccurence information in order to grasp semantic relations. For example nothing but semantics can force the system to prefer the correction (2) “мне снится, что мы в ссоре и ты на меня ругаешься и сердишься” instead of (3) “не снится, что мы в море и ты на меня ругаешься и сердишься” for the source sentence мне снится, что мы в соре и ты на меня ругаешься и сердишься (note that the source word is also in the dictionary). To calculate the semantic score we collected a frequency list for dictionary lemmata from a supplementary corpora (by frequency we mean the number of sentences containing the lemma). Then we remove the words occurring more than in 1% of sentences (they are noninformative stopwords). We retain 10,000 most frequent lemmata after this removal and for every such lemma collect the list of lemmata cooccurring with it more than a limited number (say, 10) of times. So, for every word from the list we obtain its potential collocations. Then to calculate the semantic score of the sentence we take as features both the number of words from the list of lemmata occurring in the sentence and the number of collocation pairs between these words.
The aim of the prepositional score is to determine the case of a noun knowing a preposition before it. It is often useful because most of the case flections are on the distance of one edit from each other and often simultaneously appear in the candidate list. When the noun immediately follows the preposition it can be captured by a language model, however, often there are intermediate adjectives or dependent noun phrases between the preposition and the noun. To measure this characteristic we collect the total number of prepositions in the sentence, as well as the number of prepositions which do not have nouns or pronouns of the corresponding case to its right. In future research we plan to use several analogous features, characterizing sentence morphology, such as number of coordinated adjective-noun pairs, subject-verb pairs (using gender and number agreement), as well as the total number of nominative case words and finite verbs in the sentence. However, these features are too noisy when collected from a corpus without morphological disambiguation and we do not have access to disambiguated corpora of sufficient size. Since straightforward addition of such features did not improve performance and even led to slight degradation, we decided not to use them. We rejected from application of morphological and syntactic parsers since there quality on social media texts is moderate especially when these texts contain typos. Therefore the exact role of morphology and semantics in Russian spelling correction is left for future research.
Sorokin A. A., Shavrina T. O.
Evaluating the system
We tested our system in SpellRu
Eval-2016 competition of spelling correctors for Russian social media. The development set of the competition consisted of 2,000 sentences from Russian social media texts together with their corrections. The test set included 100,000 sentences only 2,000 of which were used for testing. We used the development set to tune the parameters of baseline error model (see previous section) used in candidate selection as well as to tune the weighted edit distance. To avoid zero probabilities in Brill
Moore method we added 0.1 to the counts of every symbolto-symbol, symbol-to-space, symbol-to-nothing and nothing-to-symbol corrections, as well as to the counts of each transposition of symbols. Since phonetic similarity corrections such as ться → цца have a high cost in Levenshtein model which leads to noise and outliers in training data, we bound the obtained weighted distance by a fixed number from above. To train the language model we used a supplementary corpus of 5,000,000 sentences (50,000,000 words) obtained from a sample of GIC
R. Since these sample contained lemmata and morpho-tags (though only PO
S tags may be considered as reliable), we also used it to train morphological and semantic models. Our algorithm was implemented in Python language, we used the KenL
M toolkit train the language model and the realization of logistic regression from scikit-learn a linear classifier.
We report the results both for development and test sets. In the development phase we used one half of the set for training and another one for testing. The baseline model in the table before is the model used before the training phase, in the weighted baseline model we train a linear classifier on three features: the number of words, the error model score and the language model score. The Levenshtein model adds as a feature the weighted edit distance, the competition model also uses features F4–
F14 from Table 4 and the morphological model also uses the score of the PO
S-tag n-grams model.
Model Precision Recall F1-measure Sentence accuracy
Baseline 71.68 78.01 74.71 70.70
Weighted baseline 82.83 77.89 80.28 79.40
Levenshtein 87.69 79.15 83.20 81.90
Competition 88.43 81.44 84.79 83.20Competition+
Morpho 88.15 81.79 84.85 83.00
Model Precision Recall F1-measure Sentence accuracy
Baseline 63.11 67.26 65.12 60.06
Weighted baseline 75.55 64.27 69.46 66.93
Levenshtein 80.58 65.94 72.53 68.63
Competition 81.98 69.25 75.07 70.32Competition+
Morpho 81.12 68.98 74.56 70.22
Automatic spelling correction for Russian social media texts Our system won the first place among 7 participants by all the measures: precision, recall, F-measure and accuracy (percentage of correctly recovered sentences). Moreover, already the baseline model is on the par with the system on the second place. We observe that learning weights of different components of baseline model indeed improves its performance consistently, as well as replacing standard edit distance by its weighted version in the error model. Using additional features also improves performance quality by several percents. However, enriching the model with morphological features does not affect performance on the development set and leads to slight degradation on the test set. Note that all the systems except the baseline have higher precision than recall.
5. Results and discussion
Analyzing the mistakes, we have found two main sources of them: the first are space/hyphen errors and the second—real-word errors. For example, in all the 7 cases when the word “еслиб” should be corrected to “если б” (for example, in (4) “страшно представить еслиб с ней что-то случилось”), it was erroneously replaced by “если”. Note that this typo is indeed rather difficult: the system suggestion is also a grammatically correct sentence so language model cannot resolve this ambiguity. Moreover, every word sequence that can follow “если б”, can potentially follow если as well. There only part of our system which can capture such cases is the weighted edit distance model, but its influence is overweighed by other factors.
The second source of errors are real-word errors. In many cases they are in fact grammatical errors like (5) “у этой девченки одни плюсы и не одного минуса”. Sometimes the system cannot select a correct word between two members of a confusion set such as “формации/фармации” in (6) “если говорить точно то эти две фармации исторически противостоящие есть свойства одного”. Though the trigram и ни одного is more frequent than its counterpart и не одного, the cost of correction in a dictionary word не appears to be too high. Real-word errors of the second type could be potentially resolved using cooccurence statistics (“формации” is more likely to appear together with “исторически” than the other variant), probably, using larger corpus to train the language model or more powerful semantic representation like Word2
Vec could help in this case. A minority of errors is also due to incomplete list of informal variations like сення/сегодня or using wrong wordform of a correct lexeme like in (7) “мне было очень страшно казалось что по дороге нам встретиться или тигр или егеря или бандиты”. We plan to deal with grammar errors and real-word errors in a separate study. It is not an obvious question, whether they can be resolved without any handcoding of grammar and morphology rules.
scientific group, SpellRu
Eval-2016 Precision Recall F-measure Accuracy PlaceGIC
R corpora, MS
U 81.98 69.25 75.07 70.32 1 of 7
Sorokin A. A., Shavrina T. O.
is still a large room to improve our current model. First of all, results of that proper usage of morphology and semantics consistently ameliorates performance, which is not the case for our system. It means that PO
S tags alone do not carry enough information for reliable disambiguation and more subtle morphological categories should be taken into account. As we have already said, this hypothesis should be tested on high-quality morphologically annotated corpus of sufficiently large size. Our current model of semantics representation is one of the simplest ones, therefore only usage of more fine-grained one could resolve the question, whether semantic and morphological information could be helpful for Russian social media.
6. Conclusion
We have developed a system for automatic spelling correction for Russian social media texts. We have tested it in the competition of spellcheckers SpellRu
Eval during Dialogue Evaluation-2016, where our system won the first place by all the metrics, reaching the F1
Measure of 75%. We used edit distance together with phonetic similarity to select correction candidates, language model together with error model to score these candidates and linear classification algorithms to rerank them. The features used in the last stage include error model score, weighted Levenshtein distance between the candidate and correction, language model score and several other features like number of corrections in dictionary and non-dictionary words, capitalization, etc. The most straightforward way to improve our system is to use linguisticallyoriented features like morphology, cooccurence and collocation scores, grammatical correctness of the sentence and so on. Since our system is rather simple, we hope it could serve as a baseline for future Russian spelling correction systems. We think it could also be useful in similar tasks like grammar correction or normalization of social media texts. The system can also be successfully applied on big data collections from Russian Web, and is likely to become a part of NL
P-tools used on GIC
R—this gives other researchers the advantages of having corpus with more diverse automatic annotation and better PO
S-tagging and lemmatization.
7. Acknowledgements
Authors of this paper express their sincere thanks to all the students that helped in the annotation of the training set. We also thank Elena Rykunova for her intense assistance in technological support of the web forms and databases for making the golden standard.
Automatic spelling correction for Russian social media texts