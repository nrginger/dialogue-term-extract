Semantic compatibility (term used, for example, in (
Ghomeshi, Massam 1994)) or the ability of two words or constructions to collocate has been widely studied within different theoretical frameworks (
Goldberg 1995), (
Apresjan 1974). The meaning of complex linguistic units such as collocations is generally assumed to be non-compositional so it is not fully derived from the meaning of their parts. Consider Apresjanâ€™s example of Russian adjectives â€˜goryachijâ€™ and â€˜zharkijâ€™ (
Apresjan 2010), both translated in English as â€˜hotâ€™. They are treated as synonyms, although in fact they are not fully interchangeable in contexts as they have virtually non-overlapping sets of collocates. â€˜
Goryachijâ€™ is used to refer to a local sensation (â€˜goryachij shokoladâ€™â€”hot chocolate) and â€˜zharkiyâ€™ expresses the idea of a more general sensation of the environment (â€˜zharkoe letoâ€™â€”hot summer).
Estimating Syntagmatic Association Strength Using Distributional Word Representations Such restrictions can be treated within Construction Grammar (
Goldberg 1995) theory claiming that lexical constructions reveal the unity of form and meaning. The form is maintained by fixed elements of constructions on the one hand, and on the other hand, by selectional (morphosyntactic, lexical-semantic, propositional, etc.) restrictions imposed on the slot fillers. Construction Grammar observes a wide range of variations, from free co-occurrence of lexical features to highly idiomatic units. In our study constructions are treated as multilevel structures combining lexical (lemmata, wordforms), grammatical and semantic features. Such an approach allows us to describe collocability of a target word in a given sense in terms of constructions.
The degree of association in lexical constructions is an important factor in such NL
P applications as paraphrase generation for machine translation, language modelling, automated and semi-automatic dictionary acquisition, semantic role labelling, word sense disambiguation, etc. A number of collocation extraction methods rely on corpus evidence assuming that if a construction occurs in texts, its components can be combined. However, these methods are not applicable when a pair of words is not observed in texts. Moreover, we can imagine occasional word combinations that are not generally expected in natural context (â€˜Ğ´Ñ€ĞµĞ¼ÑƒÑ‡ĞµĞµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ğµâ€™â€”primeval balance).
In our study we compare two approaches to measuring association strength in word combinations revealing possibility of syntagmatic relations, the first one assuming compositionality of their meaning, the second one implying that such word combinations have a meaningful unit which is not derived from word meanings. The paper is organized as follows: first of all, an outline of the research in the field is presented. Then, we describe two approaches to measuring association strength using distributed vector representations. Finally, the performance of the models is evaluated within a pseudodisambiguation benchmark, and in conclusion a brief error analysis is presented.
2. Related work
Recently distributional semantic modelling has been applied to studying meaning of complex linguistic units (constructions, clauses, sentences) with the help of vector space models and their modifications (
Kolb 2008), (
Pekar, Staab 2003), (
Sahlgren 2006), (
SchÃ¼tze 1992), (
Widdows, Cohen 2010), etc. Sem
Eval 2014 competition2 included evaluation of compositional distributional semantic models of full sentences
for English. One of the recent examples is COMPOSE
S which uses compositional operations to model linguistic units in semantic space. â€œ
Contentâ€ words (e.g. nouns) are represented as vectors, while relational words (e.g., adjectives) correspond to functions mapping input items to compositional structures (
Baroni et al. 2014).
A survey on mathematical operations applied to determine compositional meaning is presented in (
Kartsaklis, Sadrzadeh 2013). The authors focus their attention on tensor-based models where relational words (verbs, adjectives) are regarded as tensors. The distributed vector representations (
Mikolov et al. 2013a) are also studied with respect to their compositionality (
Mikolov et al. 2013b).
2 http://alt.qcri.org/semeval2014/task1/
Bukia G. T., Protopopova E. V., Panicheva P. V., Mitrofanova O. A.
models for Russian have been applied in a number of applications. Serelex semantic model is incorporated in an information retrieval system which gets a target word as an input and gives a list of its associates as an output (
Panchenko 2013). It provides contextual correlates for a target word which are ranked according
to an original similarity measure based on lexical-syntactic patterns. The evaluation of various association measures and Russian distributional models has been discussed in RUSS
E competition (
Panchenko et al. 2015). However, semantic relatedness evaluation involves only paradigmatic relations between lexical units. Thus, to our knowledge, there has been no evaluation of vector space models applied to syntagmatic relations in Russian. A recent study concerning association strength measurement in syntactic constructions and testing methodology is described in (
Bukia et al. 2015). The experiments are conducted on syntactic constructions. The authors train association measure on adjective-noun collocations from a very small corpus of 350 thousand sentences. The algorithm yields high performance in predicting association possibility although it is based on a small amount of training data. Their approach is detailed below and compared with association strength evaluation results produced by vector space modelling.
Association strength measurement is closely related to identification of abnormal lexical compositions (
Vecchi et al. 2011) and automatic lexical error detection (
Kochmar, Briscoe 2013). The latter work presents a number of semantic anomaly measures in a vector space. We adopt one of the measures and apply it to a semantic space with reduced dimensionality produced by Word2
Vec.
3. Distributed word representations and their
application to association measurement3.1. Distributed word representations in word2vec toolkit
Continuous word representations in vector space have been gaining extreme popularity since (
Mikolov et al. 2013a). As reported in the paper, high quality word vectors are obtained by training recurrent neural network with two different architecturesâ€”continuous-bag-of-words (CBO
W) and skip-gram. Both yield considerable results in similarity and association measurement when using the cosine similarity measure. The authors implement their approach in a widely used word2vec3 toolkit.
3.2. Distributed word vectors and linguistic regularities
(
Mikolov 2013a) have proposed a questionnaire method (later elaborated in (
Mikolov et al. 2013c)) to estimate word vector representations in terms of semantic and syntactic relationships between words that are learned automatically. The 3 https://code.google.com/archive/p/word2vec/
Estimating Syntagmatic Association Strength Using Distributional Word Representations question is formed of two pairs of words with the same relationship such as â€œ
What is the word (x) that is similar to small(xc ) in the same sense as biggest(xb ) is similar to big(xa )?â€ It turns out that the vectorğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| |is most similar to x in terms of cosine similarity. This was proved on several groups of questions including semantic relationship (the capital of, the currency of, the female of, etc.) and syntactic or, more precise, grammatical ones (past form of, superlative form of, etc.).
The difference of two word vectors characterizes their relation which is independent of their own meanings and can be used to infer the missing word in a different pair representing the same relation.
This observation may be applied to measuring association in syntactic constructions even for word-pairs not attested in the corpus. We assume that if a pair of words comprises a collocation or construction, there is a regular semantic relationship between the two words, which is systematically replicated in other word-pairs attested in the corpus. Thus we can find such a pair of words in the corpus that its difference vector is similar to the corresponding difference vector of the given words. Otherwise, if the combination is unacceptable, the difference vector is unpredictable and does not have similar vectors in corpus. This difference vector often accounts for a relation which can not be formulated clearly but appears regularly in syntactic word combinations. Consider several examples of a noun + adjective combination and the nearest pair:â€¢	 â€˜Ğ¾Ğ²Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ ÑĞ°Ğ»Ğ°Ñ‚â€™ (vegetable salad)â€”â€˜ĞºĞ¾Ğ½Ñ„ĞµÑ‚Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ğ¾Ğ±ĞºĞ°â€™ (a box of sweets), â€˜Ğ³Ğ¾Ñ€Ğ¾Ñ…Ğ¾Ğ²Ñ‹Ğ¹ ÑÑƒĞ¿â€™ (pea soup);â€¢	 â€˜Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ„Ğµâ€™ (black coffee)â€”â€˜Ñ‚Ñ‘Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¸Ğ²Ğ¾â€™ (dark beer), â€˜Ñ€Ğ¾Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ğ°Ñ€Ñ‚Ğ¸Ğ½Ğ¸â€™ (pink martini).
The example sets consist of definitions by content and color respectively.
The association measure for a combination (a,n) is formulated as:ğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| |where the maximum value is found over all pairs (ai, nj ) occurring in the corpus. This measure is referred to as W2
Vrel below.
3.3. Compositional approach to association measurement
We adopt the compositional approach investigated in (
Kochmar, Briscoe 2013), (
Vecchi et al. 2011). The assumption is that the vector representing a noun-adjective composition is meaningful if it is closely related to the head of the composition, i.e. the initial noun. The similarity measure between the composition and the head noun is expected to positively reflect the acceptability of the noun-adjective association. The acceptable compositions are expected to be ranked as more similar to the initial head nouns than the unacceptable ones. However, with normalized vectors, as in Word2
Vec approach, the monotonicity of the functions Similarity1 and Similarity2 is the same, although Similarity2 measures the simple cosine similarity between noun and adjective: Bukia G. T., Protopopova E. V., Panicheva P. V., Mitrofanova O. A.
= maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| |
Quantifying similarity between the noun and the adjective in the same vector space yields here the same result as when quantifying similarity between the initial noun and the attributive noun phrase. The latter formula is linguistically motivated and naturally interpreted, which is not so obvious about the former. These values will be referred to as Comp below.
3.4. Count-based distributional approach
We compare the described methods to an approach proposed in (
Bukia et al. 2015). Their association measure is based on distributional word properties concerning only a fixed construction, namely, the noun-adjective association (referred to as D below). The basic assumption is that if two words relevant for a construction slot collocate in texts with similar words (contexts) relevant for another slot, the probability of the first target word to be combined with the contexts of the second target word and, vice versa, is high, even when some pairs are not observed in texts. This idea is formally expressed by the notion of confusion probability, which is computed as follows: given the contexts of the first word ğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| | and the second one ğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| |, their confusion probability is equal to P: ğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| | The association strength between two words in a collocation occurring in a corpus is usually computed by means of Fisherâ€™s exact test (
Stefanowitsch 2003). The distributional association measure between a noun and an adjective in a collocation D(a,n) is then defined as an average of such counts over all confusable words weighted by the confusion probability. As discussed in (
Bukia et al. 2015), the highest results are produced by such a measure based on mutual information (M
I) counts.
4. Evaluation
4.1. Data and experimental setup
We use a corpus of Russian fiction (146
M tokens obtained from M. Moshkovâ€™s digital library, UR
L: lib.ru). All preprocessing (tokenization, lemmatization, shallow morphological analysis) was performed by means of Py
Morphy2 Python library (UR
L: http://pymorphy2.readthedocs.org/en/latest/). About 157
K (80
K unique) adjectivenoun pairs were extracted from these texts. The 150-dimensional vectors were trained using Gensim library (
Rehurek, Sojka 2010) with skip-gram architecture and 4-word window. The count-based distributional
association takes into account only corpus frequencies of a noun, an adjective and their Estimating Syntagmatic Association Strength Using Distributional Word Representations combination. The evaluation procedure follows pseudo-disambiguation test as described in (
Bukia et al. 2015). It has been also used in (
Pekar 2004), (
Tian et al. 2013).
The following lemmata were extracted from the corpus:â€¢	 500 random nouns ğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| |;â€¢	 for each noun a random adjective ai collocating with this noun;â€¢	 for each a the nearest adjective by frequency ğ‘Šğ‘Š2ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ = maxğ‘ğ‘ğ‘–ğ‘–,ğ‘›ğ‘›ğ‘—ğ‘—âˆˆ ğ¾ğ¾< ğ‘›ğ‘›,ğ‘›ğ‘›ğ‘–ğ‘– âˆ’ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘ >|ğ‘›ğ‘›||ğ‘›ğ‘›ğ‘–ğ‘– â€“ ğ‘ğ‘ğ‘–ğ‘– + ğ‘ğ‘| ,ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†1(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› + ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘ğ‘ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†2(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›, ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)â„™ {ğ‘¥ğ‘¥1 âˆ¼ ğ‘¥ğ‘¥2} = |ğ‘ğ‘(ğ‘¥ğ‘¥1) âˆ© ğ‘ğ‘(ğ‘¥ğ‘¥2)|2|ğ‘ğ‘(ğ‘¥ğ‘¥1) ğ‘ğ‘(ğ‘¥ğ‘¥2)|ğ‘†ğ‘† = ğ‘¥ğ‘¥ğ‘ğ‘ âˆ’ ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘¥ğ‘¥ğ‘ğ‘ğ‘ğ‘ = {ğ‘›ğ‘›ğ‘–ğ‘–}ğ‘‹ğ‘‹ = {ğ‘¥ğ‘¥ğ‘–ğ‘–}| | (not attested in combination with the corresponding noun ni).
All combinations (ai,nj) are then removed and the system is trained on the rest of the corpus. Thus, 500 triplets consisting of a target noun, an acceptable and an unacceptable combination are obtained. The task is to find out, which combination of an adjective and a noun was removed, i.e. which one is acceptable but deleted from the final training corpus. In our case, the first association value is expected to be higher than the second one.
4.2. Results
It should be noticed that the pseudo-disambiguation task is limited by the fact that we do not know anything about the second combination not attested in the corpus. Thus, the results were manually checked in order to eliminate malformed triplets. In some cases, either both combinations are acceptable or even none of the chosen ones.
The accuracy counts are presented in Table 1 in the following order:â€¢	 raw result based on the assumption that the first possible pair is acceptable while the second one is not (
Acc);â€¢	 pseudo-disambiguation accuracy calculated after manual annotation of triplets (
Corr).
As mentioned above, the models are denoted as follows:â€¢	 W2
Vrel for vector difference based measure (
Section 3.2);â€¢	 Comp for measure based on vector composition (
Section 3.3);â€¢	 D for a simple distributional measure (
Section 3.4).
First of all, it should be noticed that the models based on word embeddings achieve higher accuracy than a count-based one. However, the latter one has an important advantage: its results are easy to implement and interpret.
Secondly, the results presented below should be compared only with the data provided by the models performing the same task: estimating association strength for unseen combinations. The most recent work (
Tian et al. 2013) based on quite different principles reports 88% accuracy. Thus, the discussed models yield state-of-the art performance. W2
Vrel Comp D
Acc 76% 81% 75%
Corr 88% 93% 84%
Bukia G. T., Protopopova E. V., Panicheva P. V., Mitrofanova O. A.
Error analysis
After manual error annotation the errors of different models are compared. Common errors, i.e. shared by all three methods, can be divided into two groups concerning their source: acceptable combinations representing rare or occasional metaphorical expressions (â€˜Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‡ÑƒĞ¼Ğ°â€™â€”informational boom, â€˜ĞºÑ€ÑƒĞ³Ğ»Ğ°Ñ ÑĞ¸Ñ€Ğ¾Ñ‚Ğ°â€™â€”a total (literally â€˜roundâ€™) orphan, etc.) and those containing a word with a vague or general meaning (â€˜ĞµĞ²Ñ€Ğ¾Ğ¿ĞµĞ¹ÑĞºĞ¸Ğ¹ ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»â€™â€”european quarter, â€˜ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾Ğµ ÑƒÑ…Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµâ€™â€”earnest courting, etc.).
The rest of the errors, i.e. the model-specific ones, were ordered by the acceptable combination frequency. In each experiment a group of very rare (acceptable) combinations can be found: â€˜ÑÑƒĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½â€™â€”superstitious law, â€˜Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ½Ğ°Ñ‚Ğ¸Ğºâ€™â€”unemployed fanatic. These expressions are either rare themselves or contain a rare word, and even a native speaker is scarcely able to construct a sentence where these combinations are justified. The top combinations are constructions in the sense of Construction Grammar (
Goldberg 1995): their meaning is not additive and can not be simply derived from the meaning of the constituents: â€˜Ğ¶ĞµĞ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ·Ğ¸Ğ½ĞºĞ°â€™â€”chewing gum, â€˜Ñ‚Ñ€ĞµĞ·Ğ²Ğ°Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°â€™â€”reasonable person (literally â€˜sober headâ€™).
The middle of these ordered lists contains real errors which are due to an algorithm structure or its assumptions: â€˜ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ†ĞµĞ½Ñ‚Ñ€â€™â€”copy center, â€˜ÑÑƒĞ¼Ğ°ÑÑˆĞµĞ´ÑˆĞ°Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñâ€™â€”mad story. These combinations are less idiomatic and are constructed regularly. The Word2
Vec compositional similarity measure (section 3.3) fails to extract such combinations because the constituents appear to have too few intersecting contexts. The errors of the count-based distributional model may also be explained by the underlying assumption that similar words occur in similar noun-adjective contexts. On the other hand, such expressions are correctly processed by the Word2
Vec relative measure (see section 3.2), meaning that analogous regular relations between attested nouns and adjectives were observed when looking for the nearest difference vector. Several examples are presented in test combination nearest difference combinationĞ¶ĞµĞ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ·Ğ¸Ğ½ĞºĞ°â€”chewing gum ĞºĞ°Ğ²ĞºĞ°Ğ·ÑĞºĞ¸Ğ¹ Ñ…Ñ€ĞµĞ±ĞµÑ‚â€”
Caucasian chainÑ†ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑÑ‚ÑƒĞ¿ĞµĞ½ÑŒĞºĞ°â€”cement stepĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ†ĞµĞ½Ñ‚Ñ€â€”copy center Ğ¿Ğ°Ñ‚Ñ€ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ°Ğ±Ğ»ÑŒâ€”patrol ship
Finally, it should be mentioned that although Word2
Vec compositional measure has shown the best results, it can not be improved, because its assumption is not applicable in all real world cases. The accuracy of Word2
Vec relative measure, on the contrary, can be increased, since the core idea of an additional meaning can be observed in real data.
Estimating Syntagmatic Association Strength Using Distributional Word Representations 5. Conclusion
We have applied the task of measuring association strength between Russian nouns and adjectives to compare compositional and relative Word2
Vec semantic models with a simple distributional association measure. The test was conducted following a conventional pseudo-disambiguation methodology. The models were trained with a 11
M sentences corpus where all in-sentence co-occurrences of the word pairs are deleted.
Both measures based on Word2
Vec models outperformed a simpler count-based one and achieved state-of-the-art accuracy. The error analysis allows us to talk about future improvements by applying a more sophisticated measure to determine a syntagmatic relation. More exactly, we are going to focus on the interpretation of the difference vector. Another important concern is the testing methodology which is also subject to future investigation and improvement based on human judgements. For example, separate datasets for compositional and idiomatic combinations should be created and manually assessed.
