In this work, we endeavor into outlining statistically the limits of applicability of popular contemporary language models. To avoid any terminological doubt, when we write â€œmodels of the languageâ€, we refer to any models that explain some linguistic phenomena, while â€œlanguage modelsâ€ refer to probabilistic Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference â€œ
Dialogue 2023â€
June 14â€“16, 2023
Autocorrelations Decay in Texts and Applicability Limits of Language Modelslanguage models as defined in Subsection 2.3 Probabilistic Language Models. While not long ago probabilistic language models were just models that assign probabilities to sequences of words , now they are the cornerstone of any task in computational linguistics through few-shot learning , prompt engineering fine-tuning . On the other hand, current language models fail to catch long-range dependencies in the text consistently. For example, text generation with maximum likelihood target leads to rapid text degeneration, and consistent text generation requires probabilistic sampling and other tricks . Large language models such as GP
T-3 the boundary of â€œshort textâ€ rather far (specifically, to 2048 tokens), but do not remove the problem. Our contributions in this work are the following: ï‚· We explain how the laws of autocorrelations decay in texts are related to applicability of language models to long texts; ï‚· We pioneer the use of pretrained word vectors for autocorrelation computations that allows us to study a widest range of autocorrelation distances; ï‚· We show that the autocorrelations in literary texts decay according to power laws for all these distances; ï‚· We show that distributional semantics typically provides coherent autocorrelations decay exponents for texts translated to multiple languages, unlike earlier flawed approaches; ï‚· We show that the behavior of autocorrelations decay in generated texts is quantitatively and often qualitatively different from the literary texts. 2 Models of the Language
In this section, we briefly introduce models of the language that are important for the further considerations. 2.1 Formal Grammars
Formal grammars describe how to form strings from a language's alphabet that are valid according to the language's syntax. They were introduced by Chomsky in 1950s . A formal grammar consists of a finite set of production rules in the form ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ âˆ’ â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ğ‘ğ‘™ğ‘™ â†’ ğ‘Ÿğ‘Ÿğ‘ ğ‘ ğ‘Ÿğ‘Ÿâ„ğ‘™ğ‘™ âˆ’ â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ğ‘ğ‘™ğ‘™, (1) where each side consists of a finite sequence of the following symbols: ï‚· a finite set of nonterminal symbols (indicating that some production rule can yet be applied) ï‚· a finite set of terminal symbols (indicating that no production rule can be applied) ï‚· a start symbol (a distinguished nonterminal symbol) Chomsky grammars constitute a hierarchy, see inclusion of lower class grammars to higher ones, now there are several types of grammars known to fall between or partially overlap with the original classes (see, for example, ). 2.2 Distributional Semantics and Models
Distributional hypothesis assumes that linguistic items with similar distributions have similar meanings or function and was likely first introduced by Harris 1954 and was popularized in the form "a word is characterized by the company it keeps" by Firth . The basic idea is to collect distributional information in, say, high-dimensional vectors, and then to define similarity in terms of some metric, say Euclidean distance or the angle between the vectors. Mikhaylovskiy N., Churilov I.
Early distributional approaches from 60s relied on hand-crafted features of the words , while more recent â€“ on statistics of varied sorts. The first generation of statistical distributional semantics approaches included Latent Semantic Analysis (LS
A) , Hyperspace Analogue to Language (HA
L) , and many others, see a review. The second generation primarily consists of word2vec Glo
Ve the first, implicitly, and the second, explicitly adding the word analogy task into the training objective, so that similar relationships between words should be described by similar difference vectors between embeddings. The third generation of statistical distributional semantics models was started by emergence of BER
T contextual word embeddings . BER
T have combined the word and its current context into a single vector embedding and used Masked Language Modelling training objective. A lot of recent work sprouted from BER
T. 2.3 Probabilistic Language Models
Probabilistic language models consider sequences ğ‘¡ğ‘¡1:ğ‘šğ‘š = {ğ‘¡ğ‘¡1, ğ‘¡ğ‘¡2, â€¦ , ğ‘¡ğ‘¡ğ‘šğ‘š} (2) of tokens from the lexicon â„’. An autoregressive language model estimates the probability of such a sequence = ğ‘ƒğ‘ƒ(ğ‘¡ğ‘¡1)ğ‘ƒğ‘ƒ(ğ‘¡ğ‘¡2|ğ‘¡ğ‘¡1)ğ‘ƒğ‘ƒ(ğ‘¡ğ‘¡3|ğ‘¡ğ‘¡1:2) â€¦ ğ‘ƒğ‘ƒ(ğ‘¡ğ‘¡ğ‘šğ‘š|ğ‘¡ğ‘¡1:ğ‘šğ‘šâˆ’1) = âˆ ğ‘ƒğ‘ƒ(ğ‘¡ğ‘¡ğ‘˜ğ‘˜|ğ‘¡ğ‘¡1:ğ‘˜ğ‘˜âˆ’1)ğ‘šğ‘šğ‘˜ğ‘˜=1 (3) using the chain rule. Most models introduce the Markov that the probability of a token depends on the previous ğ‘›ğ‘› âˆ’ 1 tokens only, thus approximating (3) with a truncated version â‰ˆ âˆ ğ‘ƒğ‘ƒ(ğ‘¡ğ‘¡ğ‘˜ğ‘˜|ğ‘¡ğ‘¡ğ‘˜ğ‘˜âˆ’ğ‘›ğ‘›+1:ğ‘˜ğ‘˜âˆ’1)ğ‘šğ‘šğ‘˜ğ‘˜=1(4) While the language models based on recurrent , and specifically, LST
M networks do not introduce the Markov assumption explicitly, we will shortly see that in practice they do exhibit Markovian behavior. On the other hand, it is long known that Markov models describe stochastic regular grammars . 3 Why Autocorrelations Decay Laws Matter?
In this section we explain why autocorrelation decay laws matter a lot to computational linguisticsâ€™ nearfuture. 3.1 Computing Autocorrelations Using Distributional Semantics
Suppose we have a sequence of ğ‘ğ‘ vectors ğ‘‰ğ‘‰ğ‘–ğ‘– âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘, ğ‘–ğ‘– âˆˆ . Autocorrelation function ğ¶ğ¶(ğœğœ) is the average similarity between the vectors as a function of the lag ğœğœ = ğ‘–ğ‘– âˆ’ ğ‘—ğ‘— between them. The simplest metric of vector similarity is the cosine distance ğ‘‘ğ‘‘(ğ‘‰ğ‘‰ğ‘–ğ‘–, ğ‘‰ğ‘‰ğ‘—ğ‘—) = cosâˆ (ğ‘‰ğ‘‰ğ‘–ğ‘–, ğ‘‰ğ‘‰ğ‘—ğ‘—) = ğ‘‰ğ‘‰ğ‘–ğ‘–âˆ™ğ‘‰ğ‘‰ğ‘—ğ‘—â€–ğ‘‰ğ‘‰ğ‘–ğ‘–â€–â€–ğ‘‰ğ‘‰ğ‘—ğ‘—â€–, where âˆ™ is a dot product between two vectors and â€– â€– is an Euclidean norm of a vector. Thus, ğ¶ğ¶(ğœğœ) = 1ğ‘ğ‘ âˆ’ ğœğœ âˆ‘ ğ‘‰ğ‘‰ğ‘–ğ‘– âˆ™ ğ‘‰ğ‘‰ğ‘–ğ‘–+ğœğœâ€–ğ‘‰ğ‘‰ğ‘–ğ‘–â€–â€–ğ‘‰ğ‘‰ğ‘–ğ‘–+ğœğœâ€–ğ‘ğ‘âˆ’ğœğœğ‘–ğ‘–=1. (5) ğ¶ğ¶(ğœğœ) ranges from âˆ’1 for perfectly anticorrelated sequence (for ğœğœ = 1 and ğ‘‘ğ‘‘ = 1 that would be 1, âˆ’1, 1, âˆ’1 etc.) to 1 for a perfectly correlated one (for ğœğœ = 1 and ğ‘‘ğ‘‘ = 1 that would be 1, 1, 1, 1 etc.).
A distributional semantic assigns a vector to each word or context in a text. Thus, a text is transformed into a sequence of vectors, and we can calculate an autocorrelation function for the text. Autocorrelations Decay in Texts and Applicability Limits of Language Models3.2 Transformer Language Models Exhibit Markovian Behavior
In this paper, by Markovian behavior, we mean that large language models actually use only a limited context, often significantly less than the maximum context possible. Thus they implicitly or explicitly use the Markov assumption. Two separate phenomena classes that prove that transformer language models exhibit Markovian behavior are known, and in Section 5.5 we introduce the third one. One such phenomenon is the rapid text degeneration when a transformer language model is used to generate text with maximum likelihood target . Maximization-based decoding methods such as beam search lead to output text that is bland, incoherent, or gets stuck in repetitive loops are extremely reminiscent of positively recurrent Markov chains (see Figure 1). The other phenomenon is studied in detail in . The authors have found that the networks roughly match the computational models associated with the Chomsky hierarchy: RN
Ns can solve tasks up to the regular level, Stack-RN
Ns up to the DC
F level, and Tape-RN
Ns up to the C
S level. Finally, they observed that Transformers and LST
Ms are less aligned with the Chomsky hierarchy: Transformers fail on regular tasks, while LST
Ms can solve tasks more difficult than regular. The results of summarized in describe stochastic regular grammars , we can safely say that transformers exhibit behavior no richer than regular. 3.3 Markovian Implies Exponential Correlations Decay, Probabilistic Context
Free Grammars
Can Generate Power Laws Assume that the sequence (2) is an output of a random source that takes values in â„’. If the source is Markovian, it can be shown the autocorrelations (or, equivalently, mutual information between chunks of the text) decay exponentially. Namely, the following theorem holds: Theorem 1 (). Let ğ‘€ğ‘€ be a Markov matrix that generates a Markov process. If ğ‘€ğ‘€ is irreducible and aperiodic, then the asymptotic behavior of the mutual information ğ¼ğ¼(ğ‘¡ğ‘¡1, ğ‘¡ğ‘¡2) is exponential decay toward zero for |ğ‘¡ğ‘¡2 âˆ’ ğ‘¡ğ‘¡1| â‰« 1 with decay timescale log 1|ğœ†ğœ†2| , where ğœ†ğœ†2 is the second largest eigenvalue of ğ‘€ğ‘€. If ğ‘€ğ‘€ is reducible or periodic, ğ¼ğ¼ can instead decay to a constant; no Markov process whatsoever can produce power law decay. N., Churilov I.
One the other hand, the following theorem holds: Theorem 3 (). There exist a probabilistic context-free grammar such that the mutual information ğ¼ğ¼(ğ´ğ´, ğµğµ) between two symbols ğ´ğ´ and ğµğµ in the terminal strings of the language decay like ğ‘‘ğ‘‘âˆ’ğ‘˜ğ‘˜, where ğ‘‘ğ‘‘ is the number of symbols between A and B. 3.4 If the Natural Language Exhibits Power Law Correlations Decay, We Can Do Better Than
Autoregressive Language Models Summarizing the above, if texts in the natural languages exhibit exponential autocorrelations decay, autoregressive language models are good to analyze or generate texts of any length. On the other hand, if texts in the natural languages exhibit power law autocorrelations decay, building language models that exhibit at least hierarchical, context-free-grammar-ish, slow-correlation-decay behavior may be beneficial for a variety of downstream tasks. This may be not enough to model long texts successfully because natural languages cannot be completely described by context-free grammars (see, for example, ), but may be a meaningful step. 4 Studying Autocorrelations Decay Laws in Texts
4.1 Prior Research
Schenkel, Zhang, and Zhang likely the first to empirically find the power law autocorrelations decay in texts using a random walk model with an arbitrary mapping of characters to fixed length, 5 bit sequences. They studied 10 texts in English. The obvious drawback of their approach is dependency on encoding. Amit et al. this problem in various translations of the Bible and have shown that the power law exponent depends on both the language and the codification. Testing multiple random mappings would provide a more reliable estimate of power law exponents, but such a research is a matter of future. Random walk models have later been used to find the power law in text by several researchers, including Ebeling and Neiman , Kokol et al. by the way, in our opinion have not found power-law autocorrelations in literary writing on distances studied, but found power-law autocorrelations in computer programs, in a perfect agreement with the fact that computer programs are described by context-free grammars), Pavlov et al. , who find multifractal structures in the text, and Manin , who attribute long-range correlations to slow variations in lexical composition within the text. Alvarez
Lacalle et al. a version of first-generation distributional semantic model to study autocorrelations in 12 literary texts in English to find power law autocorrelations decay. Altmann, Cristadoro, and Degli 41 binary functions on words separately on ten English versions of international novels. They separate the effects of burstiness and long-range correlations in the power spectrum and find a power law correlations decay. Lin and Tegmark a short empirical part of their study use three text corpora: 100 M
B from Wikipedia, the first 114 M
B of a French corpus and 27 M
B of English articles from slate.com. They observe the power law decay of mutual information, but note that the large portion of the long-range mutual information appears to be dominated by poems in the French sample and by the html-like syntax in the Wikipedia sample. They have also shown similar power decay laws for autocorrelations in natural music and exponential laws in generated music, the result reproduced by different means by Yamshchikov and Tikhonov . Corral et al. intervals between consecutive appearances of specific words in literary texts in 4 languages, including Finnish (a rare study of highly agglutinative language) to find that most words have a universal dimensionless probability density function described by gamma distribution. Gillet and Ausloos Montemurro and Pury sequences built from word frequencies and word lengths to find the power law autocorrelations decay. 4.2 Research Questions
Given the prior art, many research question remain unanswered. The ones we address in this work are: Q1. How accurately can we say that autocorrelations in texts decay according to a power law? Q2. Can we reject the hypothesis of exponential decay of correlations? Q3. Does the law of decay depend on the language of the text? Q4. Over what range of distances does the decay in autocorrelations follow a power law? Autocorrelations Decay in Texts and Applicability Limits of Language Models
Q5. Are autocorrelations in L
M-generated texts any different from literary texts? 4.3 Methods
In this work we use two distributional semantic models to estimate autocorrelations in long texts. One is a bag-of-words (BO
W) embedding model of Alvarez
Lacalle et al. . The other distributional semantic model we use is Glo
Ve . We use pretrained multilingual Glo
Ve vector embeddings from . We filter out both frequent and rare words filtering similarly to using BO
W. BO
W assigns a vector of dimension ğ‘‘ğ‘‘ to each word first, and then averages these vectors over a window of the size ğ‘ğ‘. This averaged vector is then assigned to a word in the center of averaging window. The exact procedure for BO
W is described in detail in . Glo
Ve naturally maps each word to a vector; we then center the vector system by subtracting the average of vectors over the whole text, and, similarly, average over a window of the size ğ‘ğ‘ when we need direct comparison to BO
W. After that in both cases we can compute the autocorrelation function following Section 3.1. 5 Experiments
5.1 The Dataset
For our studies we have collected a dataset of long literary and philosophical works in English, Spanish, French, German and Russiani each: Critique of Pure Reason, Don Quixote de la Mancha, Moby
Dick or, The Whale, The Adventures of Tom Sawyer, The Iliad, The Republic and War and Peace. The only translation absent is Moby
Dick in German, which happened to be substantially abridged. The texts have been obtained from Project Gutenberg, Wikisource, Royallib and lib.ru and preprocessed so as to fit our research purposes: ï‚· copyright texts were removed from the files; ï‚· author and translator notes were removed; ï‚· table of contents and any indices were removed, except for the table of contents from Don Quixote; ï‚· any links to illustrations have been removed; ï‚· in the Russian version of War and Peace any non
Russian text have been replaced with Russian translations; ï‚· etymology section was removed from Moby
Dick or, The Whale, where encountered, as some languages missed it. 5.2 Choosing Between Hypotheses of Power Law and Exponential Decay of Correlations
To address Q1. â€œ
How accurately can we say that autocorrelations in texts decay according to a power law?â€ and Q2. â€œ
Can we reject the hypothesis of exponential decay of correlations?â€ for each text, we log-log coordinates. Right: log-linear coordinates. Mikhaylovskiy N., Churilov I.have computed autocorrelations for a series of distances ğœğœ = ğ‘›ğ‘› âˆ— 10ğ‘˜ğ‘˜, ğ‘›ğ‘› âˆˆ and approximated the points produced by a straight line in both log-log and log-linear coordinates using the least squares regression. We have evaluated the goodness of fit of each model by MAP
E (
Mean Absolute Percentage Error). The range of ğœğœ for Glove was chosen from the first non-negative autocorrelation value Îµ (autocorrelations on small distances ğœğœ = to be sometimes negative). The results for the English translation of Don Quixote are presented in the Figure 2. It can be seen that in log-log coordinates the regressed straight line approximates data well enough, unlike log-linear coordinates. Table 3 lists the MAP
E metrics of goodness of fit of autocorrelation by power and exponential laws (the smaller the better). It can be easily seen that for all the texts but one the hypothesis of exponential decay of correlations can be rejected. The peculiarity of the French translation of The Iliad is that the autocorrelation with ğœğœ = 1 is very small but still positive, thus both producing significantly larger MAP
E and ruining the approximation. Additional graphs are presented in the Appendix A. 5.3 Determining the Dependency of the Autocorrelations Decay Law on the Language of the
Text To study the dependency of the autocorrelations decay law on the language of the text, we have measured ğ¶ğ¶(ğœğœ) for the same multilingual dataset as in Section 5.1 and fitted with power law ğ¶ğ¶(ğœğœ) = ğ›½ğ›½ âˆ™ ğœğœğ›¼ğ›¼. Table 4 presents results for Don Quixote. It can be easily seen that the parameters of power law, as well as the accuracy of the approximation are extremely consistent among languages for both embeddings, with standard deviation of exponent being 7% for BO
W and 10% for Glo
Ve. Moreover, the exponents for BO
W and Glo
Ve are also consistent within 15%, which we consider a very good agreement. This is in a stark contrast with the results from critically depend on the codification and language. Law Exponential Law
W en fr es ru en BO
W en fr es ru en The Adventures of Tom Sawyer 0,16 0,11 0,16 0,14 0,21 0,52 0,32 0,33 0,33 0,55 The Republic 0,21 0,15 0,09 0,10 0,13 0,58 0,28 0,25 0,31 0,38 Don Quixote 0,20 0,11 0,12 0,09 0,20 0,66 0,24 0,22 0,23 0,44 War and Peace 0,20 0,13 0,11 0,08 0,09 0,54 0,24 0,24 0,28 0,42 Critique of Pure Reason 0,09 0,07 0,15 0,10 0,14 0,27 0,17 0,20 0,21 0,25 The Iliad 0,24 2,37 0,16 0,10 0,19 0,63 2,33 0,17 0,19 0,54 Moby
Dick or, The Whale 0,14 0,12 0,11 0,09 0,15 0,40 0,22 0,22 0,22 0,47 BO
W: a=200, d=100, ğœğœ âˆˆ a = 1, d = 300, ğœğœ âˆˆ
W Glo
Ve ğ›½ğ›½ MAP
E ğ›¼ğ›¼ ğ›½ğ›½ MAP
E en -0.7718 0.9545 0.1054 -0.7246 1.1582 0.1044 fr -0.8836 1.1407 0.2154 -0.7749 1.1051 0.2150 es -0.7601 0.9332 0.1057 -0.7083 0.9947 0.1271 ru -0.7412 0.7874 0.0787 -0.6431 0.9173 0.0548 de -0.8072 0.9542 0.1411 -0.8326 1.3478 0.1657 and embedding. ğœğœ ranges from 200 to 4000 words, d=300, a = 200 Decay in Texts and Applicability Limits of Language Models5.4 Determining the Range of Distances Where the Decay in Autocorrelations Can Be
Considered Subject to a Power Law As the BO
W approach requires a sufficiently large window size ğ‘ğ‘, we have studied the dependence of autocorrelations on distance ranges using Glo
Ve embeddings with a window size ğ‘ğ‘ = 1. For each text we explored all the ranges of ğœğœ spanning at least a decimal order of magnitude, and fitted the autocorrelations with the best fitting log, power and exponential functions. We then mapped the differences between MAP
E of power and other approximations, as well as the ranges where each function fits the data the best. The results for the Critique of Pure Reason in English and The Adventures of Tom Sawyer in Spanish are presented on Figure 3. Each small square on these images corresponds to a range of ğœğœ determined by its vertical (start) and horizontal (end) coordinates, for example, the full range of ğœğœ âˆˆ corresponds to the top right corner. Additional graphs are presented in Appendix B. It can be seen that for the shorter spans of ğœğœ the best approximations are sometimes logarithmic or exponential but their advantage is not significant, while for the longer ranges the best approximations are always power law. Additionally, the location of such ranges is hectic. We conclude that the cases where exponential or logarithmic approximation is better than the power law approximation represent natural short-range variability and cannot be considered a regularity. 5.5 Autocorrelations in Generated Texts
The behavior of autocorrelations is qualitatively different when the text is generated. The simplest way to generate an incoherent text is to shuffle words in a text. autocorrelations decay law for an incoherent text. To study autocorrelations in texts generated by large language models, we have used GP
T-2 base the default generation parameters, and Structured State Space model S4 base the default generation parameters, and generated some 10
K word continuous text with each model. The generated texts are listed in Appendix C and Appendix D, respectively. We then performed the same procedure as Sawyer in Spanish (bottom) computed using Glo
Ve, ğ‘ğ‘ = 1, ğ‘‘ğ‘‘ = 300. Vertical axis: start of ğœğœ range. Horizontal axis: end of ğœğœ range. Left: difference between power and log approximation MAP
E. Middle: difference between power and exp approximation MAP
E. Right: ranges where power (blue), exp (gray), and log (green) approximations are the best. Mikhaylovskiy N., Churilov I.in Section 5.4, mapping ranges where each decay law provides the best approximation. The results are presented on Figure 4. The autocorrelations decay in an exponential manner in the text generated by S4 model, while according to a power law on long distances and to log law â€“ on short distances in the text generated by GP
T-2. The autocorrelations in generated texts are significantly larger and decay much slower than the ones in the natural texts. In our S4 and GP
T-2 generated examples, the power law coefficients are ğ‘ğ‘ = âˆ’0.045, ğ‘ğ‘ = âˆ’0.71 and ğ‘ğ‘ = âˆ’0.027, ğ‘ğ‘ = âˆ’0.77, respectively. At the same time we have not seen the coefficient a less than 0.1 for any natural text in English we have studied, and the average is closer to 0.2, indicating almost 10-fold gap between the power law decay rates in natural and generated texts. Typical values of coefficient b for natural texts are between -1.5 and -2, indicating at least 2-fold gap between natural and generated texts. Thus we can say that the autocorrelations decay in generated texts are quantitatively and often qualitatively different from the literary texts. The conditions that influence the autocorrelations decay laws in generated texts may include sampling approach, temperature and other hyperparameters. This is a matter of future research. Glo
Ve, ğ‘ğ‘ = 1, ğ‘‘ğ‘‘ = 300, ranges where power (blue), exp (gray), and log (green) approximations are the best depicted. Vertical axis: start of ğœğœ range. Horizontal axis: end of ğœğœ range. using Glo
Ve, a=1, d=300. Left: log-log, to right: log-linear coordinates Decay in Texts and Applicability Limits of Language Models6 Conclusions
We have shown empirically that autocorrelations in literary texts are decaying following the power law with the only upper limit being the length of the work itself and the hypothesis of exponential decay can be rejected for these distances. We have also shown empirically that the laws of autocorrelation decay, if measured using distributional semantics models are typically the same for the literary work translated to different languages. This contrasts previous findings that used flawed technique based on encodingdependent random walks. Thus, we believe that distributional semantics models are a robust enough tool to measure autocorrelations in long texts. The autocorrelations decay in generated texts is quantitatively and often qualitatively different from the literary texts. Based on the above, we can conclude that for long text processing one may need architectures different from the autoregressive ones, and many questions remain unanswered. Acknowledgements The authors are grateful to their colleagues at NT
R Labs M
L division for the discussions and support. Early versions of this work were discussed with Anton Kolonin, Dmitry Manin and Alexey Tikhonov. These discussions have improved our approach and research design, for which we are very grateful. We are also extremely grateful to Tatiana Sherstinova who discussed early versions of this work, suggested numerous improvements and provided a webinar venue at HS
E to discuss this work publicly.