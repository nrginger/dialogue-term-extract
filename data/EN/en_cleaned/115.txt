Distributed word representations, or word embeddings, have already shown their power as a basis for efficient model training within the scope of neural-network approach in various natural language processing tasks. In addition to this primary mission, word embeddings are widely and successfully applied to development of solutions that do not necessarily use neural nets as their core component; e.g. contextual information encoded in the distributed representations can be used for word similarity estimation and in related problems.
Another potential application of word embeddings resides in automated wordlevel grammatical and semantical information extraction. This set of tasks is itself quite interesting for linguists: measuring the correlation between contexts of the word and its internal sense, and determining the limits of distributional approach are two questions that are still open and should be investigated broader. Moreover, such tasks can be seen as auxiliary for more complex ones. One can consider, for example, the following situation: vast amount of text available on the Web can be exploited in a variety of linguistic studies provided it is properly and fully labeled in accordance with the specific task orientation. Availability of automated word labeling methods for text corpora is thus the condition for future linguistic research.
In this paper, we investigate applicability of distributed word representations to prediction of several classifying grammemes of Russian words. In particular, we consider animacy of nouns and transitivity of verbs in the Russian language, since concrete values of these grammatical categories can hardly be predicted judging by appearance of words and morphemes that constitute them. We expect that good performance of automatic classification in the aforementioned tasks may open the way to extension of the approach into other related problems.
We propose to utilize real-valued vectors obtained from distributed representation models as features in these prediction tasks, which may lead to a scheme of grammeme prediction on a basis of insufficiently labeled corpora. We explore power of several widely used word-embedding algorithms and train models with different sets of parameters in order to achieve better performance. Additional investigation concerns testing the dimensionality reduction technique proposed recently () for enhancing word embeddings in applied tasks. Based on the experimental results, we argue that our approach is feasible for prediction of grammatical characteristics of Russian words.
2. Related work
The task of automated grammeme prediction is closely related to a more general problem of automated corpus annotation and, more specifically, to automated grammatical tagging. A classic work in the field is , where the authors utilize a stochastic algorithm to complete the first stage of two-staged tagging process, the second being manual correction of errors produced by the automatic stage. The method and a number of related ones (, ) are based on complex models with a large number of parameters to be tuned in order to achieve good performance; this may be an encumbrance in the case of small corpus on annotated data.
Exploiting Russian Word Embeddings for Automated Grammeme Prediction State-of-the-art techniques of automatic grammatical tagging mostly focus on overcoming the obstacle of insufficient amount of data available for model training. This is important, among other things, for developing automated tagging systems for languages lacking high-quality text resources and corpora on the Web. The authors of to use graph-based label propagation for cross-lingual knowledge transfer and utilize the resulting labels as features in an unsupervised model. The idea is further developed in , where ambiguous learning approach enables effective automated transfer of tags from English corpora to corpora in other languages. In techniques for low-resource tagging are shown to be feasible.
Word embeddings have also been utilized for solving a number of morphological tasks. A work an architecture and an algorithm performing well in PO
Stagging task without labeling data beforehand. a model of morphologically guided embeddings, which is capable of handling tagging tasks in a semi-supervised manner by adding labeling to the training corpus.
Several works are devoted to automatic animacy prediction (, ). The methods rely on hand-crafted features obtained from annotated sources of semantic and lexical information, achieving high accuracy over 90%. The key idea of our method is, in contrast, in taking automated grammeme prediction to a competitive level by means of minimal available corpus annotation and limited amount of data. Our approach builds upon and extends the method described in , where the authors explore the ability of word embeddings to predict certain grammatical functions including noun animacy. In our work, we try to extend their research into the Russian language and improve the performance studying the influence of various parameters, both on the stage of distributional model training and while generating classification features.
3. Method description
3.1. Formal problem statement
We consider the task of grammeme prediction as a word-level binary classification task. In other words, we train a classifier to predict whether a word has a certain value of a grammatical category or not. In our study, which is designed to give preliminary characterization to feasibility of the approach, we do not focus on homonym disambiguation and treat each unique sequence of characters as a classification object.
Thus, the task is to build is a classifier ï¿½: ï¿½ âŸ¶ {0,1} that, on the basis of feature representation ğ’˜ of the word ğ‘¤â€‰âˆˆâ€‰ï¿½, would predict whether ğ‘¤ has the grammeme ğ‘” (1 class) or not (0 class). The optimal classifier ï¿½* is found by training on the set of labeled precedents ï¿½ = {(ğ’˜ 1,â€‰ï¿½1),â€‰...â€‰,â€‰(ğ’˜ ğ‘›,â€‰ï¿½ğ‘›)}, ï¿½1â€‰âˆˆâ€‰{0,â€‰1}, ï¿½â€‰=â€‰1â€‰...â€‰ğ‘›, i.e. the process of minimization of the empirical risk ğ‘„ğ‘„(ğ‘ğ‘,ğ·ğ·) = 1|ğ·ğ·|âˆ‘ |ğ·ğ·|ğ‘–ğ‘–=1 : ğ‘ğ‘âˆ— = arg minğ‘ğ‘ğ‘„ğ‘„(ğ‘ğ‘,ğ·ğ·) : ğ‘„ğ‘„(ğ‘ğ‘,ğ·ğ·) = 1|ğ·ğ·|âˆ‘ |ğ·ğ·|ğ‘–ğ‘–=1 : ğ‘ğ‘âˆ— = arg minğ‘ğ‘ğ‘„ğ‘„(ğ‘ğ‘,ğ·ğ·) Romanov A. V.
Grammeme choice
In order to verify the assumption that word embeddings may potentially be applied for grammeme prediction, we have chosen two of classifying grammemes in the Russian language, i.e. those that are intrinsically fixed for a lexeme and constant across its derived forms. Noun animacy and verb transitivity are good examples of grammatical categories whose values cannot be easily predicted judging by appearance of words and morphemes that constitute them; therefore, it is particularly interesting if distributional models of morphologically rich languages, with Russian being an example, can be a source of ready-to-use classification features.
Noun animacy basically provides distinction between nouns referring to humans (and some other biological creatures) and those referring to various inanimate objects and phenomena. In the Russian language, it is often necessary to have information about the noun animacy in order to inflect the noun correctly. Consider, for example, the plural accusative of an animate noun Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº (â€œboyâ€) â€” Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸ĞºĞ¾Ğ², matching the plural genitive, and the plural accusative of an inanimate noun Ğ¿Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº (â€œlittle fingerâ€) â€” Ğ¿Ğ°Ğ»ÑŒÑ‡Ğ¸ĞºĞ¸, matching the plural nominative. This rule generalizes to other animate and inanimate nouns. Automated animacy/inanimacy prediction is thus useful for morphological analysis and phrase generation as well.
Verb transitivity is a property of a verb to take direct objects, a special case of a more general notion of valency. In Russian, like in English, this category is expressed syntactically, i.e. it is possible to identify an intransitive verb by attempting to supply it with an appropriate direct object but not by judging by its morphological markers. Transitivity used to be believed to be a binary characteristics of a verb; now, no verbs are mainly seen as â€œabsolutely transitiveâ€ but rather â€œmore often occurring in texts with a transitive roleâ€. Intransitive verbs, however, never appear in phrases with direct objects, and this fact enables the task of transitivity prediction to be considered as a binary classification task.
3.3. Features
The main idea of the method is to use pre-trained word embeddings â€œas isâ€ as features for classification. The advantages of this approach are its simplicity and scalability onto related problems. We tested different parameter configuration sets of distributional model training to study the effect of the configuration choice on the overall performance.
Additional experiments were devoted to:â€¢	 enriching feature space with auxiliary per-word information provided in distributional models;â€¢	 transformations of word embeddings aimed at obtaining more informative representations.
Exploiting Russian Word Embeddings for Automated Grammeme Prediction 4. Experiments
4.1. Text Data
Distributional models were trained on a collection of Wikipedia articles in the Russian language (1.3
M articles and 100
M tokens on the whole). The text was split into sentences and lowercased. Non
Cyrillic words and punctuation marks were removed. All digits and numbers were replaced by a single special token. We lemmatized the corpus with pymorphy2, a Python package.
A list of Russian nouns and verbs labeled respectively with animacy and transitivity tags was obtained from the pymorphy2 package as well. Overall, 12
K verb (7.5
K transitive) and 121
K noun (47
K animated ones, including proper names) lemmas were extracted and prepared for classification.
4.2. Distributional model training
Among frameworks offering opportunities of training distributional models, gensim and fasttext were chosen, with word2vec and Fast
Text being the models providing word embeddings.
Word2vec continuous-bag-of-words were trained with a set of default parameters. We tried different (symmetric and asymmetric) configurations of context windows in order to test a hypothesis that smaller context windows induce word embeddings with greater grammeme prediction power. We also varied the dimension of embeddings, as higher dimension leads to better performance in a number of related tasks.
Fast
Text model a promising extension of word2vec, designed to construct vectors not only for words but also for character N-grams that constitute them. This way, the words that have some N-grams in common get representations that are more similar to each other. Another useful feature of this approach is its ability to predict vectors for unseen words. Fast
Text models of various dimensions were trained as well.
4.3. Experimental results
In our experiments, we compared three types of classifiers: Support Vector Machine (SV
M), Random Forest (R
F) and Multi
Layer Perceptron (ML
P). The metrics to be measured was weighted F1 score (average F1 by classes weighted by support). Hyperparameters of classifiers (regularization constants, number of trees and hidden layers, respectively) were tuned to obtain the best performance on 5-fold stratified cross-validation scheme.
Window size and dimension effect
We selected several word2vec and Fast
Text models to study the effect of different training parameters on classification performance (i.e. on weighted F1 scores):â€¢	 word2vec, 250-dimensional vectors, context window: 5 words before + 5 words after the word;
Romanov A. V.
word2vec, 500-dim, context: 5 + 5;â€¢	 word2vec, 500-dim, context: 2 + 2;â€¢	 word2vec, 500-dim, context: 0 + 3;â€¢	 word2vec, 500-dim, context: 3 + 0;â€¢	 Fast
Text, 250-dim, context: 5 + 5;â€¢	 Fast
Text, 500-dim, context: 5 + 5;â€¢	 Fast
Text, 250-dim, context: 5 + 5, prediction of vectors for unknown words;â€¢	 Fast
Text, 500-dim, context: 5 +5, prediction of vectors for unknown words.
It is worth noting that in the last two cases, the support for classification task is much greater in size than that of other cases: in such a setting we can obtain vectors for all the words we are willing. On the contrary, in the first cases, we conduct classification on the set of words that occur in the Wikipedia corpus. Thus, in the last two cases we have 121
K nouns (54
K in the first cases) and 12
K verbs (6.4
K in the first cases) for classification. Therefore, classification performance may differ in these two cases, but it resembles the situation of automated corpus tagging to a greater extent. The results are given in Model
Transitivity AnimacySV
M R
F ML
P SV
M R
F ML
Pword2vec, 250, 5+5 0.818 0.767 0.810 0.880 0.877 0.871word2vec, 500, 5+5 0.833 0.748 0.831 0.888 0.870 0.873word2vec, 500, 2+2 0.657 0.556 0.644 0.659 0.653 0.626word2vec, 500, 3+0 0.628 0.550 0.624 0.634 0.621 0.601word2vec, 500, 0+3 0.631 0.558 0.620 0.631 0.615 0.612Fast
Text, 250 0.853 0.827 0.862 0.845 0.834 0.825Fast
Text, 500 0.859 0.834 0.868 0.848 0.820 0.830Fast
Text, 250, prediction 0.828 0.819 0.856 0.790 0.799 0.805Fast
Text, 500, prediction 0.840 0.825 0.862 0.797 0.789 0.811
The results show that the models that incorporate knowledge about character N-grams of the word are more powerful for transitivity prediction, and their features have non-linear dependencies. However, linear methods performed better on classic word2vec models for animacy prediction task, since noun animacy in the Russian language has weak correlations with the wordsâ€™ appearance. Overall, the results are comparable with those achieved in tasks in other languages.
Enriching embeddings with auxiliary training information
While training word2vec models, one can access both ï¿½ğ‘–ğ‘› and ï¿½ğ‘œğ‘¢ğ‘¡ matrices. The former is mainly used as the word embedding source, and the latter rarely comes into use in practical tasks. We investigated the applicability of both types of vectors to our problem in the following manner. Three sets of classification features were extracted from a 500-dimensional word2vec model with the default 5+5 context window:
Exploiting Russian Word Embeddings for Automated Grammeme Prediction â€¢	 ï¿½ğ‘–ğ‘› rowsâ€”as in the abovementioned experiment;â€¢	 ï¿½ğ‘œğ‘¢ğ‘¡ columns;â€¢	 stacked ï¿½ğ‘–ğ‘› rows and ï¿½ğ‘œğ‘¢ğ‘¡ columns.
Table 2 shows the results of this study.
Features
Transitivity AnimacySV
M R
F ML
P SV
M R
F ML
Pï¿½ğ‘–ğ‘› rows 0.833 0.748 0.831 0.888 0.870 0.873ï¿½ğ‘œğ‘¢ğ‘¡ columns 0.848 0.755 0.844 0.886 0.871 0.865stacked ï¿½ğ‘–ğ‘› + ï¿½ğ‘œğ‘¢ğ‘¡ 0.852 0.750 0.841 0.893 0.876 0.883
Reducing word embeddings by the main PC
A components
We applied the trick proposed in , which is said to create more powerful embeddings performing better in a bunch of tasks. The idea of the trick is to center embeddings, reducing them by their average vector, apply PC
A and remove ğ· most informative components from the vectors afterwards. We studied classification performance with several values of ğ· on the same distributional model as in the experiment described in the previous paragraph. The results are available in ğ‘«
Transitivity AnimacySV
M R
F ML
P SV
M R
F ML
Pâ€” 0.833 0.748 0.831 0.888 0.870 0.8732 0.822 0.735 0.816 0.795 0.822 0.864
3 0.826 0.733 0.822 0.787 0.818 0.862
5 0.821 0.737 0.811 0.739 0.782 0.851
10 0.815 0.720 0.811 0.729 0.749 0.839
4.4. Discussion
Performance achieved in the experiments is sufficient to claim feasibility of the approach. Surprisingly, smaller values of context window size used in word2vec training lead to significant drop in classification performance. This effect allows to assume that grammemes of animacy and transitivity are more closely related to broader, semantic contexts of a word than to narrower, syntactic ones.
Fast
Text showed better performance than word2vec did in transitivity prediction task. This can be attributed to the fact that some transitive (or, probably, intransitive) verbs in the Russian language share certain character N-grams. Thus, a model that assigns closer vectors to similarly looking words is supposed to perform better in this Romanov A. V.
At the same time, the problem of animacy prediction is solved better by word2vec models, since animate nouns cannot be distinguished from inanimate ones judging by their appearance. However, Fast
Text predictive power for unseen words still makes it a good choice for automated corpus annotation.
It is worth noting that in some of the task settings non-linear classification models did not achieve higher performance than linear ones (especially on word2vec vectors). Another interesting fact is that ï¿½ğ‘œğ‘¢ğ‘¡ matrix of word2vec models is sometimes even more informative in classification tasks than ï¿½ğ‘–ğ‘› containing input word embeddings. Removing main PC
A components did not drop significantly the quality of classification, but there was no increase as well.
We have also analyzed errors produced by classifiers in both tasks and can group them into the following categories:â€¢	 polysemantic words (e.g. Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ (transitive â€œto changeâ€ or intransitive â€œto cuckoldâ€)) and homonyms (e.g. Ğ²ĞµĞ·Ñ‚Ğ¸ (transitive â€œto carryâ€ or intransitive â€œto be luckyâ€, Ğ±Ğ°Ñ€Ğ°Ğº (â€œa barrackâ€) and Ğ‘Ğ°Ñ€Ğ°Ğº (â€œ
Barackâ€));â€¢	 rare words lacking occurrences in the training corpus (e.g. Ğ´ĞµÑ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ â€œto sashayâ€, Ñ…Ğ»ĞµĞ±Ğ¾Ğ¿Ğ°ÑˆĞµÑ† â€œa sodbusterâ€);â€¢	 transitive verbs frequently used in sentences without a direct object (e.g. Ğ¿ĞµÑ‚ÑŒ â€œto singâ€);â€¢	 inanimate proper names that can be seen as human names (e.g. Ğ‘Ñ€ĞµĞ´Ñ„Ğ¾Ñ€Ğ´ â€œ
Bradfordâ€).
Overall, the majority of classifying errors appear to arise due to labeling problems and, to a lesser degree, due to the limited amount of data for training the distributional model.
5. Conclusion
We propose a method of automatic grammeme prediction, which is based on word embedding classification and does not rely on corpora annotation. Preliminary findings show performance that is competitive with systems developed on handcrafted features.
As the future work, we plan to achieve better classification quality and extend the method to handle other grammatical categories than those described in this paper. Improvements can be made by preparatory homonym disambiguation or training on unlemmatized text with subsequent pooling on word forms for lemmas (which can be done by several promising schemes) during the classification stage. Another interesting course of future study includes extension of our approach to other languages.
Acknowledgements
We are grateful to Eugene Indenbom and Konstantin Zuev for their valuable comments and suggestions.
Exploiting Russian Word Embeddings for Automated Grammeme Prediction