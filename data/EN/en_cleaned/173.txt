The task of coreference resolution was introduced at the Sixth Message Understanding Conference(
Grishman and Sundheim, 1996), where the first dataset for coreference resolution task was introduced.
The dataset consisted of 25 articles from Wall Street Journal (30,000 words). The annotation schemewas considered a standard until the release of AC
E 2005 Multilingual Training Corpus for the 20051https://github.com/vdobrovolskii/rucoco
Computational Linguistics and Intellectual Technologies:
Proceedings of the International Conference â€œ
Dialogue 2022â€
Moscow, June 15â€“18, 2022
Automatic Content Extraction (AC
E) technology evaluation (
Doddington et al., 2004). The corpus included texts in English, Chinese and Arabic and contained around 650,000 words in total for the threelanguages.
The MU
C guidelines were domain-oriented, and their definition of a markable (mention) was mostlysyntactically motivated. But further developments in this area, starting with the AC
E initiative, increasingly involved semantic factors, so that recent corpora with coreference annotation define markablesbased on semantic class restrictions.
Quite a lot of such corpora were created in the last two decades, their primary goals being to increasethe size in order to satisfy the requirements of the data-driven approach and to improve inter-annotatoragreement which in many cases is too low, especially when a dataset addresses more complex cases ofcoreference.
The most well-known corpus of this kind is Onto
Notes 5.0 (
Pradhan et al., 2013). Onto
Notes containstexts of various genres in three languages: English, Arabic, and Chinese. The cumulative volume ofthis corpus is 2.9 million words (about 1.5 million being English). The average annotator agreement forOnto
Notes is 91.8% for normal coreference and 94.2% for appositives (
Hovy et al., 2006).
The authors of the ARRA
U corpus (
Poesio et al., 2008; Uryupina et al., 2020) concentrate on "difficult" cases of anaphora: plural anaphora, abstract object anaphora, and ambiguous anaphoric expressions, so the corpus has bridging reference and discourse deixis annotated. It contains only Englishtexts (although there is an Italian analogue Live
Memories (
RodrÃ­guez et al., 2010)); its current size is350,000 tokens. The inter-annotator agreement in ARRA
U varies from 67% (annotation of anaphoric
ambiguities) to 95% (annotation of complex anaphoric relations).
Thus, most of the largest corpora with coreference annotation contain predominantly English texts;however, with the growing interest in natural language processing of Non
English languages, corporain other languages are being developed more often. As for the Russian language, there now exist twosuch datasets, one of them being Ru
Cor (
Toldova et al., 2014; Toldova et al., 2015) and the other AnCor(
Budnikov et al., 2019).
Ru
Cor contains texts from openly available sources, such as Russian Open
Corpora, Lib.ru and
Lenta.ru (156,000 words in total). In this corpus the annotation process was conducted over morphosyntactically pre-processed texts. The annotation scheme differentiates between primary and secondary markables, according to Potsdam Coreference Scheme (
Krasavina and Chiarcos, 2007), where theprimary markables are always annotated and represent specific references, while the secondary markablesare annotated only if they are antecedents of any of the primary markables. Inter-annotator agreementfor Ru
Cor is 66% (
Cohenâ€™s Kappa) or 85% (
Mitkovâ€™s metric).
An
Cor was created for the Ru
Eval competition in 2019 and contained 523 texts of various genresfrom Russian Open
Corpora (193,000 words in total). Named entities, common N
Ps and pronouns wereannotated; the inter-annotator agreement for this dataset is 62.7% (75.5% agreement of both annotatorsand the final version).
As can be seen, although there are plenty of different corpora with coreference annotation, the largestand the most complex ones do not contain texts in Russian, and as for the Russian corpora, they aresignificantly smaller than the English ones, besides, their inter-annotator agreement is lower.
Therefore our main goals were to create a sufficiently big Russian corpus which would contain annotation of at least some difficult cases of anaphora with the inter-annotator agreement being high enoughcompared to Onto
Notes and ARRA
U.
2 RuCo
Co: Russian Coreference Corpus
2.1 Data
We utilize the news stories published by NEW
Sru.com2 as our source of text data. The texts wereautomatically collected and processed in the following way:1. Any texts containing videos or embedded widgets from other websites were discarded as well as
any texts marked as promotions.
2https://www.newsru.com/
Dobrovolskii V. A., Michurina M. A., Ivoylova A. M.2. Then the texts were converted to plain text format and cleared of any remaining HTM
L artifacts.
3. Texts that contained fewer than 20 tokens were also discarded, because they mostly consisted of a
heading and a follow-up link only.
4. We then uniformly sampled one million words worth of texts across all text lengths and news cat
egories. The total number of sampled texts is 3075.
2.2 Annotation layer
The first release of RuCo
Co covers identity (and in some cases, near-identity) coreference of nounphrases and pronouns. We do not annotate singletons, which means that each mention is linked to atleast one other mention. We do not assign any attributes to the markables.
Mentions: We treat all noun phrases as potential mentions. Additionally the following types of pronouns are annotated:â€¢ personal, possessive and reflexive pronouns;â€¢ reciprocal pronouns, such as Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° (â€˜each otherâ€™);â€¢ relative pronouns;â€¢ interrogative pronouns.
However, at this point we do not annotate coreference links with adjectives, clauses and expressionsof time, all of which are going to be treated as valid mentions in the second revision of the corpus.RuCo
Co: a new Russian corpus with coreference annotation
Mention boundaries: In most cases full noun phrases are annotated. To avoid overlapping of mentions referring to the same entity, participle and relative clauses that depend on the mention head arenot included in mention boundaries. Therefore, in the following example there is no overlapping:{ĞºĞ»Ğ¸ĞµĞ½Ñ‚}0, {ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹}0 Ñ…Ğ¾Ñ‚ĞµĞ» Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ {ÑĞ²Ğ¾Ğ¹}0 ÑÑ‡ĞµÑ‚ (â€˜{a customer}0, {who}0 wanted totop up {their}0 accountâ€™). Parenthesis is not annotated unless it contains an independent clause, in whichcase it is treated as a regular sentence.
Coreference and anaphora: Coreference is annotated only for mentions of concrete entities. Forgeneric mentions and mentions of abstract entities, events and properties we only annotate anaphora:ĞœĞ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ğ¾? ĞœĞ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ {Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ°}0 Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ñ‚ÑŒ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼?ĞœĞ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ {Ğ¾Ğ½Ğ°}0 Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ? (â€˜
Can a machine act intelligently? Can {a machine}0 have aconsciousness? Can {it}0 feel how things are?â€™). Here, the first mention of Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ° (â€˜a machineâ€™) is notannotated as coreferent with other mentions, because it is a generic mention.
Ellipsis: Mentions with elided heads are not annotated, as it would create ambiguity: Ğ­Ñ‚Ğ¾ Ñ‚Ğ²Ğ¾ÑÑĞµÑÑ‚Ñ€Ğ° Ğ¸Ğ»Ğ¸ {Ğ”Ğ°Ğ½Ğ¸ÑĞ»Ñ}0? Ğ­Ñ‚Ğ¾ {ÑĞµÑÑ‚Ñ€Ğ° {Ğ”Ğ°Ğ½Ğ¸ÑĞ»Ñ}0}1, {Ğ¾Ğ½Ğ°}1 Ğ¿Ñ€Ğ¸ĞµÑ…Ğ°Ğ»Ğ° Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ. (â€˜
Is thisyour sister or {
Danielâ€™s}0? This is {{
Danielâ€™s}0 sister}1, {she}1 came for the weekend.â€™). In the exampleabove, the underlined mention could be recovered as ÑĞµÑÑ‚Ñ€Ğ° Ğ”Ğ°Ğ½Ğ¸ÑĞ»Ñ (â€˜
Danielsâ€™ sisterâ€™), but we do notannotate it as referring to entity #1, because there would be two identical mentions referring to differententities.
Split antecedents: In RuCo
Co, we annotate split antecedents as a means of dealing with the followingchallenges:â€¢ Mentions referring to multiple referents: {ĞŸÑ€ĞµĞ¼ÑŒĞµÑ€-Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€}0 Ğ¸ {Ğ³Ğ¾ÑĞ¿Ğ¾Ğ¶Ğ° Ğ¡Ğ°Ğ¹Ğ¼Ğ¾Ğ½Ğ´Ñ}1Ğ¿Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ»Ğ¸ÑÑŒ Ğ²Ñ‡ĞµÑ€Ğ° Ğ´Ğ½ĞµĞ¼, Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ñ†ĞµÑ€ĞµĞ¼Ğ¾Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° Ğ² Ğ’ĞµÑÑ‚Ğ¼Ğ¸Ğ½ÑÑ‚ĞµÑ€ÑĞºĞ¾Ğ¼ ÑĞ¾Ğ±Ğ¾Ñ€Ğµ.
{ĞŸĞ°Ñ€Ğ°}0,1 Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ·Ğ´Ğ½ÑƒĞµÑ‚ ÑĞ²Ğ°Ğ´ÑŒĞ±Ñƒ Ñ ÑĞµĞ¼ÑŒĞµĞ¹ Ğ¸ Ğ´Ñ€ÑƒĞ·ÑŒÑĞ¼Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ»ĞµÑ‚Ğ¾Ğ¼.3 (â€˜{Prime
Minister}0 has married {
Carrie Symonds}1 yesterday afternoon in a "small ceremony" at Westminster Cathedral. {
The couple}0,1 would celebrate again with family and friends next summer.â€™).
â€¢ Coordinate dependents: {Ğ¡Ğ±Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ Ğ¾ÑÑĞ¸Ğ¸ Ğ¸ ĞšĞ°Ğ½Ğ°Ğ´Ñ‹}0,1 Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñƒ Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ»Ğ¸ÑÑŒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»Ğ°Ñ… Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ°Ñ‚Ğ¾Ğ² Ğ¼Ğ¸Ñ€Ğ°. <..> {ĞÑ‚ĞµÑ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ…Ğ¾ĞºĞºĞµĞ¸ÑÑ‚Ñ‹}0 Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ»Ğ¸ {ĞºĞ°Ğ½Ğ°Ğ´Ñ†ĞµĞ²}1ÑĞ¾ ÑÑ‡ĞµÑ‚Ğ¾Ğ¼ 5-3 Ğ² Ğ¡Ñ‚Ğ¾ĞºĞ³Ğ¾Ğ»ÑŒĞ¼Ğµ Ğ² 1989 Ğ³Ğ¾Ğ´Ñƒ. (â€˜{
National teams of Russia and Canada}0,1 have notplayed in IIH
F finals before. <..> {
The Russian team}0 defeated {the Canadians}1 5-3 in Stockholmin 1989.â€™)
Further in the text we refer to mentions linked to split antecedents as plural anaphors and to entities builtfrom such mentions as plural anaphor entities. The number of such entities in the corpus can be seen in
Category Words Mentions Entities PA
Entities APA
Entitiesrussia 352,672 55,338 13,891 1,083 (7.8%) 2,471 (17.8%)world 311,445 50,283 12,660 1,045 (8.3%) 2,122 (16.8%)finance 94,015 11,739 3,020 176 (5.8%) 447 (14.8%)sport 80,352 11,807 3,331 279 (8.4%) 705 (21.2%)cinema 53,645 8,003 2,116 167 (7.9%) 431 (20.4%)realty 34,227 4,509 1,274 72 (5.7%) 184 (14.4%)hitech 31,365 3,895 1,080 77 (7.1%) 150 (13.9%)auto 24,735 2,914 881 40 (4.5%) 94 (10.7%)blog 17,649 1,917 624 39 (6.3%) 94 (15.0%)
Total 1,000,105 150,405 38,877 2,978 (7.7%) 6,698 (17.2%)
Metonymy: Linking of metonymies is allowed: {Ğ›Ğ¾Ğ½Ğ´Ğ¾Ğ½}0 Ğ¸ {Ğ‘Ñ€ÑÑÑĞµĞ»ÑŒ}1 Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑŠÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ Brexit. {Ğ•Ğ²Ñ€Ğ¾ÑĞ¾ÑĞ·Ñƒ}1 Ğ¸ {Ğ’ĞµĞ»Ğ¸ĞºĞ¾Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ğ¸}0 ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ3https://www.newsru.com/world/30may2021/bjohnson.html
Dobrovolskii V. A., Michurina M. A., Ivoylova A. M.ÑĞ¾Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ± Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ÑĞ»Ğµ Brexit. (â€˜{
London}0 and {
Brussels}1 have announced a Brexittrade deal. {
The European Union}1 and {the United Kingdom}0 have agreed on a post
Brexit tradedeal.â€™).
Corpus format: RuCo
Co is distributed as a collection of JSO
N-formatted files. An entity is represented as a list of character offset pairs. Antecedents of plural anaphor entities are listed in the "includes"section.
{"entities" : ], , ], ]],"includes" : , ],"text": "
At half-past nine, that night, Tom and Sid were sent to bed, asusual. They said their prayers, and Sid was soon asleep.\n"â†’Ë“}
Listing 1: JSO
N-formatted annotation of the following example: At half-past nine, that night, {
Tom}0and {
Sid}1 were sent to bed, as usual. {
They}0,1 said their prayers, and {
Sid}1 was soon asleep.
3 Corpus annotation
3.1 Metrics
There exist a number of coreference evaluation metrics, such as MU
C (
Vilain et al., 1995), B3 (
Baggaand Baldwin, 1998), CEA
F (
Luo, 2005), BLAN
C (
Recasens and Hovy, 2011) and others. Since theCoNL
L-2012 shared task (
Pradhan et al., 2012), the average score of MU
C, B3 and CEA
Fğ‘’ğ‘’, has becomea de-facto standard way to evaluate coreference resolution systems. However, several shortcomingsof these three metrics were demonstrated by Moosavi and Strube (2016), who also introduced LE
A, acoreference evaluation metric designed to overcome those shortcomings. LE
A of a set of entities ğ¾ğ¾ iscomputed as: âˆ‘ï¸€ğ‘’ğ‘’ğ‘–ğ‘–âˆˆğ¸ğ¸(ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘–ğ‘–ğ‘–ğ‘–)Ã— ğ‘–ğ‘–ğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘–ğ‘–ğ‘–ğ‘–))âˆ‘ï¸€ğ‘’ğ‘’ğ‘—ğ‘—âˆˆğ¸ğ¸ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘–ğ‘–ğ‘—ğ‘—)(1)where ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘–ğ‘–) = |ğ‘–ğ‘–| and the resolution score of entity ğ‘˜ğ‘˜ğ‘–ğ‘– is calculated against the response set ofentities ğ‘…ğ‘… as follows:ğ‘–ğ‘–ğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘˜ğ‘˜) =âˆ‘ï¸ğ‘Ÿğ‘Ÿğ‘—ğ‘—âˆˆğ‘…ğ‘…ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘˜ğ‘˜(ğ‘˜ğ‘˜ğ‘–ğ‘– âˆ© ğ‘–ğ‘–ğ‘—ğ‘—)ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘˜ğ‘˜(ğ‘˜ğ‘˜ğ‘–ğ‘–)(2)
Here, ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘˜ğ‘˜(ğ‘–ğ‘–) calculates the number of unique coreference link within ğ‘–ğ‘–: ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘˜ğ‘˜(ğ‘–ğ‘–) = |ğ‘–ğ‘–| Ã— (|ğ‘–ğ‘–| âˆ’ 1)/2.
We adopt LE
A as our primary metric for measuring inter-annotator agreement and evaluating the neuralcoreference resolution model. As LE
A does not support split antecedents out of the box, we modifythe metric in the following way: for each plural anaphor entity we additionally calculate the scores of aspecial dummy entity with ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– set to be the number of antecedent entities and ğ‘–ğ‘–ğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–computed based on the directed links between the plural anaphor entity and its antecedent entities.
The corpus was annotated by a team of 20 students of General Linguistics. The annotators werechosen based on trials that involved annotating documents of up to 1500 words. Each of the resultingdocuments was compared to the gold annotation using the LE
A metric. The passing score was set to 0.9;the passing rate was 67%. Five of the annotators with the highest annotation quality were later appointedas moderators.
3.2 Neural pre-annotator
To speed up the annotation process, we developed a neural coreference resolution model to pre-annotatethe texts. The model is based on the architecture proposed by Lee et al. (2018) and improved by Joshi etal. (2019) with the following differences:RuCo
Co: a new Russian corpus with coreference annotationâ€¢ We use the Russian version of RoBER
Ta (
Liu et al., 2019) pretrained by Sber A
I4.
â€¢ We replace the neural mention extraction module with a rule-based syntactic mention extractor builton top of spa
Cy (
Honnibal and Montani, 2017). This allows us to explicitly define what a mentionis instead of relying on neural networks for mention extraction.
â€¢ Following Dobrovolskii (2021), we represent mentions using only weighted sums of the subtokenembeddings that constitute the mention.
To train the model, we used the automatically merged annotations obtained during the early phases ofannotation. We ignored plural anaphors and used the original LE
A to evaluate the pre-annotation quality.
The model performed at 0.62 F1 after being trained on 100,000 words, at 0.68 F1 after being trained on400,000 words and at 0.73 F1 after training on the whole dataset of 1,000,000 words.
3.3 Annotation process
The annotation process consisted of two steps: the first 100,000 words were annotated from scratch, i.e.
the task was to identify and link all coreferent mentions in raw texts; the remaining 900,000 words werefirst pre-annotated by a neural coreference resolution model and the annotators were asked to correct theresulting documents.
Each text in the corpus was annotated by two annotators and then finalized by a moderator who received an automatic merge of the two versions with differences highlighted. Additionally, 3500 wordsof each annotator were manually checked by the authors of the markup scheme to provide feedback onan early stage.
3.4 Inter-annotator agreement
We measured the inter-annotator agreement and found it to be 0.759 F1. Because the annotators do nothave a closed set of mentions to link, we suspect that some of the differences between annotations can beattributed to lack of attention. To eliminate this factor, we conducted the following experiment on a subsetof the data approximately 50,000 words in size: each annotator was given back their own annotationsautomatically merged with the other annotation versions. The annotators were asked to independentlycorrect the documents. The resulting inter-annotator agreement was 0.890 F1.
3.5 Disagreement analysis
We analysed discrepancies of the two phases of corpus annotation: 1) from scratch (50 random texts,about 16,000 words examined) and 2) pre-tagged annotation (158 random texts, 50,000 words examined).
Discrepancies were divided into several categories:â€¢ missing/redundant coreference cluster;â€¢ missing/redundant markable;â€¢ missing/redundant anaphoric chain;â€¢ plural anaphors with split antecedents;â€¢ mentions referred to different entities;â€¢ N
P borders.
To make the comparison more informative, we carried out the error analysis of the neural model usedfor pre-tagging, although we need to keep in mind that after the first 100,000 words were checked,we made a number of minor clarifications and changes in the guidelines to facilitate the work of ourannotation team. See the comparison of discrepancies in annotation from scratch, model errors andpre-tagged texts in Figure 3.
By missing/redundant coreference cluster we mean all cases when one of the two annotators skippedthe whole cluster or marked up an unnecessary entity. It is the most frequent type when annotatorsdisagree (about 39% for both annotation stages). There was no closed set of entities, moreover, forabstract and generic entities, events or referents denoting open sets (so-called non-concrete entities) onlyanaphora must be annotated. Thus, annotators should decide whether the entity is concrete or nonconcrete. They disagree on the following examples: locations without proper names like ĞºÑ€Ğ¸Ğ·Ğ¸ÑĞ½Ñ‹Ğ¹4https://github.com/sberbank-ai/model-zoo
Dobrovolskii V. A., Michurina M. A., Ivoylova A. M.0 10 20 30 40 50 60 70 80 90 100
Annotation from scratch
Neural model annotation
Pre-tagged annotation
Coreference cluster Markable Anaphoric chain
Plural anaphors Different entities N
P bordersÑ€ĞµĞ³Ğ¸Ğ¾Ğ½ (â€˜crisis areaâ€™), Ğ¶Ğ¸Ğ»Ğ¾Ğ¹ ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ» (â€˜residential areaâ€™), and also when locations are nested in anorganisation name: Ğ Ğ¾ÑÑĞ¸Ñ (â€˜
Russiaâ€™) in ĞœĞ¸Ğ½Ğ¸ÑÑ‚Ñ€ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ Ğ¾ÑÑĞ¸Ğ¸ (â€˜
Minister of Transport of
Russiaâ€™) or in Ğ’ĞµÑ€Ñ…Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑÑƒĞ´Ğµ Ğ Ğ¤ (â€˜
Supreme Court of the Russian Federationâ€™). Some otherpopular types are events like ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚ Ğ² ĞœĞ¾ÑĞºĞ²Ğµ (â€˜the concert in Moscowâ€™), Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ°Ñ‚ Ğ Ğ¾ÑÑĞ¸Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾ĞºĞºĞµÑ (â€˜
Russian ice hockey championshipâ€™) and some abstract entities that are very similar to eventsas they have participants like ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚ (â€˜contractâ€™), ÑƒĞ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞ»Ğ¾ (â€˜criminal caseâ€™).
Missing/redundant markable (about 20% and 28% respectively) is the case when an annotatormissed one or several mentions, although the coreference cluster is there in both annotation versions.
For these cases we examined types of N
Ps missed by one annotator in the annotation from scratch stage,having preserved the taxonomy as in (
Toldova et al., 2015) in order to compare them. See Table 2 tocheck numbers. We can observe that both annotation groups of students tend to miss noun groups (i.e.
noun phrases headed by a noun) more than any other N
P type.
N
P Type Our Data, % Toldova et al., 2015, %
Reflexive pronouns 4.73 3.76
Relative pronouns 1.77 6.20
Anaphoric pronouns 4.73 12.47
Possessive pronouns 2.37 6.48
Noun groups 85.2 71.08
Adverbs (here/there) 1.18 0.00
As for missing/redundant anaphoric chain (3.3% and 8.6%) i.e. chains with abstract or genericentities where only anaphora resolution was performed, annotators mostly missed chains containinga relative pronoun ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ (â€˜which/thatâ€™) as an anaphoric element e.g. ÑÑ€Ğ¾Ğº, Ğ´Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ (â€˜thedeadline by whichâ€™), Ñ‚Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ (â€˜the policy thatâ€™).
In plural anaphors with split antecedents (9% out of all discrepancies, both stages), the most common discrepancy is a missing relation between a person and a group of people: a son and a family,ĞšĞ¾Ğ½Ğ´Ğ¾Ğ»Ğ¸Ğ·Ğ° Ğ Ğ°Ğ¹Ñ (â€˜
Condoleezza Riceâ€™), ÑĞµĞ½Ğ°Ñ‚Ğ¾Ñ€ Ğ¥Ğ¸Ğ»Ğ»Ğ°Ñ€Ğ¸ ĞšĞ»Ğ¸Ğ½Ñ‚Ğ¾Ğ½ (â€˜
Senator Hillary Clintonâ€™) andĞ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (â€˜politiciansâ€™). Less frequent cases of disagreement are the following: part-whole relations(which are not annotated as split anaphora) and entities denoting several items with part of these itemsas split antecedents: 50 Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¾Ğ² (â€˜50 terror attacksâ€™) and 20 Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¾Ğ² (â€˜20 terror attacksâ€™).
Mentions referred to different entities (6.4% and 7.9%) include cases where one or several mentionswere assigned to different clusters by annotators in some confusing contexts (e.g. pronouns) or oneannotator labelled some mentions in one and the same chain while the other one has divided it intoRuCo
Co: a new Russian corpus with coreference annotationseveral chains e.g. cases with metonymy like ĞŸÑ…ĞµĞ½ÑŒÑĞ½ (â€˜
Pyongyangâ€™) and ĞšĞĞ”Ğ  (â€˜
North Koreaâ€™),Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»ÑŒ (â€˜
Israelâ€™) and Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»ÑŒÑĞºĞ°Ñ Ğ°Ñ€Ğ¼Ğ¸Ñ (â€˜
Israel Defense Forcesâ€™).
Disagreement on N
P borders covers 21% of discrepancies in the first stage and substantially less onthe pre-tagged stage (7.5 %). We may assume that it may be due to the ability of our model to find correctborders or that it is due to the clarified guidelines of syntactic ambiguities we made before the secondannotation stage: we have highlighted that in all such cases the maximum N
P border must be annotated.
This category presupposes cases where annotators excluded modifiers as in Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ (â€˜changesâ€™) vs.
ÑĞ°Ğ¼Ñ‹Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ (â€˜the most significant changesâ€™), complements e.g. Ğ‘Ğ°Ğ½Ğº (â€˜the
Bankâ€™) vs. Ğ‘Ğ°Ğ½Ğº Ğ Ğ¾ÑÑĞ¸Ğ¸ (â€˜the Bank of Russiaâ€™) and less often appositives: Ğ‘ĞµÑ€Ñ‚ ĞÑŒÑĞ±Ğ¾Ñ€Ğ½ (â€˜Burt
Neuborneâ€™) vs. Ğ‘ĞµÑ€Ñ‚ ĞÑŒÑĞ±Ğ¾Ñ€Ğ½, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¾Ñ€ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğ° ĞÑŒÑ-Ğ™Ğ¾Ñ€ĞºĞ° (â€˜
Burt Neuborne
Professor of Civil Liberties at New York Universityâ€™).
This analysis was presented to the moderators so that they would know what to pay attention to.
Despite all these discrepancies, the resulting inter-annotator agreement is still 0.890 F1 and all the disagreements were resolved by our moderators.
4 Conclusion
The result of our work is the Russian Coreference Corpus, which is the largest corpus with coreferenceannotation for Russian so far. We managed to achieve almost 90% inter-annotator agreement; we alsoanalyzed the most common disagreements between our annotators so that we know what issues are tobe solved. Further developments will include annotating more difficult cases of anaphora as well asincreasing the size and genre diversity of the corpus.
Acknowledgements
We are grateful to our annotation team from General Linguistics Department of RSU
H for their hardwork, attentive approach to the project and immense help in discussions. We would also like to thank
Prof. Svetlana Toldova and Evgeniya Inshakova for their useful observations and helpful advice.
