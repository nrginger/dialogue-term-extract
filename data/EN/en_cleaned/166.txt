The automatic induction and learning of morphological paradigms is very popular in the last years. State-of-the-art works include , but several other papers are also worth mentioning (
Ahlberg et al., 2014], ). This task has various applications, e.g. synthesis
of surface word forms in machine translation and the automatic extension of morphological resources, such as wiktionary.org. The methods developed for paradigm learning can also be used in the automatic morphological analysis, e.g. for PO
S-tagging or lemmatization.
Automatic Detection of Morphological Paradigms Using Corpora Information The automatic induction of morphological paradigms has a long history in the Russian linguistic tradition. The seminal work of A. A. Zaliznyak “
Russkoe imennoe slovoizmenenie” exactly this problem: how the complete description of morphological inflection could be recovered from empirical data. If we reconsider the algorithm of Zaliznyak from the computational point of view and omit the technical details specific to Russian phonology, it is essentially based on the method of longest common subsequence (LC
S): the invariant part of inflected forms of the same lexeme is exactly their LC
S. The method of LC
S for automatic induction of morphological paradigms was reintroduced in works of Ahlberg, Hulden et al. (, ). However, for the purposes of computational linguistics, automatic induction of morphological paradigms from inflected tables is only the preliminary step. A more important question is how to detect the paradigm label and hence the complete inflectional table using only the base form of the lexeme. This problem is solved by machine learning techniques, using substrings of the source lexeme (e.g., its prefixes or suffixes) as features for the classifier.
There are practically no works on automatic detection of morphological paradigms for Russian: some results for noun declension but the quality of the source data is too low to consider them significant. We reimplement the method of Hulden for paradigm induction with several technical modifications and use a linear classifier to derive these paradigms automatically from the lexeme. Our algorithm is able to recover complete morphological paradigm both for Russian nouns and verbs with accuracy of 77% for paradigms and 93 and 88% for word forms respectively. We also demonstrate that the usage of corpora information improves the percentage of correctly predicted paradigms up to 82% for nouns and 86% for verbs.
2. Abstract paradigms
For the compressed representation of morphological inflection we use the notion of an abstract paradigm, introduced in . From the mathematical point of view, a paradigm is a tuple of functions 𝐹𝐹 = 〈𝑓𝑓1, … ,𝑓𝑓𝑛𝑛〉 max𝑖𝑖 𝑃𝑃�𝑐𝑐(𝐿𝐿) = 𝑐𝑐𝑖𝑖�𝑓𝑓𝑗𝑗(𝐿𝐿) = 1�𝑠𝑠(𝐿𝐿, 𝑐𝑐𝑖𝑖) =∑ − log𝑃𝑃𝑙𝑙𝑙𝑙(𝑤𝑤𝑖𝑖,𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑚𝑚 = ∑ − log𝐶𝐶(𝑤𝑤𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑞𝑞𝑗𝑗 = 𝑁𝑁𝑗𝑗𝑁𝑁𝐷𝐷(𝒩𝒩,𝒫𝒫) = �𝑞𝑞𝑗𝑗 log𝑞𝑞𝑗𝑗𝑝𝑝𝑗𝑗⋅ log𝑁𝑁𝑗𝑗 taking the same variables x1, ..., xr ∈ Σ+, where fi (x1, ..., xr ) operates from (Σ+)r to Σ+ (, see also ). Here Σ is the finite alphabet and Σ+ denotes the set of all words over this alphabet. Each of the functions fi corresponds to some grammatical meaning ci, the functions in set F are arranged according to a fixed order c1, ..., cn of possible grammatical meanings. Literally speaking, a paradigm is a mapping from variables to strings. We use the term “abstract paradigm” to represent morphological paradigms formally. An abstract paradigm is a tuple of strings containing variables x1, x2, ..., xn (the variables are the same for all strings and have the same order elsewhere) and constant fragments, which are the same for all lexemes satisfying the given paradigm. These constant fragments vary between the forms of the same lexeme. On the contrary, the variables have the same value for all inflected forms but differ from lexeme to lexeme.
Let us explain these formal terms on a short example. Consider the declension tables of two Russian nouns кусок and песок. The paradigm function F is the same for both of them; in the first case it takes the variables x1 = кус and x2 = к, in the second one—x1 = пес, x2 = к.
Sorokin A. A., Khomchenkova I. A.
1. Abstract paradigm: an example
Grammeme Pattern F (кус, к) F (пес, к)Nom.
Sg x1+о+x2 кусок песокNom.
Pl x1+x2+и куски пескиGen.
Sg x1+x2+а куска пескаGen.
Pl x1+x2+ов кусков песковDat.
Sg x1+x2+у куску пескуDat.
Pl x1+x2+ам кускам пескамAcc.
Sg x1+о+x2 кусок песокAcc.
Pl x1+x2+и куски пескиInstr.
Sg x1+x2+ом куском пескомInstr.
Pl x1+x2+ами кусками пескамиPr.
Sg x1+x2+е куске пескеPr.
Pl x1+x2+ах кусках песках
Given the variable values, an abstract paradigm unambiguously determines the complete inflectional table. When a pattern and a word form are known, usually there is only one way to fit the pattern to the word: for example, the word мешок and the pattern x1+о+x2 yield a single combination of variable values x1=меш, x2=к. Nevertheless, applying the same pattern to the word носок results in two variants x1=н, x2=сок and x1=нос, x2=к. If we take into account several possible patterns, the number of decompositions can grow up dramatically. However, the variables are extracted not from a single word form, but from all the paradigm elements simultaneously, which restricts the set of possible combinations.
3. Longest common subsequence
Consider again the abstract representation of morphological paradigms. If we substitute strings of letters for the variables, these strings form a common subsequence of all generated words. In order to capture as much common material as possible, that subsequence should be the longest one. Therefore, the problem of paradigm detection has been reduced to the task of finding the longest common subsequence. We are not going to discuss the linguistic relevance of this approach and use it only as an empirical procedure. However, several important questions emerge:1. How to calculate the longest common subsequence algorithmically?
2. What subsequence to select when several subsequences have the same length?
3. How to extract variable values when the LC
S is known?
For the first task we use finite automata. It is straightforward to construct an automaton recognizing all the common subsequences of given strings and then extract the longest word this automaton accepts (we omit algorithmical details). Although, Automatic Detection of Morphological Paradigms Using Corpora Information this automaton could be nondeterministic and an equivalent deterministic state automaton may have much larger number of states (up to 2n where n is the number of states of initial nondeterministic automaton). To prevent this exponential growth we bound the length of gaps between the consequent letters of the subsequence, as well as the gap before the first letter of the subsequence. This limitation is also justified from the linguistic point of view: consider two verb forms разместиться and размещусь, their LC
S размес has length 6. However, с in the LC
S is an artifact of the method, not an element of common stem. Besides, alterations like ст/щ are among the phenomena which are difficult to capture by LC
S algorithm.
The construction of finite automata recognizing all common subsequences for the words моток and окот is illustrated below. The edges contain not only the symbols, but also the positions of these symbols in the words. This trick allows to simplify the extraction of an abstract paradigm from the LC
S.
In the example above there are 3 longest common subsequences: оо, ок, от. Possible variants of their positioning are shown in the table below.
tab. 2. LC
S for the words моток and окотLC
S LC
S positioning variantsо-о моток, окото-т моток, окото-т моток, окото-к моток, окото-к моток, окот
Already in this artificial example there are multiple variants for LC
S positioning. The same problem emerges in practice: consider a partial declension table of the word песок. There are two candidates for the LC
S: пес-о and пес-к both of length 4.
Sorokin A. A., Khomchenkova I. A.
3. Ambiguous LC
S positioning: an exampleNom.
Sg песок Nom.
Sg песокGen.
Pl песком Gen.
Pl пескомInstr.
Sg песков Instr.
Sg песков
We use two heuristics for disambiguation: the first selects the variant with the minimal number of variables (variables are the maximal contiguous parts of the LC
S). However, this heuristic does not give us a solution here: both subsequences consist of two variables. Then we apply our second heuristic: choose the variant with the least total length of gaps. Then the variant песок-песков-песком is preferred, since it leads to a single gap of length 1 while its counterpart generates two such gaps (of total length 2).
4. Automatic detection of paradigms
In the previous section we have discussed the algorithm for morphological paradigms induction. However, it is not a central problem of the paper; we are mainly interested in the automatic detection of such paradigms for unknown words. We consider the following task: given an unknown word of a known part-of-speech (say, a noun арка), determine its complete declension table. The algorithm selects one of many potential variants, several of which are listed in tab. 4. Multiple possible paradigms for the word арка
Paradigm Variables1#1+ы#1+а#1+ов#1+у#1+ам#1#1+ы#1+ом#1+ами#1+е#1+ах 1=арка
1+а#1+а#1+ы#1#1+е#1+ам#1+у#1+ы#1+ой#1+ами#1+е#1+ах 1=арк
1#1+ы#1+а#1+ов#1+у#1+ам#1#1+а#1+ов#1+ами#1+е#1+ах 1=арка
1+2+а#1+2+и#1+2+и#1+о+2#1+2+е#1+2+ам#1+2+у#1+2+и
#1+2+ой#1+2+ами#1+2+е#1+2+ах1=ар,
2=к
We may attempt to recover a correct paradigm using deterministic rules such as “when a noun ends with а then this а is a flection, not a part of a stem” (counterexample: баккара) and if such word ends with “
Cка” for some consonant C then о is inserted between C and к in genitive plural (counterexample: ласка). However, all such rules have counterexamples and their manual design is a very labour-intensive task. Therefore we have decided to learn inflectional patterns automatically applying algorithms of machine learning. We use as features all the suffixes1 whose length does not exceed the given maximum (say, 5). The suffixes are encoded as binary indicators; for example, the word учитель is described by a binary vector with five nonzero elements, corresponding to suffixes -ь, -ль, -ель} etc. (see Table 5 below). The absence of a suffix in the training set is encoded by a special placeholder, in this case longer 1 We use the term “suffix’’ (“prefix’’) for an arbitrary substring in the end (in the beginning)
without any regard to morphology
Automatic Detection of Morphological Paradigms Using Corpora Information suffixes are not taken into account since they were not observed in the training set either. For example, if the suffix -ль was preceded only by е in the training set, then both words мораль and фасоль are encoded by a vector containing three ones for suffixes -ь, -ль and !ль} where ! denotes an unobserved letter.
tab. 5. Feature encoding schemeа к ка ла ик рка …арка 1 0 1 0 0 1 …школа 1 0 0 1 0 0 …блик 0 1 0 0 1 0 …… … … … … … … …
Since prefixes carry no information about noun morphology, we do not use them as features for noun paradigm prediction. In the case of verbs, conversely, they can be used to determine verb aspect. If d is the maximal length of suffixes used as features, then the number of possible features grows roughly exponentially with d and may reach 20,000 for d = 5. To reduce training time and remove noisy features we retain only a fixed percentage of the most unambiguous features. As the measure of ambiguity for the feature fj we take 𝐹𝐹 = 〈𝑓𝑓1, … ,𝑓𝑓𝑛𝑛〉 max𝑖𝑖 𝑃𝑃�𝑐𝑐(𝐿𝐿) = 𝑐𝑐𝑖𝑖�𝑓𝑓𝑗𝑗(𝐿𝐿) = 1�𝑠𝑠(𝐿𝐿, 𝑐𝑐𝑖𝑖) =∑ − log𝑃𝑃𝑙𝑙𝑙𝑙(𝑤𝑤𝑖𝑖,𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑚𝑚 = ∑ − log𝐶𝐶(𝑤𝑤𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑞𝑞𝑗𝑗 = 𝑁𝑁𝑗𝑗𝑁𝑁𝐷𝐷(𝒩𝒩,𝒫𝒫) = �𝑞𝑞𝑗𝑗 log𝑞𝑞𝑗𝑗𝑝𝑝𝑗𝑗⋅ log𝑁𝑁𝑗𝑗—the probability of the most frequent class provided fj is present. We also remove features which appear less than 3 times in the training set.
5. Evaluation of paradigm classifier
We have evaluated our approach on Russian verbs and nouns. For both tasks we took 5,000 most frequent words of the corresponding part of speech from the dictionary of Lyashevskaya and Sharoff (). We automatically downloaded complete inflectional tables from the Wiktionary (ru.wiktionary.
org). For nouns the tables contained at most 12 items for 6 cases and 2 numbers (several cells in the paradigm could be empty, e.g. for pluralia tantum). Sometimes the cell contained two values (for example, Instr.
Sg. of first declension nouns), in this case we always chose only the first form. We extracted 239 abstract paradigms for noun declension, 69 of them contain more than 5 examples and 108—only a single example. 10 most frequent paradigms are listed in Table 11 of the Appendix.
In the case of verbs typical Wiktionary form for imperfect aspect contains 21 simple forms (https://ru.wiktionary.org/wiki/\%D0\%B6\%D0\%B5\%D0\%BB\%D0\%B0\%D1\%82\%D1\%8
C) including infinitive and omitting composite future form and empty cells. For paradigm induction we used only 13 of them: 6 present forms, 4 past, 2 forms of the imperative and the basic infinitive form. Even in such restricted
form verb conjugation demonstrate more irregularities then noun declension, so the sample of 5,000 verbs contains 305 paradigms with 120 of them having 5 or more representatives and 92—a single representative. 10 most frequent paradigms are shown in Table 12. We bound maximal gap length by 2, therefore the algorithm does not recognize с as part of the LC
S in the examples like играться/играешься/играйтесь.
Sorokin A. A., Khomchenkova I. A.
our experiments we randomly separated the sample on 2 equal halves, using one for testing and the other one for training. The results were averaged for 5 random splits. In the case of nouns we did not use prefixes as features and bound suffix length d by 3, 5 or 7. The percentage p of selected features was 0.10, 0.25 or 0.5. In the case of verbs
we calculated the suffix length without the reflexive affixes -ся and -сь. We also used the prefix features with the maximal length of 2 for verb conjugation. To predict paradigm labels we used the logistic regression classifier from sklearn package , which itself uses the LIBLINEA
R library . The results are presented in Table 6 and predicted abstract paradigms) and per-form (the fraction of correct word forms) accuracy.
tab. 6. Prediction accuracy for noun paradigms classification0.1 0.25 0.5
3 77.19 93.47 77.26 93.47 77.25 93.47
5 77.38 93.50 77.32 93.48 77.32 93.48
7 77.44 93.45 77.35 93.43 77.35 93.43
Since the result of nouns is practically independent from the classifier parameters, we fix p = 0.1 and d = 5 in future experiments. We use the same setting for the verbs task, however, in this case the impact of feature length is more significant.
tab. 7. Prediction accuracy for verb paradigms classification0.1 0.25 0.5
3 51.41 79.96 51.41 79.96 51.41 79.94
5 76.30 88.83 76.09 88.62 75.94 88.62
7 77.06 88.36 78.01 89.35 77.96 89.38
We also study how the prediction quality changes with the size of the training set. When there is little training data available, a lemma may not fit to all inflectional patterns observed in training phase (say, a verb ends with -ти and all the infinitives in the training set ended with -ть, -ться or -чь). In such cases we allow the system consult a complete list of paradigms, no matter whether they were observed in training. The dependence between training data size and system performance is shown in tab. 8. Train data percentage and performance qualityTask
Training data fraction0.1 0.25 0.5 0.6 0.7 0.8
Nouns 71.7691.15
75.05
92.32
77.38
93.50
77.95
93.70
77.88
93.77
77.40
93.84
Verbs 65.5083.83
71.50
86.27
76.30
88.83
77.49
89.36
77.60
89.41
77.56
89.50
Automatic Detection of Morphological Paradigms Using Corpora Information 6. Analysis of results
It is uninformative to compare results for different languages and even for different datasets. As we know, the only experiment on paradigm detection for Russian nouns was conducted by Ahlberg et al. in , showing per-table accuracy of 66% and per-form accuracy of 89%. However, they used data collected from Freeling (), which is of much lower quality than ours. They also used 5-fold cross-validation for performance evaluation, which means that 80% was left for training instead of only 50% in our experiment. However, the results for other languages, such as Catalan, French or Italian, reported in much higher with pertable accuracy of over 90%. We claim that corpus-free methods are incapable of reaching comparable accuracy on Russian data due to the objective linguistic factors. There are two main sources of errors in the case of noun paradigm prediction: the first is animacy/inanimacy affecting the forms of accusative, the second is -а/-ы in the form of Nom.
Pl. In both cases the correct category does not depend on the surface form (consider волчонок vs бочонок or голос vs колос). The system also fails to discriminate between masculine and feminine nouns ending with ь (мозоль vs король). It is obvious that these ambiguities cannot be resolved without corpus statistics. We discuss this question in details in the next section.
For verbs the problem is more subtle. Often the mistake happens for the forms of imperative mood, for example, *тревожи is predicted instead of тревожь or *похити for похить. In such cases the forms or indicative mood are usually correct. Another common source of mistakes are е/ё in verb flections (compare хлопнуть and толкнуть). In this case the flection depends on the stress position in the infinitive form, however, we removed the stress signs in our data since they are marked inconsistently in Wiktionary itself. Such mistakes affect only several forms (imperative or third person present tense). Errors of the second type touch practically all forms of the paradigm. It often happens for the verbs ending on -ать (венчать vs кричать). The system also fails in the case of phonetic alterations (унизить/унижу), especially when they happen inside the stem (звать/зову or слать/шлю). Summarizing, the spectrum of possible errors for Russian verb paradigm prediction is wider than for Russian nouns, which explains lower per-form quality in the verb prediction task. However, in both cases more training data does not help, as shown in 7. Corpus-based methods of paradigm predictions
In this section we experiment with other features which might be helpful for automatic paradigm detection. In the verb paradigm task incorrectly predicted forms sometimes violate the rules of Russian phonology like in *осуществься or *исчежь} for исчезни. These incorrect forms might be rejected if we extend the model by phonological features. This idea is realized as following:
First, we train a character n-gram model on the training data. Then we augment the algorithm with second classifier on the top of the first. It takes as features Sorokin A. A., Khomchenkova I. A.
probabilities predicted by the classifier on the first level as well as the scores of the language model. If the basic classifier has predicted ci as paradigm label for the lemma L, we generate all the forms wi, l , ..., wi, m of this lexeme according to the paradigm; then we take as language model score the averaged sum𝐹𝐹 = 〈𝑓𝑓1, … ,𝑓𝑓𝑛𝑛〉 max𝑖𝑖 𝑃𝑃�𝑐𝑐(𝐿𝐿) = 𝑐𝑐𝑖𝑖�𝑓𝑓𝑗𝑗(𝐿𝐿) = 1�𝑠𝑠(𝐿𝐿, 𝑐𝑐𝑖𝑖) =∑ − log𝑃𝑃𝑙𝑙𝑙𝑙(𝑤𝑤𝑖𝑖,𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑚𝑚 = ∑ − log𝐶𝐶(𝑤𝑤𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑞𝑞𝑗𝑗 = 𝑁𝑁𝑗𝑗𝑁𝑁𝐷𝐷(𝒩𝒩,𝒫𝒫) = �𝑞𝑞𝑗𝑗 log𝑞𝑞𝑗𝑗𝑝𝑝𝑗𝑗⋅ log𝑁𝑁𝑗𝑗where P lm (wi, j ) is the probability of word form wi , according to character ngram model. We test two ways of accomodating the language model log-scores: in the first case we use them as features of the linear classifier. In the second variant we used language model scores only for filtering, discarding a paradigm ci if its score s (
L, ci) is greater than s0 + α where s0 is the lowest value among s (
L, ci ) and α is some redefined constant. We used 5-gram language models trained on the set of word forms from the training data and smoothed the model counts using Witten
Bell smoothing (). The results for Nouns and Verbs tasks are presented in Table 9, we used p=0.1 and d=5 for feature fraction and suffix length in all trials, the percentage of training data was again 0.5.
tab. 9. Using character model for paradigm prediction
Task No character scores
Character scores as features
Character scores as filters
Nouns 77.38 77.42 77.36
Verbs 76.30 80.37 77.01
We observe that language model has no effect for the Nouns task. On the contrary, on the verbs task filtering already improves performance significally, while combining language model scores with initial paradigm probabilities increases prediction quality by 3 percents more. It is easy to explain since the main source of errors for nouns was the confusion between animate/inanimate nouns where both the predictions are phonologically plausible. Conversely, in the Verbs task the mispredicted forms in imperative like *осуществься has low probability according to character ngram models which allows the system to exclude them.
The main contribution of our paper is corpora-based algorithm for paradigm prediction. Again, we accommodate corpora counts together with the logarithmic probabilities predicted by the basic classifier on the second stage of our algorithm. More precisely, after generating the word forms wl , ..., wm of the lexeme L according to hypothetic paradigm cj , we calculate the corpus score by the formula 𝐹𝐹 = 〈𝑓𝑓1, … ,𝑓𝑓𝑛𝑛〉 max𝑖𝑖 𝑃𝑃�𝑐𝑐(𝐿𝐿) = 𝑐𝑐𝑖𝑖�𝑓𝑓𝑗𝑗(𝐿𝐿) = 1�𝑠𝑠(𝐿𝐿, 𝑐𝑐𝑖𝑖) =∑ − log𝑃𝑃𝑙𝑙𝑙𝑙(𝑤𝑤𝑖𝑖,𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑚𝑚 = ∑ − log𝐶𝐶(𝑤𝑤𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑞𝑞𝑗𝑗 = 𝑁𝑁𝑗𝑗𝑁𝑁𝐷𝐷(𝒩𝒩,𝒫𝒫) = �𝑞𝑞𝑗𝑗 log𝑞𝑞𝑗𝑗𝑝𝑝𝑗𝑗⋅ log𝑁𝑁𝑗𝑗, where C (wj ) is the number of times wj occurs in the corpora. All counts are incremented by 1 to avoid zero probabilities. This method resembles the method of , however, we make one modification to deal with homonymy: if a word form occurs two times in the paradigm (for example, in nominative and genitive), then we divide all the corpora counts of it by 2. Without this modification, this algorithm favours invariable nouns.
Automatic Detection of Morphological Paradigms Using Corpora Information However, we are still unable to discriminate between inanimate and animate nouns by our algorithm since the set of word forms is the same in both cases. The only difference is that genitive forms of animate nouns would be more frequent than the ones of inanimate since they appear in accusative also. To capture this difference we should measure the similarity between the expected distribution of case forms and the observed proportion of their counts. Let P = the expected probabilities of different word forms according to their grammemes and N = their observed counts. We normalize the empirical distribution by its sum N = ∑ j Nj, obtaining the empirical probability distribution Q = , where 𝐹𝐹 = 〈𝑓𝑓1, … ,𝑓𝑓𝑛𝑛〉 max𝑖𝑖 𝑃𝑃�𝑐𝑐(𝐿𝐿) = 𝑐𝑐𝑖𝑖�𝑓𝑓𝑗𝑗(𝐿𝐿) = 1�𝑠𝑠(𝐿𝐿, 𝑐𝑐𝑖𝑖) =∑ − log𝑃𝑃𝑙𝑙𝑙𝑙(𝑤𝑤𝑖𝑖,𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑚𝑚 = ∑ − log𝐶𝐶(𝑤𝑤𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑞𝑞𝑗𝑗 = 𝑁𝑁𝑗𝑗𝑁𝑁𝐷𝐷(𝒩𝒩,𝒫𝒫) = �𝑞𝑞𝑗𝑗 log𝑞𝑞𝑗𝑗𝑝𝑝𝑗𝑗⋅ log𝑁𝑁𝑗𝑗. Then the difference score equals𝐹𝐹 = 〈𝑓𝑓1, … ,𝑓𝑓𝑛𝑛〉 max𝑖𝑖 𝑃𝑃�𝑐𝑐(𝐿𝐿) = 𝑐𝑐𝑖𝑖�𝑓𝑓𝑗𝑗(𝐿𝐿) = 1�𝑠𝑠(𝐿𝐿, 𝑐𝑐𝑖𝑖) =∑ − log𝑃𝑃𝑙𝑙𝑙𝑙(𝑤𝑤𝑖𝑖,𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑚𝑚 = ∑ − log𝐶𝐶(𝑤𝑤𝑗𝑗)𝑙𝑙𝑗𝑗=1𝑞𝑞𝑗𝑗 = 𝑁𝑁𝑗𝑗𝑁𝑁𝐷𝐷(𝒩𝒩,𝒫𝒫) = �𝑞𝑞𝑗𝑗 log𝑞𝑞𝑗𝑗𝑝𝑝𝑗𝑗⋅ log𝑁𝑁𝑗𝑗
Note that this measure is simply Kullback
Leibler divergence between Q and P multiplied by the log count of the given lexeme. The expected form counts were collected in the training phase separately for each paradigm. The results for corporabased paradigm prediction are shown in Table 10. We used the counts from Russian National Corpora available on ruscorpora.ru/corpora-freq.html.
tab. 10. Using character model for paradigm prediction
Task No corpora
Corpora counts as features
Counts and divergences as features
Nouns 77.38 80.21 82.73
Verbs 76.30 84.30 83.66
We observe that using corpora counts indeed leads to a substantial gain in performance in both tasks. However, in the case of verbs most of the advantage is obtained from corpora counts themselves, using similarity scores slightly worsens performance. On the Nouns task similarity scores, on the contrary, leads to a further improvement in per-table accuracy. Indeed, the most difficult problem for nouns is animacy/inanimacy differentiation where absolute counts are useless. In the verb tasks, conversely, homonymy plays no role, therefore, similarity scores are redundant and make the data noisier.
Inspecting remaining incorrect predictions, we found that in the Verbs task they are mainly caused by wrong imperative form generation. Often corpus counts cannot resolve this problem because imperative forms are not very frequent for many verbs: both кровоточи and *кровоточь do not appear in the RN
C counts. Often corpora features are not powerful enough to overcome the gap caused by first level classifier. For example, for the verb лгать the correct paradigm has probability 0.01 after the first stage. Joint classifier raises it up to 0.3, however, it is too low to rank this hypothesis on the top. The same problem arises in the task of noun paradigm prediction: for most of the erroneous predictions the correct paradigm was excluded already by the basic classifier or obtained an extremely low probability.
Sorokin A. A., Khomchenkova I. A.
also combined character n-gram scores with the corpora-based classifier, which improved the performance further. For the Nouns task the gain was marginal (82.80% instead of 82.73% for per-table accuracy), however, the accuracy of paradigm prediction for verbs achieved 86.51% instead of 84.30%. The per-form accuracy also increased significantly, reaching 95.66% in comparison with 93.81%.
8. Conclusion
We have developed a system for automatic paradigm induction and prediction. Our algorithm of paradigm induction is based on the method of longest common subsequence. To predict paradigms automatically we apply a logistic regression classifier using suffix and prefix features. This classifier achieves accuracy of 77% on Russian nouns and 76% on Russian verbs in paradigm prediction task, the percentage of correctly predicted forms is 93% and 88% respectively. We have also designed a corporabased algorithm of paradigm prediction using the basic classifier on its first stage. This improves the accuracy of paradigm prediction to 82% on nouns and 86% on verbs, per-form accuracy reaches 95 % for both tasks. These results are substantially better than previously achieved for Russian in authors of that work used another dataset and experiment setting).
We plan to improve our results further by using corpora information more extensively. Our results show that taking into account relative frequencies of grammemes enhances the quality of corpora-based methods. Therefore modelling the distribution of grammemes more accurately should leave to further improvement. For this goal we plan to use morphologically disambiguated corpora. Another improvement could be achieved by grouping together the corpus statistics for the words of presumably the same paradigm.
Our results could be used for automatic morphological analysis and synthesis in such tasks as PO
S-tagging or lemmatization. Modern techniques of lemmatization such as used in use the LC
S approach but apply it to each word form separately without using full inflectional table. Our method incorporates information from the whole paradigm, therefore it could potentially improve state-ofthe-art algorithms of morphological analysis for Russian. Since our system does not predict the best inflectional table only, but returns the probabilities of possible paradigms, it can be used as a component of a joint classifier, taking into account context model probabilities as well as single word scores. Using context information together with suffix/prefix features could also help to determine word part-of-speech, which is a preliminary step for our algorithm.
This task is especially important for Web texts, which contain numerous out-ofvocabulary words whose inflection cannot be determined by dictionary-based methods. We plan to test our approach for morphological processing of social media texts in future studies.
Automatic Detection of Morphological Paradigms Using Corpora Information