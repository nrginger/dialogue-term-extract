The notion of frame been a key concept for semantics for several decades, until it was recently replaced by a network approach where informa‑tion is stored in a trained classifier and is not readily available for linguistic analysis. We developed an automatic system for the analysis of natural language text, designed to invoke emotional reactions (gestures, facial expression, speech replies) from the meaning of incoming texts. The system can animate a companion robot F‑2 or can store the extracted meaning with a suggested emotional reaction to a database. In this work we describe the processing of the incoming text up to its meaning which is stored to the database. We return to the classical form of semantic representation—frame, and describe semantics as a set of semantic features (markers) divided up into a set of valencies (agens, patient, etc.). To construct the semantic representation we develop a parser, implementing morphological, syntactic and semantic processing of an in‑coming text as suggested by theoretical linguistic models, e.g. . The parser is written in C#, the grammar of Russian is described in syntXM
L language and the dictionary is stored in an SQ
L database. On each step of processing, the parser may save the intermediate results of analysis to an SQ
L database or transfer them to the next software component—ture of the parser and the methods to analyze the semantic representations, extracted during a text analysis.
Frames Revisited: Automatic Extraction of Semantic Patterns from a Natural Text2. The architecture of the parser
Scripts (the models of emotional reactions) are designed to detect emotional (in‑put) patterns in incoming texts . The representations, to be searched by scripts, are quite shallow: ‘
X is a tasty food’ or ‘
I was hit by someone’ are suffi‑cient patterns to suggest an emotional reaction to the robot. To detect the patterns, parser should store the main taxonomic markers for natural words—suggest that John is ‘a person’ and strawberry is ‘a food’, as well as the emotional markers—idiot is ‘an in‑adequate person’. The parser should also allocate the markers to specific valencies in order to distinguish the antagonist and protagonist roles in an emotional event. Emotions (unlike formal logical notation) are sensitive to the syntactic structure of a sentence: as suggested in a classic work by Blakar, The police took in the demonstrators sounds more violent and emotional than The demonstrators were taken in by the police . This implies that the changes of diathesis should be revealed in the required semantic representation in a form of semantic/syntactic roles—valen‑cies. Therefore, the task of the parser is to construct a semantic representation, where semantic markers are applied to a set of valencies. This exactly correspond to the clas‑sic representation of frame. Table 1 shows the semantic representation of an utterance Linguists have noticed psychologists at the conference (
Lingvisty otmetili psihologov na konferencii). Such semantic representations can be quite shallow, sufficient only to detect the input patterns for the activation of scripts. In the architecture of the parser, scripts play the major role—they select the best tree with the precision, suf‑ficient to extract the input patterns. The distinction between trees, equally weighted by the emotions, is ignored.
Linguists have noticed psychologists at the conference
Valency p ag cont loc
Semantic markersthinkpay‑attentionsomebodyprofessionsomebodyprofessionabstract‑entitycontainerabstract‑container
Kotov A. A. et al.2.1. Morphological processing: lemmatization and dictionary
For a given wordform, the lemmatizer should extract (a) a required set of semantic markers and (b) a set of grammemes to construct a syntactic tree and further allocate the valency for the specific word. The morphological dictionary is based on the Open‑
Corpora project stores all wordforms and grammemes for 98,000 lemmas. To each incoming segment the lemmatizer assigns a number of morphological hypotheses. In case no hypothesis is assigned, a segment is consecutively processed by (a) a user dictionary, where one can store a noun with an inflectional class and (b) a guesser, which assigns 5 morphological hypotheses, based on a trained Keras neural network (for standard words this offers 96,7% recall). Words from a user dictionary and guesser do not get any semantic representation and are considered as semantically void. 30,000 words in the main dictionary are manually annotated by semantic markers (from 1 to 18 markers per word). We rely on semantic markers from two main groups: taxonomy markers and emotional markers.
(a) Taxonomy markers are used to classify the word and the whole predication. We use markers like ‘somebody’, ‘ profession’, ‘move’, ‘move body’, here we rely on the annotation suggested in the Russian semantic dic‑tionary . Unlike in traditional ontologies, a word may keep semantic markers from different classes: bank has the markers for ‘organiza‑tion’, ‘building’ and ‘abstract container’. This polysemy allows us to simulate situational effects , where a word meaning is shifted depending on the situation or the emotion to be invoked by semantics—top‑down emotional processing . In our case, different scripts may detect different markers in the lexical semantics, addressing dif‑ferent word meanings.
(b) Emotional markers, on one side, are markers, explicitly expressing a posi‑tion in an emotional situation and an emotional evaluation like words fool, great or useless. These words are annotated by specific script markers: ‘agent/antagonist in a situation of inadequate actions’ (inadeq script), ‘protagonist in a situation of superiority’ (super), ‘antagonist/excessive for the actions’ (freedom) or ‘protagonist/someone, incorrectly ignored’ (unneed). On the other side, we annotate ambiguous markers, that may contribute to emo‑tions—emotionally sensitive markers, like ‘intensity’—relevant in the emo‑tional phrases like Why do you push me? .
Emotional markers have attributes, indicating their position in the focal/back‑ground semantics of the corresponding word. For example, the word fool has the markers ‘inadequate’ in its focal zone, and ‘somebody’ (‘human’) in its background zone, while the word man has the marker ‘somebody’ in its focal zone.
Words may have different meanings (homonymic and polysemic), that is rep‑resented by the number of subsets of semantic markers—one subset for each mean‑ing. Different input patterns of scripts may address different markers in the semantics of a word. A script will select the meaning that fits best its input pattern.
Frames Revisited: Automatic Extraction of Semantic Patterns from a Natural Text2.2. Syntactic processing: syntXML
The task of the syntactic processor is to construct a syntactic representation where each actant gets its valency. The parser implements a left‑to‑right process‑ing and combines rule‑based approach to construct syntactic links with the trained classifier approach to evaluate the syntactic structures. Syntactic rules are designed on syntXM
L language . A rule is defined as a possible reduction, where the right‑hand side (one or many segments a, b, … n) can be reduced to the left‑hand head h (1). Head h can also be a member of the right‑hand side and subordinate other segments (2).
→ <a, b, … n> (1) → <a, b, h, … n> or <a, b, hhead, … n> (2)
Right‑hand side of a rule may contain one or many segments a, b, … n, segments except n can be optional. This allows us to describe immediate constituent rules, where h is not a member of the right‑hand side rule section, as well as dependency rules—both with any number of subordinate segments, so the grammar is not lim‑ited to binary relations. A rule can (a) verify if a segment matches a specific lexeme, (b) check the presence of a particular grammeme within a segment, (c) check gram‑matical agreement between rule segments, (d) set or rewrite grammemes for the rule head h, (e) copy grammemes/markers to the rule head h. This combination is repre‑sented in an XM
L form—as a syntXM
L rule. The grammar for Russian uses 550 rules and structurally corresponds to the System of Syntactic Groups by Gladkiy . In particular—conjunctions are defined as immediate constituents with a vir‑tual head, as the features of the whole construction cannot be fully represented by any segment, whereas other grammar structures are represented as dependency trees.
Each incoming segment is added to a stack and the parser tries to reduce the stack head with a grammar rule, upon the reduction the subordinate segments are excluded from the stack and are linked to the rule head. Morphological and syntactic ambiguities are accounted: a separate stack instance is constructed for each morpho‑logical hypothesis and for each ambiguity in the application of syntactic rules. At the same time, the maximum number of stacks m is limited, currently by m = 512. Each rule application is evaluated, taking into account the corpus data (basing on a con‑verted SynTag
Rus corpus): co‑occurrence of (a) particular words within this rule, (b) word2vec vectors of the words within the rule, (c) sets of grammemes of the words within the rule. In other words, a syntactic link is more probable, if it exists in the corpus between: (a) the same words, (b) similar words, where similarity is calculated by word2vec distance, or (c) words with the same sets of grammemes. An aggregated score is calculated for each stack, after that m best stacks are preserved for further analysis. Stacks with lower scores (stacks with less probable syntactic links and stacks with few syntactic links) are discarded. In case several trees are generated by the end of the sentence (the possible number is from 0 to m), the best tree is determined and selected depending on its comparison with scripts, as explained in 2.3.
The semantic structure for a tree is constructed in the following way. For each predicate (finite verb, predicative, etc.) a valency p is assigned. Once a rule binds Kotov A. A. et al.a predicate with an actant (usually, a noun phrase), it may assign a valency to the noun phrase. In our project we rely on the list of valencies, suggested by , and use a list of 22 valencies: ag (agent), pat (patient), instr (instrument) etc. We have marked subcategorization frames for 13,000 Russian verbs, in part, relying on data from Frame
Bank project . A verb gets a number of syn‑tactic markers, where each marker allows the verb to trigger a specific rule to assign an actant to a particular valency. For example, the subcategorization frame of the verb zvenet’ (‘to ring’—звенеть) has 8 such valencies, as shown in Actant: case and prepositionN
P, nom‑inative, no prep.
N
P, instru‑mental, no prep.
N
P, ac‑cusative, prep. v (in)N
P, geni‑tive, prep. ot (from)N
P, geni‑tive, prep. u (at)N
P, accusa‑tive, prep. o (about)
Adverb or adver‑bial NP
Adverb or adver‑bial NP
Valency ag (agent)instr (in‑strument)targ (target)caus (cause)pos (pos‑sessor)ca (coun‑terparty)t (time) loc (lo‑cation)
Thus, the syntactic structure of the sentence U menya zvenit v ushah (‘
It rings in my ears’—у меня звенит в ушах) will assign the predicate zvenit (‘rings’) to p va‑lency, N
P u menya (~‘at me’) to pos valency (this triggers 1‑у‑
Sgen‑pos marker of the predicate) and N
P v ushah (‘in ears’) to loc valency (this triggers 1‑loc marker).
Following the syntactic analysis, the semantic representation is constructed based on the lexical semantics of each word and the semantic valencies assigned to the words in the syntactic structure. Semantics of a single clause is represented by a semantic predication—a two‑level tree with predicate p at the top and a number of actants at the bottom—where each node (predicate or actant) is a set of semantic markers. Such a semantic tree can be represented as a table—frame—taking into account that p is the head of the whole clause. The semantic frame of the sentence Lingvisty otmetili psihologov na konferencii (‘
Linguists have noticed psychologists at the conference’) is shown in Table 1, while U menya zvenit v ushah (‘
It rings in my ears’)—in U menya zvenit v ushah (‘
It rings in my ears’)
Valency p pos loc
Markers to‑sound objectsomebodyother‑personprincipal (speaker)objectcontainercontainer‑smallpartpart‑of‑human
For a compound (multi‑clause) sentence several frames are constructed: one frame for each predicate—with co‑reference or attributive links between the valen‑cies of the frames. Further, the semantics of each tree is compared to the list of scripts (their input patterns) to determine the best tree and to suggest an emotional reaction for the F2 robot.
Frames Revisited: Automatic Extraction of Semantic Patterns from a Natural Textv ushah (‘
It rings in my ears’—у меня звенит в ушах)
Kotov A. A. et al.2.3. Evaluation with scripts
To classify and evaluate the sentences, as well as to simulate the emotional dy‑namics for the F‑2 robot, the parser relies on the set of scripts. Scripts are defined as ifthen productions, a script is activated once an incoming semantic predication matches its’ if‑condition—input pattern. We consider an input pattern as an implementation of frame—a structure, similar to the semantic representation of a clause. For each in‑coming semantic predication, the parser calculates the distances with the input pat‑terns of all the existing scripts. A tree with the highest similarity to the scripts is se‑lected. We rely on the list of emotional scripts, represented in . The list includes 13 scripts for negative situations: danger, appropr (“
Appropriation”), subjv (“
Subjectivity”—e. g. ‘all he thinks about is himself!’) etc., and 21 scripts for positive situations: control, care, comfort, attention (e. g. ‘they all adore you!’), approval (e. g. ‘you did it like a real man!’) etc. By comparison of each incoming meaning (frame) to the script’s input pattern the parser tries to classify a situation as a negative event—‘this is dangerous’, ‘he is inadequate’, or as a positive event—‘they all adore me!’the utterance Lingvisty otmetili psihologov na konferencii (‘
Linguists have noticed psychologists at the conference’)
Similarity Script Possible response1.14 ATTENTIO
N—gratitude Thank you for your support!
1.14 ATTENTIO
N It seems, everyone notices me!
1.09 PLA
N Did they mean that? They plan something evil!
1.09 SUBJ
V They only think about this!
1.01 UNUSUA
L It’s a whole new and unusual world!
1.01 COMFOR
T It’s nice to be here!
As shown in Table 4, attention script is responsible for the “experience” of emo‑tion, while “attention—gratitude” is responsible for a communicative reaction—the expression of gratitude, where the internal “expression” of emotion is not definitively required. In both scripts the robot associates itself with the pat valency (‘a psycholo‑gist’)—the most emotionally relevant position for the scripts. If any marker suggests a better reference to the robot (
Robot has noticed psychologists, Linguists have noticed the robot)—the activation of scripts changes accordingly. Both attention scripts are sensitive to ‘pay‑attention’ marker in p valency and ‘somebody’ markers in ag and pat valencies. Further scripts plan and subjv are responsible for negative reactions—they can be used to simulate a depressive mood or irony . Scripts unusual and comfort are sensitive to the situations where an agent acts inside or is entering some “magic world” or a “comfort place”. They are sensitive to the actions, taking part in ‘containers’: unusual treats ‘the conference’ as a ‘big and unusual container’, like walking in a wild forest, while comfort considers the conference as a tiny and cozy container. As each semantic representation (frame) is associated with a script input pattern, each pair <script input pattern, semantics> serves as an illustration of frame and its actual fillers, like ‘an idiot’ = John or ‘a cozy place’ = conference.
Frames Revisited: Automatic Extraction of Semantic Patterns from a Natural Text
Adverbs and adjectives may add additional markers to the predicate structures, changing their evaluation. For an utterance Linguists always notice psychologists at the conferences—subjv (“
Subjectivity”) becomes the leading script with the similarity 1.53—robot judges the ‘linguists’ as “limited” persons, who ‘can do nothing else but
noticing the psychologists’. This is an example of emotionally sensitive marker, as de‑scribed in 2.1(b). Although the most active script is considered as the most relevant classifier of the situation and the best reaction for the robot, other scripts may be pre‑ferred for specific reactions: They plan something evil! reaction (plan) may be invoked to simulate robot’s depressive state or irony . This ambiguity in the per‑ception of text semantics may be used to study the architecture of consciousness—where one script (and the corresponding representation) constitutes the “accurate” understanding, while other scripts may form “ironical” or “hypothetical” understand‑ing of the text . Different scripts may even suggest the selection of dif‑ferent syntactic trees, this option is not actually used, although it can be useful for computational simulation of humor, where the second semantics (tree) is used for the humorous semantic shift.
3. Facts database
Parser operates in a daily test mode: it collects through RS
S and processes about 6,000 sentences per day from 25 information sources—15 media sites and 10 most
popular Live
Journal blogs. The texts are analyzed by syntactic and semantic mod‑ules at a speed of 100 sentences per minute, the extracted semantic representations are stored in a database (PostgreSQ
L). As the parser is designed for the extraction of a specific set of scenarios (input patterns), the results of analysis cannot be di‑rectly compared to the results of other parsers: the analysis is not sensitive to the changes in the syntactic trees, not relevant to the input patterns. As we suggest, one of the possible extensions of the parser is the conversion of the extracted semantic representations to the input patterns of scenarios: up to now the parser distinguishes only the emotionally relevant representations. However, the extracted facts (as in Ta‑bles 1 and 3) can be converted to input patterns to recognize ‘possible’ or ‘regular’ situations. These situations do not offer specific communicative reactions to the robot (and thus their development is a side task for the project) but may extend the parser accuracy in a large number of the recognized situations.
The extracted shallow semantic representations can be easily stored and pro‑cessed in a database. They offer diverse types of requests for information retrieval and classification.
Lexical analysis. For a given lexeme (head of a noun phrase or head of a verb phrase) it is possible to retrieve its participation in different valencies with other surrounding actants. As shown in Table 5, the analysis of a word distribution across valencies reveals the semantic features and polysemy of the word—where a single notion with certain polysemy may be treated as an active agent, patient, cause, loca‑tion, time, source/target, or a trajectory. This approach may be used to extend the semantics of a particular word: e. g. add markers for a possible actor, time, 2d or 3d‑location, etc.
Kotov A. A. et al.
Valency Examplesag (agent) Snow falls. Snow melts under black feet.
pat (patient) John has been raking snow with bare hands. Their children cannot eat snow.
cont (content) He simply didn’t notice the snow. It resembled a fire-breathing snow.
instr (instrument)
Traces would be drifted with the snow. His hands were frozen but he immediately rubbed them with snow.
caus (cause) Everything is white because of the snow. The wreckage could happen due to the snow.
foc (focus) We were lucky with the snow. Their ingression with the snow to the water was the reason of the crimson color.
it (interpreta‑tion)
He was surprised: it was a sweet snow! The construction has melted and turned into a simple snow.
loc (location) Do they sleep right on the snow? People saw crows on the snow.
t (time) After the snow in his dreams he knew, everything would be fine in the morning.
src (source) An arrogant exclamation came out from the snow.
targ (target) They threw rifles to the snow. He fell from the stairs and dug his head into the snow.
tr (trajectory) He managed to get through the spring snow. He dragged the case in the snow.
Analysis of a semantic/syntactic valency. For a given verb (head of a verb phrase) and its valency it is possible to study typical fillers—like what do people drink? (patient for a verb drink—see Table 6) or who is usually a patient of violence? (patient of any violence verb). This aggregation may serve as a basis for question answering—for questions to a specific participant of a situation, as well as to extend lexical seman‑tics: e. g. add markers of a possible ‘drink’ to the fillers of the frame.
pat of ‘drink’ predicates Number of casestea 144water 78wine 72coffee 64beer 51what 49vodka 40blood 31something 19champagne 172 Wrong analysis of a clause drink
Frames Revisited: Automatic Extraction of Semantic Patterns from a Natural Textpat of ‘drink’ predicates Number of casesmilk 14glass 13bottle 11drink 10cognac 10it 10sin2 10cocktail 9liquid 9
Search and aggregation of semantic frames. The annotation of the valencies with semantic markers makes it possible to search for all the facts (semantic predi‑cations) corresponding to a given pattern. For example, in the cases of extremism, where ‘a person causes harm to someone’, named after his/her ‘nationality’ or ‘profes‑sion’—as in the following examples:(1) His girlfriend has beaten the driver with an unidentified object.
(2) Following the investigation records, the businessman has shot the director of the plant.
(3) Husband has also started to beat the policemen.
The aggregation of specific examples, meeting the requested pattern may be fur‑ther used to extend the accuracy of the input patterns. In this sense, the parser repre‑sented here is frame-centric: frames (input patterns of scripts) are used to select a tree and each word meaning, further, frames can be used to extend lexical semantics, new frames can be designed basing on the clustering of the extracted meanings, and the existing frames may be refined by the new examples falling within the same script.
