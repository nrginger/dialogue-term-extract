The online reputation analysis task, performed on social networks‚Äô data such as Twitter data, has several differences from the traditional sentiment tasks. We suggest that performance of systems designed to solve this problem depends on three factors:(i) Lexicon actualization‚Äîthe first issue is that there are many texts that do not contain any intuitively subjective words, but nonetheless, express a person‚Äôs attitude. Usually such words are domain-specific. For example, in the context of everyday media usage of the word, the verb ‚Äú–≤—ã–¥–∞–≤–∞—Ç—å‚Äù (most closely translated as to ‚Äúfib‚Äù), has negative sentiment, because it is frequently used in meaning ‚Äúto lie‚Äù (‚Äú–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —á—Ç–æ-–ª–∏–±–æ –Ω–µ —Ç–µ–º, —á–µ–º –æ–Ω–æ —è–≤–ª—è–µ—Ç—Å—è –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ‚Äù) or ‚Äúto betray‚Äù (‚Äú–¥–µ–ª–∞—Ç—å –¥–æ–Ω–æ—Å, –ø—Ä–µ–¥–∞–≤–∞—Ç—å‚Äù)1. However, in banking, the same word means ‚Äúto issue a credit card‚Äù or ‚Äúprovide a loan‚Äù (‚Äú–ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ —á—å–µ-–ª. —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ‚Äù) and usually has a positive sentiment. A promising approach to sentiment word extraction was described in . In this paper, we study different Word2
Vec models, trained by using news and social networks data.
The second issue is that pejorative lexicon used by social network users does not always indicates a negative opinion. For example, someone may be using swear language to indicate either negative or positive affect, which may not be obvious immediately.
1 Meanings of the verb ‚Äú–≤—ã–¥–∞–≤–∞—Ç—å‚Äù are provided by Wiki
Dictionary: http://ru.wiktionary.
org/wiki/–≤—ã–¥–∞–≤–∞—Ç—å
Entity Based Sentiment Analysis Using Syntax Patterns and Convolutional Neural Network (ii) Object matching‚Äîthe issue here is linking the sentiment word with key object, especially when text is long or when there are multiple entities mentioned. For example, it would be very difficult to analyze the sentence ‚Äú–ë–∏–ª–∞–π–Ω, –∫–æ—Ç–æ—Ä—ã–º —è –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–≤–∞ –≥–æ–¥–∞, –≥–æ—Ä–∞–∑–¥–æ –ª—É—á—à–µ –ú–¢–°‚Äù (‚Äú
Beeline, that I‚Äôve used for two years, is much better than MT
S‚Äù) using only linear context because key sentiment word ‚Äú–ª—É—á—à–µ‚Äù is much closer in absolute word distance to the object ‚ÄúMT
S,‚Äù rather than ‚Äú
Beeline.‚Äù Also, according to our experience, analysis of comparison structures such as ‚Äú
A is better than B,‚Äù without syntax information, produces erroneous results. Both our approaches incorporate syntax information, as described later in the paper. With that, we are basing our method on the classical approaches to solving this problem as described in , .
(iii) Subjective fact interpretation‚Äîrecent sentiment evaluation competitions show tendency of adding fact interpretations to sentiment analysis. For example, in the sentence ‚Äú–°–±–µ—Ä–±–∞–Ω–∫ –ø–æ–¥–∞—Å—Ç –≤ —Å—É–¥ –∏—Å–∫ –ø–æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤—É –ú–µ—á–µ–ª–∞‚Äù (‚Äú
Sberbank will bring a bankruptcy case against Mechel to court‚Äù), we have a fact of a bankruptcy, negative for ‚Äú
Mechel‚Äù, but an ordinary bank activity for ‚Äú
Sberbank.‚Äù Processing such data requires many specific, often counteractive rules to deal with the problem of contradicting sentiments in the traditional rulebased approach, but could be efficiently performed by modern neural networks.
Recent works involving CN
N-based approaches in English , , demonstrated excellent results on various classification tasks, including sentiment analysis. Because we expected that (ii) and (iii) factors could only be solved with syntax-dependency information, we used CN
N, which uses not only linear word order, but also syntax dependencies to extract sentiment, and could allow for more efficiency in the task processing.
Rule-based approach, described later in this paper, is similar to the RC
O approach , but there are differences in text preprocessing and lexical dictionaries‚Äô extraction. The CN
N approach is also similar to but we have changed the input vector and made entity token with special TARGE
T mark to achieve a more efficient objectoriented sentiment analysis. We also used custom convolution patterns in this work.
2. Methods
with graphematic, morphological, and syntax parsers at the Text preprocessing stage. The rule-based approach assumes that predefined syntax patterns are enhanced by preliminarily generated Word2
Vec models and sentiment dictionaries. as Resulting feature vector is analyzed by the extremely naive classifier that labels the object sentiment according to quantity of sentiment facts, linked with this object in the text. The resulting sentiment is a net sum of positive and negative sentiment labels. In case of the CN
N-based approach, preprocessed text is vectorized with preliminary generated Word2
Vec model. CN
N returns the sentiment label as a result. We first build two separate classifiers, which can be easily combined, as shown in experiments section later in the paper. We now discuss each module in detail.
Karpov I. A., Kozhevnikov M. V., Kazorin V. I., Nemov N. R.
WV
Sentiment lexicon
Sentiment facts detection
Na√Øve classification CNN-classification
Text vectorisation
Text preprocessing
Input text
Sentiment SentimentCN
N-based approach
Rule-based approach
External resources2.1. Text preprocessing
Since input data from social networks is very noisy, a substantial amount of preprocessing is required. These steps are discussed below.
2.1.1. Remove UR
Ls
UR
Ls do not carry a lot of substantial information regarding the sentiment of the tweet and contaminate the dictionary, so we remove them with simple regex.
2.1.2. Remove nontextual data
Hashtags and tokens, starting with an ‚Äúat‚Äù sign (@) represent important information about the reviewed object. In order to find it, we remove certain punctuation such as quotation marks, hyphens, asterisks, ‚Äúat‚Äù signs, etc.
2.1.3. Tokenisation & morphology
We applied our own NL
P toolkit Mystem parser developed by Yandex2 for text preprocessing. Morphological analysis shows similar results, but tokenization, done by Mystem, was not designed to handle emotions and other punctuation specifics of social networks, so we preferred our own parser, which could overcome these limitations.
2.1.4. Named Entity (N
E) recognition
We used Wikipedia hyperlink structure to find entities and their possible occurrences in the text as proposed in . The basic algorithm was enhanced by adding transcripts and translations for each separately occurring appearances of key objects. We also generate separate grammatical cases for each normal form of the word 2 https://tech.yandex.ru/mystem/
Entity Based Sentiment Analysis Using Syntax Patterns and Convolutional Neural Network or phrase, describing the key object, and add them as a possible occurrence of key object in the text. As a result, we formulate the dictionary of the key objects‚Äô occurrences in the text. During the text processing step we replace key objects‚Äô occurrence with a special ‚ÄúTARGE
T‚Äù token and an appropriate morphology information.
2.1.5. Syntax parsing
We process entire dataset with malt parser , trained on our own news corpora to get dependency trees used by both approaches. If the tweet contains multiple root nodes, they are all added as descendants of special fake ‚ÄúROO
T‚Äù node. Sample syntax parse result is shown at figure 2. In general, our constructs had a single root, but in case it was not so, we used the described approach.
2.2. Word2
Vec training
We use the Word2
Vec (V
W) in both the ruleand the CN
Nbased approaches. In case of a rule-based approach, W
V is used for computing semantic similarity between sentiment words. In CN
N, W
V is needed to represent text as a matrix for the neural network input. W
V is trained on word lemmas with part-of-speech codes. We exclude punctuation, conjunctions, prepositions, particles and short (less than 3 symbols) English words from the training data. We use 300-dimension vector size skipgram model with the minimum cut-off for the number of words = 3 in all cases.
Corpora lexicon plays an important role in generating W
V model. We gathered nearly 1.5 million twitter search results about general topics such as music, cinema, travelling, literature, sports, etc3. Obtained model takes into account the specifics of twitter language, but still suffers from the word sense ambiguity problem. Therefore, we also gathered twitter search results for banking and telecom topics of nearly 100,000 tweets each.
3 Selected categories list, trained models and project code can be found at http://github.com/
lab533/RuSentiEval2016
Karpov I. A., Kozhevnikov M. V., Kazorin V. I., Nemov N. R.
following combinations of gathered corpora were made to find the balance between corpora size and word ambiguity problem:‚Ä¢	 WV_
Banks_clear: 120,000 bank tweets‚Ä¢	 WV_TT
K_ clear: 120,000 telecom tweets‚Ä¢	 WV_
Twitter: 1,500,000 gathered twits‚Ä¢	 W
V_news: 4,500,000 news texts
We also added news-based W
V to explore the role of twitter-specific vocabulary in sentiment tasks. Different mixtures of gathered corpora was evaluated as described in experiments section.
2.3. Rule-based approach
As a first step, we look for sentiment words of a tweet. We use our own universal dictionary of sentiment words for this purpose. Dictionary consists of 2,074 positive and 6136 negative normal word forms, manually verified by experts. After inflection of normal words forms and their enrichment with top 2 most similar W
V words, dictionary was transformed to 60,288 positive and 189,953 negative word forms. Using the syntax tree of the sentence, which contains sentiment word, we detect modal verbs and negotiation markers (like ‚Äú–Ω–µ‚Äù, ‚Äú–Ω–µ—Ç‚Äù etc.).
Next, we define sentiment facts associated with sentiment words. Sentiment fact is a semantically isolated part of a syntactic tree, which contains the sentiment word. In our rule-based approach, there are two types of sentiment facts, depending on parent of the sentiment word. If a parent of the sentiment word is a subordinate part of a sentence, a sentiment fact is a branch of the syntax tree with the parent of the sentiment word. This is the first type of sentiment facts. An example of such fact is the phrase ‚Äú—É—Ä–æ–¥–ª–∏–≤–æ–µ –∑–¥–∞–Ω–∏–µ –°–±–µ—Ä–±–∞–Ω–∫–∞‚Äù (‚Äúugly Sberbank building‚Äù) of the sentence ‚Äú–í –∫–∞–∂–¥–æ–º –≥–æ—Ä–æ–¥–µ –†–æ—Å—Å–∏–∏ –µ—Å—Ç—å —É—Ä–æ–¥–ª–∏–≤–æ–µ –∑–¥–∞–Ω–∏–µ –°–±–µ—Ä–±–∞–Ω–∫–∞‚Äù (‚Äú
There is an ugly Sberbank building in each city in Russia‚Äù), as shown at figure 3.
The second type of a sentiment fact is the sentiment word or its parent, which is one of the subjects of the sentence or one of its predicates. In this case, the sentiment fact includes a predicate, a subject, and all of their children tokens. For example, Entity Based Sentiment Analysis Using Syntax Patterns and Convolutional Neural Network the sentiment fact here is the ‚Äú–Ω–µ–Ω–∞–≤–∏–∂—É –†–∞–π—Ñ—Ñ–∞–π–∑–µ–Ω –±–∞–Ω–∫‚Äù (‚Äúhate Raiffaizen bank‚Äù) in the sentence ‚Äú–Ø –Ω–µ —É—Å—Ç–∞–Ω—É –ø–æ–≤—Ç–æ—Ä—è—Ç—å, —á—Ç–æ –Ω–µ–Ω–∞–≤–∏–∂—É –†–∞–π—Ñ—Ñ–∞–π–∑–µ–Ω –±–∞–Ω–∫‚Äù (‚Äú
I will never stop saying that I hate Raiffaizen bank‚Äù), as shown at figure 4.
Next, we unite neighboring sentiment facts: if one of the tokens of the sentiment fact has a syntactic connection with a token of another fact, these two facts get combined into one. Then we apply rules of combination of positive and negative sentiment words inside facts, and calculate integer sentiment index for each fact.
To improve general performance of the algorithm, we also made some individual rules for each domain:‚Ä¢	 Stop-words list (words from dictionary that do not have any sentiment for a specific domain);‚Ä¢	 Unigram and n-gram words list (words that have a sentiment value only for a specific domain);‚Ä¢	 Applying ‚Äú
No-rule‚Äù (words or n-grams that have sentiment only with or without negotiation);
Finally, we find sentiment facts that contain a target object. If there is no sentiment fact with a target object, we assign object to the nearest fact in the syntactic tree. Then we calculate total sentiment score for each object and use it as a final sentiment result. We mark tweets that do not have any sentiment facts as neutral.
2.4. Convolutional neural network approach
Convolutional neural networks (CN
Ns), originally invented in computer vision , in recent years have been applied in many natural language processing (NL
P) tasks such as authorship detection, question answering, and sentiment analysis. Let ùë•ùë•ùëñùëñ ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)
the ùëò-dimensional word vector corresponding to the ùëñ-th word in the sentence. The sentence of length n can be described asùë•ùë•ùëñùëñ ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)

Karpov I. A., Kozhevnikov M. V., Kazorin V. I., Nemov N. R.
‚äï is the concatenation operator. Such vector is considered to be CN
N input. A convolution operation involves a filter ùë•ùë•ùëñùëñ ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)
, which is applied to a window of h words to produce a new feature. This filter is applied to each possible window of words in the sentence to produce a feature map. Max-overtime pooling over the feature map is applied to capture the most important feature‚Äîone with the highest value‚Äîfor each feature map. These features maps form the penultimate layer and are passed to a fully connected softmax layer, whose output is the probability distribution over labels.
2.4.1. Dependency-based Convolution
We are using the Mingbo‚Äôs to include syntax information into the classification process, where dependency-based convolution is described as follows: ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)
ùë•ùë•ùëñùëñ ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)
the ùëñ-th word‚Äôs ùëõ-th parent, which is recursively defined as:ùë•ùë•ùëñùëñ ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)

Text preprocessing notation and the peculiarities of twitter text often cause the TARGE
T node to be separated from the sentiment fact into a different sentence. In order to capture these long-distance dependencies in the entire tweet, we use sibling convolutions defined as:ùë•ùë•ùëñùëñ ‚àà ùëÖùëÖùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•1 ‚äï ùë•ùë•2 ‚äï ‚Ä¶‚äïùë•ùë•ùëõùëõùë§ùë§ ‚àà ùëÖùëÖ‚Ñéùëòùëòùë•ùë•1:ùëõùëõ = ùë•ùë•ùëñùëñ ‚äï ùë•ùë•ùëùùëù(ùëñùëñ) ‚äï ‚Ä¶‚äïùë•ùë•ùëùùëùùëõùëõùëõ1(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ)ùëùùëùùëõùëõ(ùëñùëñ) = ÔøΩùëùùëùÔøΩùëùùëùùëõùëõ‚àí1(ùëñùëñ)ÔøΩ ùëõùëõ > 0ùëñùëñ ùëõùëõ = 0ùë†ùë†(ùëñùëñ, ùëóùëó) = ÔøΩ1 ùëùùëù(ùëñùëñ) = ùëùùëù(ùëóùëó)0 ùëùùëù(ùëñùëñ) ‚â† ùëùùëù(ùëóùëó)
ùëñ > ùëó. We take maximum five first left siblings of ùëñ-th token to avoid combinatorial explosion.
2.4.2. Convolution patterns
Inspired by rule-based approach, we added several convolution patterns of length two to four words. Maximum pattern length was taken from the rule-based approach, where we have very few patterns longer than four tokens deep. It should be mentioned that one token doesn‚Äôt equal one word, because we replace phrases with TARGE
T mark during object matching phase.
Pattern depth Patternword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent childword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent child
Entity Based Sentiment Analysis Using Syntax Patterns and Convolutional Neural Network Pattern depth Patternword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent childword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent childword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent childword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent childword parentword parent child* childwordgrand parentword parentword child * childword parent grand parent great grand parentword parent grand parent child
Asterisk in table 1 means that information about this word is not included to a convolution pattern. We also add information about the sequential token order in the tweet to compensate for parsing errors during the syntax analysis stage. The final input vector is a concatenation of feature maps from tree-based information and n-grams, with n=5.
2.4.3. Training
We substitute all ‚Äúword + PO
S‚Äù pairs are by unique ids and align all sentences to length 50 (zero padding). We take first 5 anchestors and first 5 siblings for each word in a sentence and concatenate all words to form input vector for our N
N. Neural network consists of the following layers:‚Ä¢	 embedding layer‚Äîto turn word ids to word vectores, we used only words, contained in training;‚Ä¢	 convolution layer‚Äîlayer with rectified linear unit (ReL
U) activation where convolution patterns are applied as described in table 1;‚Ä¢	 max
Pooling layer‚Äîwhich is down-sampling convolution layer output;‚Ä¢	 dropout layer‚Äîwith dropout rate was set to 0.25;‚Ä¢	 dense layer‚Äîwith ReL
U activation;‚Ä¢	 dropout layer‚Äîwith dropout rate was set to 0.5;‚Ä¢	 softmax layer‚Äîto form classification output.
We employ random dropout on penultimate layer to avoid overtraining as described in . We trained our CN
N for 40 epochs, but did not observe any increase in quality after the 2th epoch. Training was done through stochastic gradient descent over shuffled mini-batches with the Ada
Grad update rule. Trained CN
N models with exact parameters could be found at project repository, noted at section 2.2.
Karpov I. A., Kozhevnikov M. V., Kazorin V. I., Nemov N. R.
Experiments
Results of our evaluation are presented in of the RusSenti
Eval, the macro-averaged F1-measurewas used as a primary evaluation metric . Table 2 below describes positive and negative sentiment classes and micro-averaged F1.
approaches in different configuration
Domain Approach
Training collection WV
F1 positive
F1 negative
Macroaverage F1
Microaverage F1
Banks Rule-based Banks ‚Äî 0.387 0.501 0.443 0.463
Rule-based with domain rules
Banks ‚Äî 0.394 0.524 0.459 0.482CN
N Banks Random 0.425 0.555 0.490 0.523
Banks News 0.422 0.555 0.489 0.523
Banks Twitter 0.429 0.552 0.490 0.522
Banks & TT
K Random 0.446 0.618 0.532 0.574
Banks & TT
K News 0.455 0.611 0.533 0.572
Banks & TT
K Twitter 0.456 0.615 0.536 0.574
Telecom Rule-based TT
K ‚Äî 0.280 0.682 0.481 0.569
Rule-based with domain rulesTT
K ‚Äî 0.285 0.695 0.490 0.582CN
N TT
K Random 0.097 0.556 0.326 0.497TT
K News 0.091 0.557 0.324 0.499TT
K Twitter 0.091 0.559 0.325 0.500
Banks & TT
K Random 0.307 0.738 0.523 0.681
Banks & TT
K News 0.298 0.740 0.519 0.682
Banks & TT
K Twitter 0.313 0.739 0.526 0.682
In the table above, the column ‚Äú
Training collection‚Äù describes the collection, chosen to train the model. In case of ‚Äú
Banks & TT
K‚Äù value, model was trained on both Banks and Telecom data shuffled in random order. ‚ÄúW
V‚Äù column describes Word2
Vec model, used in the experiment. Results in Table 2 demonstrate that training corpora size is more important than the selected V
W model. It also appears that W
V is extremely sensitive to the input data. In our case V
W, trained with only the domain specific data, shows better results that can be increased by acquiring bigger corpora.
3.1. Overall Performance
The evaluation metric used in the SentiRu
Eval 2016 competition is the macroaveraged F1 measure calculated over the positive and negative classes. Table 3 shows the overall performance of our system for bank and telecom datasets.
Entity Based Sentiment Analysis Using Syntax Patterns and Convolutional Neural Network F1 measure among all participants
Domain Approach
F1 positive
F1 negative
Macroaverage F1
Microaverage F1
Banks Rule-based 0.394 0.524 0.459 0.482CN
N 0.456 0.615 0.536 0.574
Hybrid 0.457 0.619 0.538 0.577SentiRu
Eval best 0.552
Telecom Rule-based 0.285 0.695 0.490 0.582CN
N 0.313 0.739 0.526 0.682
Hybrid 0.313 0.740 0.527 0.684SentiRu
Eval best 0.559
In case of rule-based approach, the system was not developed for banks or telecom companies‚Äô domains specially. Rule-based approach did not use any machine learning. Training collection was used only for extracting the proposed domain-specific rules, which approximately increased macro-average F-measure by 0.015.
With the Hybrid approach, final sentiment marks of neutral tweets, gained from rule-based approach, are inputs for a CN
N. In general, rules give more precise result, but fail in recall. This method shows small performance progress in case of telecom domain, but does not help in bank domain, which may be caused by overfitting when multiple rules interfere each other.
4. Conclusions
We presented results of sentiment analysis on Twitter by building two approaches based on hand-written syntactic rules and CN
N. Rule-based linguistic method showed average performance result, which makes it useful when training collection is not available. Few hand-written rules with well-filtered dictionaries can give a little boost to the CN
N output, but the system degrades as rules count increases. CN
N show very high quality result that coincides with the best results of the competition, but this approach requires relatively large training collections. The same problem occurs in distributive semantics, applied in this work. Word2vec can extract deep semantic features between words if training corpora is large enough.
Acknowledgment
The article was prepared within the framework of a subsidy granted to the HS
E by the Government of the Russian Federation for the implementation of the Global Competitiveness Program.
Karpov I. A., Kozhevnikov M. V., Kazorin V. I., Nemov N. R.
