The goal of text simplification (or T
S, in short) is to reduce the linguistic complexity of the given textfragment to improve its readability and to make it easier to understand. Text complexity depends onthe presence of participial and adverbial constructions, complex grammatical structures, infrequent andambiguous words, and subordinate sentences. Thanks to its numerous applications, the T
S problem hasreceived significant attention in Natural Language Processing (or NL
P). For instance, it may simplifycommunication for non-native speakers and people with cognitive disorders such as aphasia or dyslexia.
In addition, text simplification can improve language model performance on such NL
P tasks as semanticrole labeling, summarization, information extraction, machine translation, etc.
Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference â€œ
Dialogue 2023â€
June 14â€“16, 2023
Text simplification as a controlled text style transfer task
One standard approach to solving this task is to fine-tune a pre-trained language model on a large textcorpus containing aligned complex and simplified sentences.
In this paper, we step aside from this paradigm and consider T
S as a text style transfer task, regardingthe â€œsimplicity of the textâ€ as a particular style. For this purpose, we use methods of controllable textgeneration. Namely, the Ge
Di algorithm proposed in (
Krause et al., 2020) and further developed in(
Dale et al., 2021). Following their methodology we use a paraphrase model (the main model) guidedby another language model conditioned for the â€œsimpleâ€ style (or Ge
Di-classifier). The choice of suchan approach was motivated by its several advantages compared to standard fine-tuning of the pre-trainedlanguage model. First, it does not change the main language model. The trained Ge
Di-classifier canbe used with different main models (for example, rewriter based on RuT5
Large, rewriter based onRuT5-X
L, summarizer based on RuT5
Large, summarizer based on RuT5
Large, etc.), which gives morefreedom for its usage. Thus, it simplifies the fine-tuning process as the classifier should only be trainedonce and then can be used in combination with various main models. Second, we can train severalGe
Di-classifiers with different target styles (sentiment, simplification, toxicity, etc.) and use them withany of the main language models we have. Thus, we only need to fine-tune ğ‘€ğ‘€ main models and ğ‘ğ‘Ge
Di-classifiers instead of fine-tuning ğ‘ğ‘ *ğ‘€ğ‘€ models for each combination.
In this work, we perform a series of experiments on the simplification dataset from theRuSimpleSent
Eval-2021 Shared Task (
Sakhovskiy et al., 2021). We compare the controllable text styletransfer approach with standard fine-tuning of autoregressive language models and show that Ge
Di-basedapproach of controllable text style transfer achieves quality comparable with standard fine-tuning.
The rest of the paper is structured as follows: first, in section 2 we overview the papers related to thefield of T
S and a paraphrase task, which can be regarded as its generalization, as well as the methods forcontrollable style generation. Next, in section 3 we discuss the controllable text style transfer approachwe use. Then, section 4 describes the experimental setup. Section 5 presents evaluation results. Finally,section 6 concludes the paper.
2 Related Work
The task of text simplification is a popular generation task in NL
P, useful in many applications: frompre-processing for machine translation to assistive technology for people with cognitive disorders. Thesystems of T
S improve text readability and simplify text understanding while retaining its original information content as much as possible. The automation of this process is a complex problem which hasbeen explored from many points of view. Several good extensive surveys cover the datasets and most ofthe classical methods for T
S problem (
Shardlow, 2014), (Al
Thanyyan and Azmi, 2021).
The interest and the development of T
S systems for the Russian language rapidly increased with theRuSimpleSent
Eval Shared Task (
Sakhovskiy et al., 2021), for which the authors presented the datasetand baselines. In addition, other Russian datasets exist for T
S, among which are ruBT
S (
Galeev et al.,2021) and the aligned parallel T
S dataset from language learner data (
Dmitrieva et al., 2022).
The T
S task can be considered the sub-task of the paraphrase task due to the similarity of the taskdefinition and criteria of the generated text: the format should be changed while preserving the originaltext content. For the Russian language, several paraphrase models in the open source are commonlyused, for example, paraphrased library (
Fenogenova, 2021), or models by David Dale 1. These modelswork on the sentence level. In addition, there exist a model from Sber 2 that rewrites extensive texts,which can contain many sentences.
For the evaluation of paraphrase tasks, the standard natural language generation (NL
G) metrics arecommonly used. There are surface-based metrics such as variations of BLE
U, ROUG
E, CHR
F+; andBER
T-base metrics such as LABS
E (
Feng et al., 2020) and Bert
Score (
Zhang et al., 2019). For instance,their combinations are presented in the GE
M benchmark (
Gehrmann et al., 2021). Besides, for the T
Stask, special metrics such as SAR
I (
Xu et al., 2015), included in the EASS
E 3 package and Lens (
Maddela1https://huggingface.co/cointegrated/rut5-base-paraphraser
2https://sbercloud.ru/ru/datahub/rugpt3family/demo-rewrite
3https://github.com/feralvam/easse
Tikhonova M., Fenogenova A.et al., 2022), were proposed.
The controllable text style transfer approach has received considerable attention in recent years. Oneof the pioneers in this field was (
Keskar et al., 2019), where authors use conditioned controlled codes forguided text generation.
Ge
Di (
Krause et al., 2020) uses a small external language model classifier (or simply Ge
Di-classifier)to guide the generation of the main language model, re-weighting next token probabilities and, thus,increasing the probabilities of words in the given style. ParaGe
Di (
Dale et al., 2021) adopts this ideato the paraphrasing task by applying the Ge
Di approach in combination not with the standard languagemodel but with the paraphraser fine-tuned to rephrase the original text preserving its original meaning.
In (
Liu et al., 2021) the authors proposed D
Experts. Their approach uses two extra language modelsconditioned towards and against the desired style (or topic), which are used to re-weight the probabilitiesof the next tokens predicted by the main language model.
(
Yang and Klein, 2021) explores the usage of text classifiers for controllable text generation withFUDG
E. This idea is further developed in (
Sitdikov et al., 2022), where authors proposed CAI
Fsampling, which is a method for controllable text generation based on re-weighting logits with a freeform classifier.
Thus, while most methods for controllable text style transfer concentrate on controllable text generation in a given style, we focus on the task of paraphrasing the original text in a given style, preservingthe meaning and applying the ideas from the ParaGe
Di method for text simplification, regarding the simplicity of the text as a specific style. It should also be noted that while the work ParaGe
Di uses GP
T-2language models, we use RuT5
Large based models. In other words, both components are derived fromthe same pre-trained language model version. Such an approach avoids problems with the difference inthe vocabulary in the process of fine-tuning.
In addition, we perform our research for the Russian Language, which distinguishes our work fromthe papers mentioned above, which concentrate on English.
3 Method
Besides the standard approach of fine-tuning a pre-trained language model used as a baseline for thestyle-transfer experiments, we consider several versions of controlled text generation models based onthe Ge
Di algorithm proposed in (
Krause et al., 2020). In it a language model performs text generationguided by another language model conditioned for the specific topic or style or topic. More precisely,in our work, we adopt the extension of this method presented in (
Dale et al., 2021), where the authorsenable the model not only to generate but to paraphrase the input text. Below, a brief description of themethod is given.
3.1 Ge
Di
In the original Ge
Di algorithm, the whole model consists of two parts. The first component is a generationautoregressive model. The second model is an autoregressive discrimination model, trained on sentenceslabeled with a specific style or topic, which we will further refer to as Ge
Di-classifier. Thus, in theprocess of training Ge
Di-classifier learns the word distributions conditioned on a particular label. Ateach generation step, the distribution of the next token predicted by the main language model ğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ isadjusted using the Bayes rule and an additional class-conditional language model ğ‘ƒğ‘ƒğ·ğ·:ğ‘ƒğ‘ƒ (ğ‘¥ğ‘¥ğ‘¡ğ‘¡|ğ‘¥ğ‘¥<ğ‘¡ğ‘¡, ğ‘ğ‘) âˆ ğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ (ğ‘¥ğ‘¥ğ‘¡ğ‘¡|ğ‘¥ğ‘¥<ğ‘¡ğ‘¡)ğ‘ƒğ‘ƒğ·ğ·(ğ‘ğ‘|ğ‘¥ğ‘¥ğ‘¡ğ‘¡, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡)
Here, ğ‘¥ğ‘¥ğ‘¡ğ‘¡ is the current token, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡ is the prefix of the text, and ğ‘ğ‘ is the desired style (e.g. simplicity orsentiment) â€” one of ğ¶ğ¶ classes. The first term in the formula is predicted by the main language modelğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ . The second term is calculated using Ge
Di-classifier ğ‘ƒğ‘ƒğ·ğ·ğ·ğ· via the Bayes rule. As a result the tokenswhich are more likely to appear in a text of the chosen style receive a higher probability:ğ‘ƒğ‘ƒğ·ğ·(ğ‘ğ‘|ğ‘¥ğ‘¥ğ‘¡ğ‘¡, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡) âˆ ğ‘ƒğ‘ƒ (ğ‘ğ‘)ğ‘ƒğ‘ƒğ·ğ·ğ·ğ·(ğ‘¥ğ‘¥, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡|ğ‘ğ‘)
Text simplification as a controlled text style transfer task
In the original paper, Ge
Di was successfully used for guided text generation with GP
T-2 languagemodel making the generation of the less toxic texts.
3.2 ParaGe
Di
In our work, we adopt the approach of ParaGe
Di, where the authors enable Ge
Di to preserve the meaningof the input text. For this, they replace the language model with a paraphraser. Thus, ParaGe
Di modelsthe following probability:ğ‘ƒğ‘ƒ (ğ‘¦ğ‘¦ğ‘¡ğ‘¡|ğ‘¦ğ‘¦<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥, ğ‘¥ğ‘¥) âˆ ğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ (ğ‘¦ğ‘¦ğ‘¡ğ‘¡|ğ‘¦ğ‘¦<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥)ğ‘ƒğ‘ƒ (ğ‘¥ğ‘¥|ğ‘¦ğ‘¦ğ‘¡ğ‘¡, ğ‘¦ğ‘¦<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥) â‰ˆ ğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ (ğ‘¦ğ‘¦ğ‘¡ğ‘¡|ğ‘¦ğ‘¦<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥)ğ‘ƒğ‘ƒğ·ğ·(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦ğ‘¡ğ‘¡, ğ‘¦ğ‘¦<ğ‘¡ğ‘¡)where ğ‘¥ğ‘¥ is the original text, ğ‘¦ğ‘¦ is the generated text of length ğ‘‡ğ‘‡ , and ğ‘¥ğ‘¥ is the desired style.
The last transition in the equation above is an approximation which allows us to decouple the paraphraser from the Ge
Di-classifier model. As a result, the paraphraser and the Ge
Di-classifier can betrained independently in such a formulation.
As for the training process, ParaGe
Di loss â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ·ğ·ğ‘ƒğ‘ƒ consists of two components: the generativeloss â„’ğ‘ƒğ‘ƒ used in language model training and the discriminative loss â„’ğ·ğ· which further pushes differentclasses away from one another.
â„’ğ‘ƒğ‘ƒ = âˆ’ 1ğ‘ğ‘ğ‘ğ‘âˆ‘ï¸ğ‘ƒğ‘ƒ=1ğ‘‡ğ‘‡ğ‘ƒğ‘ƒğ‘‡ğ‘‡ğ‘–ğ‘–âˆ‘ï¸ğ‘¡ğ‘¡=1logğ‘ƒğ‘ƒ (ğ‘¦ğ‘¦(ğ‘ƒğ‘ƒ)ğ‘¡ğ‘¡ |ğ‘¦ğ‘¦(ğ‘ƒğ‘ƒ)<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥(ğ‘ƒğ‘ƒ))â„’ğ·ğ· = âˆ’ 1ğ‘ğ‘ğ‘ğ‘âˆ‘ï¸ğ‘ƒğ‘ƒ=1logğ‘ƒğ‘ƒ (ğ‘¥ğ‘¥(ğ‘ƒğ‘ƒ)|ğ‘¦ğ‘¦(ğ‘ƒğ‘ƒ)1:ğ‘‡ğ‘‡ğ‘–ğ‘–)â„’ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ·ğ·ğ‘ƒğ‘ƒ = ğœ†ğœ†â„’ğ·ğ· + (1âˆ’ ğœ†ğœ†)â„’ğ‘ƒğ‘ƒwhere ğœ†ğœ† âˆˆ the weight of the discriminative loss.
Besides, to improve the preservation of the original content and to increase the style transfer accuracy,the following heuristics are used:
First, the conditional language model probability is raised to the power ğ‘¤ğ‘¤ ğ‘¤ 1, which biases thediscriminator towards the correct class in the process of generation:ğ‘ƒğ‘ƒ (ğ‘¦ğ‘¦ğ‘¡ğ‘¡|ğ‘¦ğ‘¦<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥, ğ‘¥ğ‘¥) âˆ ğ‘ƒğ‘ƒğ¿ğ¿ğ¿ğ¿ (ğ‘¦ğ‘¦ğ‘¡ğ‘¡|ğ‘¦ğ‘¦<ğ‘¡ğ‘¡, ğ‘¥ğ‘¥)ğ‘ƒğ‘ƒğ¶ğ¶ğ¶ğ¶(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦ğ‘¡ğ‘¡, ğ‘¦ğ‘¦<ğ‘¡ğ‘¡)ğ‘¤ğ‘¤
Second, probabilities are smoothed by adding a small ğ›¼ğ›¼ ğ‘¤ 0 to all probabilities from the conditionallanguage model:ğ‘ƒğ‘ƒğ›¼ğ›¼(ğ‘¥ğ‘¥|ğ‘¥ğ‘¥ğ‘¡ğ‘¡, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡) =ğ›¼ğ›¼+ ğ‘ƒğ‘ƒ (ğ‘¥ğ‘¥)ğ‘ƒğ‘ƒğ¶ğ¶ğ¶ğ¶(ğ‘¥ğ‘¥, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡|ğ‘¥ğ‘¥)âˆ‘ï¸€ğ‘ğ‘â€²âˆˆğ¶ğ¶ (ğ›¼ğ›¼+ ğ‘ƒğ‘ƒ (ğ‘¥ğ‘¥â€²)ğ‘ƒğ‘ƒğ¶ğ¶ğ¶ğ¶(ğ‘¥ğ‘¥, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡|ğ‘¥ğ‘¥â€²))
Such a heuristic discourages the generation of tokens with low probability conditional on all classes.
Third, for class-conditional corrections, asymmetric lower and upper bounds (ğ‘™ğ‘™ and ğ‘¢ğ‘¢) are used :ğ‘ƒğ‘ƒğ›¼ğ›¼ğ›¼ğ›¼ğ›¼ğ›¼ğ›¼ğ›¼(ğ‘¥ğ‘¥|ğ‘¥ğ‘¥ğ‘¡ğ‘¡, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡) = max(ğ‘™ğ‘™,min(ğ‘¢ğ‘¢, ğ‘ƒğ‘ƒğ›¼ğ›¼(ğ‘¥ğ‘¥|ğ‘¥ğ‘¥ğ‘¡ğ‘¡, ğ‘¥ğ‘¥<ğ‘¡ğ‘¡))).
This discourages the insertion of new tokens, as opposed to prohibiting existing tokens.
4 Experiments
4.1 Data
We perform a series of experiments on the dataset RuSimpleSent
Eval-2021 Shared Task (
Sakhovskiy etal., 2021). This simplification dataset contains parallel pairs of sentences: complex â€“ their correspondingsimplified versions. Below, a sample from the dataset is presented.
Tikhonova M., Fenogenova A.
Example from the dataset:
Source sentence:â€œĞšĞ»Ğ¸Ğ¼Ğ°Ñ‚ ĞšĞ°Ğ·Ğ°Ğ½Ğ¸ â€“ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ , ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ñ‹ Ğ¸ Ğ¿Ğ°Ğ»ÑÑ‰Ğ°Ñ Ğ¶Ğ°Ñ€Ğ° Ñ€ĞµĞ´ĞºĞ¸Ğ¸ Ğ½Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°â€
Simplified paraphrases:1. â€œĞ’ ĞšĞ°Ğ·Ğ°Ğ½Ğ¸ Ñ€ĞµĞ´ĞºĞ¾ Ğ±Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ñ‹, Ğ¸ Ğ¶Ğ°Ñ€ĞºĞ°Ñ Ğ»ĞµÑ‚Ğ½ÑÑ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°â€
2. â€œĞ’ ĞšĞ°Ğ·Ğ°Ğ½Ğ¸ Ğ·Ğ¸Ğ¼Ğ¾Ğ¹ Ğ½Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾, Ğ° Ğ»ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¶Ğ°Ñ€ĞºĞ¾â€
3. â€œĞ’ ĞšĞ°Ğ·Ğ°Ğ½Ğ¸ Ğ·Ğ¸Ğ¼Ğ¾Ğ¹ Ğ½Ğµ Ğ¾Ñ‡ĞµĞ½ÑŒ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾, Ğ° Ğ»ĞµÑ‚Ğ½ĞµĞ¹ Ğ¶Ğ°Ñ€Ñ‹ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğµ Ğ±Ñ‹Ğ²Ğ°ĞµÑ‚â€
The organizers of the RuSimpleSent
Eval-2021 shared task constructed the dataset using automatictranslation and post-processing Wiki
Large corpus (
Zhang and Lapata, 2017). The resulting dataset wassplit into the train, dev and two test sets (public and private). And while the train set was not filteredor verified, the organizers validated the dev, public and private test sets via crowd-sourcing using Yandex.
Toloka 4 and filtered them. In this work, we evaluate the results on official public and private testsets. We additionally filtered the train part, which contains inappropriate examples due to its originalautomatic construction. For its cleaning, we used the following procedure: exclude examples with lessthan two lemmas in the intersection between the lemmatized source and target sentences (lemmatization was done via pymorphy2 5 tagger (
Korobov, 2015)); discard examples where the source sentenceis a sub-string of the target one and the length is bigger than of the source one. Besides training andvalidation, we also use extra dev set filtered by the organizer.
4.2 Models
We conduct experiments and compare the results of the following models:â€¢ Golden testset. We evaluate the golden references (first answer) from the fixed RuSimpleSent
Eval2021 test sets (public/private);â€¢ Paraphraser. We use a paraphrase model 6 trained on 7000 examples from different sources ofvarious domains: 1) text level: literature domain, prose; back translation (with ru-en translationmodel 7) of the texts from different domains filtered with Bertscore Rouge
L); 2) sentence level:
Russian version of Tapaco corpus (
Scherrer, 2020) and filtered Paraphraser
Plus (
Gudkov et al.,2020) corpus.
â€¢ Fine-tuned paraphraser. We additionally fine-tune the paraphrase model on the train set to checkthe hypothesis of the capabilities combinations that the model learn (both paraphrasing and simplification);â€¢ Fine-tuned ruT5
Large 8. We fine-tune the row ruT5
Large model on the simplification train set.
â€¢ ParaGe
Di. We train Ge
Di-classifier on the train part of the RuSimpleSent
Eval-2021 set and usethe paraphrase model described above as the main model for ParaGe
Di controllable approach.
In our work, all models we use are derived from the pre-trained RuT5
Large9 model, which is a T5model (
Raffel et al., 2020) pre-trained for the Russian language. The fact that we derive both componentsfrom the same model allows us to avoid problems with the difference in the model vocabulary.
As for the Ge
Di-classifier model, we fine-tune RuT5
Large on the RuSimpleSent
Eval-2021 Shared
Task train set. We use the Adam optimizer with the learning rate 1ğ‘’ğ‘’âˆ’ 4, three epochs, and the weight ofthe discriminative loss ğœ†ğœ† = 0.3.
We evaluate several style power coefficients (ğ‘¤ğ‘¤ = 5, 10, 15, 20). It should also be noted that we do notevaluate ğ‘¤ğ‘¤ = 0 as, in this case, the influence of the Ge
Di-calssifier is neglected, and the result is equalto the original paraphrase model, which is our baseline. To avoid randomness, we use the followinggeneration parameters:4https://toloka.ai/tolokers/
5https://github.com/pymorphy2/pymorphy2
6https://habr.com/ru/company/sberdevices/blog/667106/
7https://huggingface.co/Helsinki-NL
P/opus-mt-en-ru
8https://huggingface.co/sberbank-ai/ru
T5-large
9https://huggingface.co/sberbank-ai/ruT5-large
Text simplification as a controlled text style transfer task
â€¢ ğ‘‘ğ‘‘ğ‘‘ğ‘‘_ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  = ğ¹ğ¹ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ,â€¢ ğ‘›ğ‘›ğ‘›ğ‘›ğ‘ ğ‘ _ğ‘Ÿğ‘Ÿğ‘ ğ‘ ğ‘Ÿğ‘Ÿğ‘›ğ‘›ğ‘Ÿğ‘Ÿğ‘›ğ‘›ğ‘ ğ‘ ğ‘‘ğ‘‘_ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘›ğ‘›ğ‘ ğ‘ ğ‘›ğ‘›ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  = 1,â€¢ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘šğ‘š_ğ‘ ğ‘ ğ‘ ğ‘ ğ‘›ğ‘› = 128.
4.3 Metrics
We evaluate the model on public and private test sets of RuSimpleSent
Eval-2021 Shared Task using thefollowing metrics:â€¢ BertScore(
Zhang et al., 2019), which is computed between the original (complex) sentences andmodel predictions.
â€¢ SAR
I (
Xu et al., 2016), which is commonly recognized as a metric for evaluating automatic textsimplification systems. The metric compares the model predictions against the references and theoriginal (complex) sentences.
â€¢ BLE
U score(
Papineni et al., 2002), which in our case is computed between the reference answersand predictionsâ€¢ iBLE
U (
Sun and Zhou, 2012) which is computed as follows:ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– = ğ‘ ğ‘  *ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘ ğ‘ ğ‘Ÿğ‘Ÿğ‘ ğ‘ ğ‘‘ğ‘‘ğ‘ ğ‘ ğ‘ ğ‘Ÿğ‘Ÿğ‘ ğ‘ ğ‘ğ‘ğ‘ ğ‘ ) + (1âˆ’ ğ›¼ğ›¼) *ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–(ğ‘ ğ‘ ğ‘Ÿğ‘Ÿğ‘ ğ‘ ğ‘‘ğ‘‘ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘‘ğ‘‘ğ‘›ğ‘›ğ‘Ÿğ‘Ÿğ‘ ğ‘ ğ‘ ğ‘ )ğ‘where ğ›¼ğ›¼ is the parameter responsible for the balance between adequacy and dissimilarity. In ourwork, we follow the methodology from the original paper and use ğ›¼ğ›¼ = 0.9.
â€¢ Diversity We report a degree of diversity measured using the mean number of distinct n-grams,normalized by the length of text (
Li et al., 2015). We report dist-1, dist-2, and dist-3 scores fordistinct uni-, bi-, and trigrams, respectively.
5 Results
Results on public and private test sets are presented in Tables 1 and 2, respectively. The results reveal thatthe Ge
Di-based approach with style power coefficients of 5 and 10 shows quality comparable with thestandard fine-tuning approach. Larger values of the style power coefficient lead to a decrease in qualityas the classifier influence becomes too strong, which negatively affects the generated output. Thus, theParaGe
Di-based approach can be considered a good alternative to standard fine-tuning. In addition, aslong as it does not change the initial model and can be used with different main models, it gives morefreedom for its usage.
Model Bert
Score SAR
I BLE
U iBLE
U 0.9 dist 1 dist 2 dist 3
Golden testset 0.816874 66.106573 1.0 0.916141 0.971855 0.940157 0.882364
Paraphraser 0.925663 41.004799 0.314653 0.342387 0.964854 0.923054 0.855773F
T paraphraser 0.970198 41.594171 0.367276 0.412937 0.974326 0.932282 0.866955F
T ruT5
Large 0.969541 41.819602 0.369884 0.415395 0.974098 0.931853 0.866066ParaGe
Di (sp 5) 0.914065 40.792974 0.310180 0.332548 0.965152 0.919561 0.848917ParaGe
Di (sp 10) 0.888886 40.501325 0.295284 0.307751 0.969362 0.911230 0.831918ParaGe
Di (sp 15) 0.826108 38.539389 0.256159 0.255457 0.882723 0.815006 0.731320ParaGe
Di (sp 20) 0.659992 33.045052 0.081489 0.075360 0.401245 0.356622 0.307940different Style Power coefficients (sp in shortly). F
T stands for fine-tuned. Detailed metrics descriptionsare given in subection 4.3.
In addition, we compared our results with the top-3 solutions of the RuSimpleSent
Eval-2021 competition (
Sakhovskiy et al., 2021), which include qbic solution based on Multilingual Unsupervised Sentence
Simplification (
Martin et al., 2020) and fine-tuned GP
T-based solutions by orzhan, ashatilov, and alenusch. To complete the picture, we also included mBAR
T-based (
Liu et al., 2020) baseline presented bythe organizers. Results are presented in Ru
T5-based) surpass the baseline. Second, most of them, including the ParaGe
Di method with reasonable style power coefficient of 5 and 10, outperform competition winners (mostly GP
T-based) showing
Tikhonova M., Fenogenova A.
Model Bert
Score SAR
I BLE
U iBLE
U 0.9 dist 1 dist 2 dist 3
Golden testset 0.816874 66.106573 1.0 0.967823 0.940655 0.883676 0.882364
Paraphraser 0.92467 40.418701 0.301265 0.330843 0.961526 0.922913 0.857691F
T paraphraser 0.968782 41.643578 0.358353 0.404432 0.968473 0.931082 0.866247F
T ruT5
Large 0.965881 41.517535 0.357556 0.402777 0.969426 0.929413 0.863115ParaGe
Di (sp 5) 0.912825 40.859850 0.300608 0.324721 0.961111 0.918092 0.848473ParaGe
Di (sp 10) 0.887088 40.240902 0.274954 0.289805 0.960448 0.907891 0.830453ParaGe
Di (sp15) 0.824515 38.249361 0.255155 0.255730 0.873924 0.810920 0.730028ParaGe
Di (sp 20) 0.668402 33.238699 0.098595 0.091794 0.432894 0.389271 0.339774coefficients (sp in shortly). F
T stands for fine-tuned. Detailed metrics descriptions are given in subection 4.3.
higher SAR
I scores. Such results can be regarded as another proof of the quality of the ParaGe
Di approach. In addition, such results indicates that Ru
T5 is a better backbone for the text simplification taskthan the GP
T-based models. We observe the same trends on the T
S task in the GE
M benchmark 10. The
T5-small model shows the best performance on the analogous datasets for English, among which arewiki auto, asset turk, and test turk datasets (
Xu et al., 2016)).
Model SARI
Golden testset 66.106F
T ruT5
Large 41.819F
T paraphraser 41.594
Paraphraser 41.004ParaGe
Di (sp 5) 40.792ParaGe
Di (sp 10) 40.501â„–1 orzhan 40.233â„–2 alenusch 38.870â„–3 ashatilov 38.843ParaGe
Di (sp 15) 38.539ParaGe
Di (sp 20) 33.045BASELIN
E 30.152
Model SARI
Golden testset 66.106F
T paraphraser 41.643F
T ruT5
Large 41.517
Paraphraser 40.418ParaGe
Di (sp 5) 40.859ParaGe
Di (sp 10) 40.240â„–1 qbic 39.689â„–2 orzhan 39.279â„–3 ashatilov 38.491ParaGe
Di (sp 15) 38.249ParaGe
Di (sp 20) 33.238BASELIN
E (left) and private (right) test sets respectively. Following the original competition approach, we compareresults using the SAR
I metric. sp stands for style power. F
T stands for fine-tuned. BASELIN
E stands forthe mBAR
T-based baseline from RuSimpleSent
Eval-2021 competition.
Analyzing modelâ€™s performance on the concrete examples, we explored the behavior of Fine-tunedparaphraser, which showed best performance on the private test set (see Table 4). It can be seen thatthe model preserves the original meaning of the sentence and does not change facts. However, it onlyslightly simplifies sentences replacing individual words rather than rewriting the whole sentence in amore simplistic style. Thus, there is still room for improvement.
As for the limitations of the ParaGe
Di method, with the growth of the style power coefficient, ParaGi
Distarts to lose coherence and forget the original sentenceâ€™s meaning. While the generations with the stylepower coefficients 5 or 10 yields reasonable quality, sentences generated with the coefficient 20 often lack10https://gem-benchmark.com/results
Text simplification as a controlled text style transfer task
Original sentence PredictionĞ”Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ° ÑĞ»ĞµĞ´Ğ¾Ğ² Ñƒ Ğ²Ğ¾Ğ»ĞºĞ° Ñ€Ğ¾Ğ²Ğ½ĞµĞµ Ğ”Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ° ÑĞ»ĞµĞ´Ğ¾Ğ² Ñƒ Ğ²Ğ¾Ğ»ĞºĞ° Ñ€Ğ¾Ğ²Ğ½Ğ°Ñ,Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ñ€Ğ¾Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ¾Ñ‡ĞºÑƒ, Ğ° Ñƒ ÑĞ¾Ğ±Ğ°Ğº â€” Ğ¸Ğ·Ğ²Ğ¸Ğ»Ğ¸ÑÑ‚Ğ°Ñ Ğ»Ğ¸Ğ½Ğ¸Ñ.
Ğ° Ñƒ ÑĞ¾Ğ±Ğ°Ğº â€” Ğ¸Ğ·Ğ²Ğ¸Ğ»Ğ¸ÑÑ‚ÑƒÑ Ğ»Ğ¸Ğ½Ğ¸Ñ.
Ğ’ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ³Ğ¾Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ° Ğ¾Ñ„Ğ¸Ñ Ğ’ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ³Ğ¾Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ° Ğ¾Ñ„Ğ¸ÑĞ² ĞšĞ°Ğ·Ğ°Ğ½Ğ¸; Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ğ»Ğ°, Ğ² ĞšĞ°Ğ·Ğ°Ğ½Ğ¸. ĞœĞ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº ÑÑ‚Ğ°Ğ»Ğ¸Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº ÑÑ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ ĞºĞ°Ğº Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼,ĞºĞ°Ğº Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ‚Ğ°Ñ‚Ğ°Ñ€ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ‚Ğ°Ñ‚Ğ°Ñ€ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.
Ğ’ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ¿Ğ¾ÑĞ²Ğ¸Ğ²ÑˆĞµĞ¹ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ’ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºÑˆĞµĞ¹ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑĞ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ²Ñ€Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑĞ»Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ²Ñ€Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞµĞ³Ğ¾Ğ² Ğ¾Ğ´Ğ½Ñƒ Ğ¸Ğ· Ğ¼Ğ¾ÑĞºĞ¾Ğ²ÑĞºĞ¸Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ğº. Ğ² Ğ¾Ğ´Ğ½Ñƒ Ğ¸Ğ· Ğ¼Ğ¾ÑĞºĞ¾Ğ²ÑĞºĞ¸Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ğº.
Ğ’ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğµ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… ĞµĞ³Ğ¸Ğ¿Ñ‚ÑĞ½ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ’ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğµ Ğ´Ñ€ĞµĞ²Ğ½Ğ¸Ñ… ĞµĞ³Ğ¸Ğ¿Ñ‚ÑĞ½ Ğ±Ñ‹Ğ»Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ñ…,Ğ³Ğ¾Ñ€Ğ¾Ñ…, Ğ±Ğ¾Ğ±Ñ‹ Ğ¸ Ğ½ÑƒÑ‚, Ğ¾Ğ³ÑƒÑ€Ñ†Ñ‹, Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ±Ğ¾Ğ±Ñ‹ Ğ¸ Ğ¾Ğ³ÑƒÑ€Ñ†Ñ‹, Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°Ğ»ÑÑ ÑĞ°Ğ»Ğ°Ñ‚-Ğ»Ğ°Ñ‚ÑƒĞº. Ğ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°Ğ»ÑÑ ÑĞ°Ğ»Ğ°Ñ‚-Ğ»Ğ°Ñ‚ÑƒĞº.
ĞÑ‚Ğ»Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ¾Ğ³Ñ€ĞµÑ‚Ñ‹Ğµ ĞÑ‚Ğ»Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚Ğ“Ğ¾Ğ»ÑŒÑ„ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ¼, Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑÑÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ·Ğ¸Ğ¼Ñ‹; Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ·Ğ¸Ğ¼Ñ‹, Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ·Ğ¸Ğ¼Ğ¾Ğ¹Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ·Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ·Ğ´ĞµÑÑŒ Ğ±Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½ĞµĞ³Ğ¾Ğ¿Ğ°Ğ´Ñ‹,Ğ·Ğ´ĞµÑÑŒ Ğ±Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½ĞµĞ³Ğ¾Ğ¿Ğ°Ğ´Ñ‹, Ñ…Ğ¾Ñ‚Ñ ÑĞ½ĞµĞ³ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ½ĞµĞ´Ğ¾Ğ»Ğ³Ğ¾.
Ñ…Ğ¾Ñ‚Ñ ÑĞ½ĞµĞ³ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ½ĞµĞ´Ğ¾Ğ»Ğ³Ğ¾.
meaning. In addition, as long as the ParaGe
Di approach uses two language models, it works slower andrequires more computational resources during the inference stage compared to the fine-tuned languagemodels.
6 Conclusion
In this paper, we dealt with the text simplification problem regarding it as a special case of text styletransfer task. We adopted the ParaGe
Di method, which uses the idea of controlled text style transfer.
We used the combination of two RuT5
Large models (paraphrase model and Ge
Di-classifier) to solvethis task. In the experiments, that approach proved quite promising; the results are comparable to finetuning for the single style class. The ru
T5-based simplification models surpassed the best results on theRuSimpleSent
Eval-2021 shared task.
As a part of future research, we plan to consider the reverse problem of making the text more complexand official. Thus, we plan to explore the capabilities of the models, which can work in both directions:simplifying the text or making it more complex and official.
