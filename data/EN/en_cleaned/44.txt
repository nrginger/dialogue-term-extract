As an attention module of an advanced classification model traverses a complex sentence or a documentsequentially, it may become confused as to which phrases pertain to the document’s class and whichrepresent a different class from the document’s meaning, or whether certain phrases argue for or againstthe author’s position. It is possible to uncover relations between text parts with discourse parsing. The
Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference “
Dialogue 2022”
Moscow, June 15–18, 2022
Rhetorical Structure Theory (RS
T ) is the discourse framework suggesting that texts have a hierarchical, connected structure, with both intraand inter-sentential relations. A rhetorical tree shows howelementary discourse units (ED
Us) and non-elementary units combine to form the overall meaning of adocument (see Figure 1).
Обычно орутчто вирус вранье,
JointРад что у нас ещё есть благоразумные комментаторы.
Interpretation-evaluation2-3
Jointдолой власть и т.д.
долой маски,1-4
1-3
(ED
U1−3) is the nucleus, and the right constituent (ED
U4) is the satellite.
Text classification methods adopt one of two main approaches based on the discourse parsing: either(1) weighting tokens based on their position in an unlabeled discourse tree for lexicon-based analysis, or (2) combining phrases based on the discourse structure to determine an overall classscore . These methods focus mainly on sentiment analysis; several studies have also identified aconnection between the rhetorical and argumentation structures .
This paper investigates the impact of discourse parsing on document classification for argument miningin social media texts in Russian. We first improve the performance of the current RS
T parser for Russianby introducing it to the top-down paragraph parsing. Then, we investigate the text classification applyinga Tree LST
M on the predicted discourse structures. We use this module to correct thepredictions of the fine-tuned language model. The classification methods are tested on the RuAR
G-2022 shared task.
Our contributions can be highlighted as follows:• To the best of our knowledge, we are the first to explicitly analyze the effect of discourse amongopinion mining and argument classification in Russian.
• We achieve a significant improvement in RS
T discourse parsing for Russian using a top-down algorithm for unlabeled tree construction at the paragraph level.
• We propose a new method to utilize RS
T discourse structure in Tree LST
M for paragraph-level textrepresentation learning.
Our code is publicly available1.
2 Related work
Rhetorical structure in text classification: A number of early studies investigated the possibility ofopinion mining using shallow text structures derived from the discourse connectors vocabulary or based on the manual discourse annotations . The development of automatic discourse parsers for
English has strengthened research in this area. Finding the most common nucleus of rhetorical relationsin each sentence, authors of whether the sentiment lexicon can be weighted based onRS
T structure. Assuming that the nuclei of each relation encapsulate the general idea of the text, theyexplain the lack of performance improvement through the poor accuracy of the early RS
T parser SPAD
E. However, the results obtained with the SPAD
E and HILD
A in animprovement in sentiment classification when weighting words based on the depth of correspondingsubtree and nuclearities in it. Markov logic and the sentence-level discourse trees predicted by the1https://github.com/tchewik/discourse-aware-classification
Chistova E., Smirnov I.HILD
A parser are used in calculate the sentiment score using information about contrastive(
Contrast, Concession) and non-contrastive (the rest of RST-D
T relations) rhetorical relations occurringbetween elementary discourse units.
More recent approaches explore the integration of the explicit discourse structure in deep learningmodels. In , sentiment scores are propagated recursively up the RS
T tree to the root via a neuralnetwork with architecture specific for each parse, and scalar parameters related to particular relationsare tuned. They do not construct latent representations of discourse units and also train a simplifiedversion of the DPL
P RS
T parser , focusing only on distinguishing contrastive and non-contrastiverelations. Method the trainable representations of discourse units at all levels. The authorspropose to build a shared text vector representation for a discourse tree node based on the composition ofrepresentations of individual ED
Us and subtrees. A weighting of the importance of individual discourseunits is automated through the attention mechanism. They test the method on multiple text classificationtasks. In , the authors apply Tree LST
M to the unlabeled sentence-level RS
T trees with nuclearities.
Binary Tree LST
M in their method does not process left and right children of the current node, but ratherits predefined nucleus and satellite (or two nuclei). The use of the DPL
P parser to construct structuralneural networks is also demonstrated in . They construct RecN
N Tree LST
M . Toreduce the complexity of the neural network to be constructed, they consider individual sentences ratherthan ED
Us as leaves of the discourse tree. In the representation of each discourse tree node, the textembedding and the rhetorical relation embedding are concatenated; the sentence embeddings are trainedindependently. In authors propose a Tree LST
M model similar to the one proposed in additional tree nodes augmentation. In their study, they predict the polarity of each ED
U usingdictionaries and word embeddings and found that incorporating embeddings leads to strong overfittingin the Tree LST
M models.
Argument mining using RS
T annotation: Argument mining is known to benefit from discourse analysis. It has been shown certain semantic groups of discourse connectors are indicative of eitherclaims or premises and can be used to differentiate between the two. There are certain argumentativerelations in RS
T that represent supportive, incentive, justification, and persuasion arguments, as outlinedin . Communicative discourse structure inspired by RS
T is used in categorize texts as beingeither argumentative or non-argumentative. The authors of combining a BER
T-based classifier with a gradient boosting model based on a rhetorical relation label in the root of the discourse tree.
Examples of how the classifier on discourse relations corrects the predictions of BER
T are given in order to illustrate how some RS
T relations, such as Evaluation or Antithesis, correlate with argumentativeones. TreeLST
M over RS
T structure is probed for argumentation mining in . This module is usedto obtain a vector representation of the text (the root of the rhetorical tree) based on ED
U embeddings,which are formed by concatenating word, sentence, and part-of-speech tag embeddings.
In this work, we propose a TreeLST
M-based text classification method for argument mining. Current text classification methods using TreeLST
M over RS
T structures, usually designed for sentimentanalysis, are subject to strong overfitting due to the high dimensionality of discourse unit embeddingstrained jointly with the recursive neural module. The key difference between our work and previouswork is that we do not train TreeLST
M from scratch in conjunction with the text encoder, but insteaduse the module to refine predictions of a high-performance sequential text classifier on documents withrhetorical structure.
3 Improving discourse parsing for Russian
This section describes the end-to-end RS
T parsing method we later use in text classification. We proposeconstructing unlabeled trees at the paragraph level by using a top-down approach, which improves thestructure awareness of the recent discourse parser for Russian.
Method: RS
T parser for Russian recently proposed in proven to be highly accurate for relationclassification and ED
U segmentation, although its greedy bottom-up tree-building algorithm limits itsoverall performance for document parsing. However, the method takes on the challenge of segmenting
Discourse-aware text classification for argument mininglong texts into separate discourse trees despite weakly paragraph related tree boundaries, a feature of the
Russian RS
T corpus RuRS
Treebank disallows direct application of state-of-the-art unlabeledtree construction methods (1 document = 1 tree) developed for other languages. Therefore, for ourexperiments, we reproduce , but replace the sentenceand paragraph-level unlabeled tree constructionmethods in the parser with the recent top-down parsing approach proposed in the assumptionthat each paragraph corresponds to a separate subtree. As opposed to prior top-down discourse parsingmethods considered each span separately at each time step, the novel method allowsfor comparison of subtree candidates globally at the full-tree level by computing all span boundaryrepresentations in text at each time step and using beam search to find the best subtree candidate.
Data, Results, and Discussion: We use the standard RuRS
Treebank corpus training and evaluation, focusing on two genres: news and blogs, and selecting 15% of data for the test. Since thereis no available language model for long documents in Russian, we rely on character and pretrainedword2vec embeddings for the initial representation of the document. For training on gold segmentation,we use the following parameters: beam size = 20, batch size = 4000 tokens. In Table 1, we compare theend-to-end discourse analysis performance at the different granularity levels between the system usinggreedy bottom-up paragraph parsing the one proposed in this study using micro-averaged standard Parseval metric . In both cases, we use the same BiLSTM-CR
F discourse segmentation modelon pretrained EL
Mo embeddings, achieving 88.4% F1 on the test set. We use word2vec and EL
Mopretrained models provided by Rus
Vectores2. For our structure-aware classification method, the parser’smost important feature is its ability to retrieve discourse structure regardless of labeled relations. Thetop-down approach improves the unlabeled tree construction (span identification) performance by 10.5%
F1 at the sentence level, 10.4% F1 at the paragraph level, and 8.9% F1 at the document level, taking intoaccount that the relations between paragraphs are in both cases detected by applying the same greedybottom-up algorithm. The full end-to-end parsing performance increases by 10.6%, 7.0%, and 6.2% F1,respectively. We publish the source code for the end-to-end parser used in our experiments3.
Method
Sentence level Paragraph level Document levelspan nuc rel full span nuc rel full span nuc rel full
Greedy 38.9 27.8 27.1 49.4 31.0 20.4 20.3 43.6 27.3 18.0 17.7
Beam search 68.5 50.6 38.1 37.7 59.8 38.8 27.5 27.3 52.5 34.2 24.2 23.9
In this study, improving parsing performance at both the sentence and paragraph levels is crucial.
Analysis of the Ru
Arg-2022 dataset reveals that each example corresponds to a single automaticallyidentifiable sentence. However, the sentence segmenter often fails to segment social media commentsproperly, because some sentences end with emojis, parentheses, ellipses, or no punctuation at all. Inaddition, social media users often write extremely long sentences that could be broken down into severalgrammatically correct shorter ones. Therefore, in some situations, it may be necessary to analyze intersentential discourse relations.
4 Discourse-aware classification method
In this section, we detail our proposed method for stance and argument classification, addressing thelimitations of unstructured full-text classification methods. We discuss the pipeline-based frameworkfor the classification of texts with or without recognizable rhetorical structure. The first stage involvesfine-tuning the sequential model on the dataset including texts of different lengths and complexity. In thesecond stage, we freeze the base model and then train a discourse-aware neural module on top of it forthe classification of texts with discourse structure.
2https://rusvectores.org/
3https://github.com/tchewik/isanlp_rst/releases/tag/v2.0
Chistova E., Smirnov I.4.1 BERT
For text classification based on token sequences, we adapt the multitask baseline model architecture proposed by the competition organizers, where two outputs are being trained simultaneously in a classifierbased on a language model. We use the Deep
Pavlov RuBER
T Conversational4 along with BER
T poolingto encode the document. This particular language model was chosen because it is pretrained on dialogueand social media texts, so it is well suited for encoding social media comments. The hidden representation is then passed through two fully-connected layers for stance and argument prediction; all parametersare trainable.
The model as it stands is used in the final pipeline for predicting labels for structure-lacking sentences(ED
Us). It is also used in the structure-aware model for the initial encoding of discourse tree nodes.
4.2 RST-LST
M
RS
T parsers represent discourse as a binary constituency tree. If the binary discourse tree is traversedfrom the bottom up, information from the left and right constituents can be combined to represent thetree node at the upper level and all the way up to the root. Our structure representation module is basedon the Binary Tree LST
M network . In Binary Tree LST
M, a non-elementary discourse unit’s hiddenand cell states are determined by the hidden and cell states of its left and right constituents rather thanthe sequence of words inside it. It allows computation over self-contained phrases within a complexdiscourse. We draw inspiration from previous work on Tree-LST
M over RS
T structure for documentclassification, but instead of classifying each node in an unlabeled RS
T tree based on text features ,or dictionary-based class scores , we use outputs of a pretrained classifier and a type of rhetoricalrelation as input features of each node to predict the only label for the rhetorical root of the document.
A single overall class label is defined for the entire text in the tasks presented in this paper. Hence,we propose a deep model for aggregation of the class labels predicted for all the discourse units in adocument by a sequential text classifier. First of all, this allows for a strong sequential text classificationmethod, one that itself takes into account some aspects of discourse . Additionally, the methodsbased on training high-dimensional ED
U representations simultaneously with Tree-LST
Ms are found tobe prone to strong overfitting . Therefore, it is important to produce D
U representations that areas compact and informative as possible, which the proposed method achieves by encoding them with apre-trained classifier.
The six types of fine-grained relations in the RuRS
Treebank corpus outlined in usedin the initial feature representation of each node. These include Coherence (
Background, Elaboration, Restatement, Interpretation-evaluation, Preparation), Causal-argumentative:
Contrastive (
Concession, Contrast, Comparison), Causal-argumentative:
Causal (
Purpose, Evidence, Cause-effect), Causalargumentative:
Condition (
Condition), Structural (
Sequence, Joint, Same-unit), and Attribution.
Considering RS
T tree 𝑡𝑡 and the current nonterminal node (nonelementary discourse unit) 𝑢𝑢𝑖𝑖 ∈ 𝑡𝑡,its left and right constituents 𝑢𝑢𝑖𝑖1 and 𝑢𝑢𝑖𝑖2 sharing relation 𝑟𝑟𝑖𝑖 = (𝑟𝑟𝑖𝑖1 , 𝑟𝑟𝑖𝑖2) (e.g., Attribution_N
S =(Attribution_
Nucleus, Attribution_
Satellite)) are initially encoded into representations 𝑈𝑈𝑖𝑖1 and 𝑈𝑈𝑖𝑖2 asfollows:𝑈𝑈𝑖𝑖𝑗𝑗 = 𝑗𝑗 = 1, 2. (1)
An additional Root relation is introduced to encode a root node that is not a constituent. We derive boththe BER
T-based text encoder Enc and the fully-connected layers for preliminary labels predictions F
Cfrom the sequence-level base model with frozen weights. Since all Structural relations are multinuclearand do not have satellites, the one-hot vector 𝑟𝑟𝑖𝑖𝑗𝑗 of discourse unit labels (Coherence_
Nucleus, Coherence_
Satellite, Root, etc.) in our model has a length of 12. Binary Tree LST
M is then applied to theserepresentations 𝑈𝑈𝑘𝑘 ∈ 𝑡𝑡. The model uses a Tree LST
M hidden representation of the root discourse unitfor both stance and argument prediction and has two output feedforward layers as with the BER
T model.
4https://huggingface.co/DeepPavlov/rubert-base-cased-conversational
Discourse-aware text classification for argument mining
5 Experiments
5.1 Data
The dataset for joint stance and premise classification is provided by the Ru
Arg-2022 competition organizers. The stance label represents the point of view of the author in relation to the given claim.
The presence of arguments for, against, or mixed in the text is indicated by the premise (argument) label.
There are three claims in the dataset regarding the COVI
D-19: “
Wearing masks is beneficial for society”,“
Vaccination is beneficial for society”, and “
The introduction and observance of quarantine is beneficialfor society”.
parsing. Since the texts in the dataset do not have paragraph breaks, each text is considered to belongto a single tree. Thus, if the text is 𝑙𝑙 elementary discourse units long, its rhetorical structure contains𝑙𝑙 − 1 relations. Each subset of the data contains about 25% simple sentences with no automaticallyrecognizable discourse structure. From this, we hypothesize that for 75% of the data, the classificationperformance can be improved by analyzing the coherence structure within the text. Most examples arefound to have only one discourse relation between two elementary units; in the official test set, this is thecase in 35.9% of examples.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
Number of elementary discourse units0.0
5.0
10.0
15.0
20.0
25.0
30.0
35.0
Percentage of subset, %traindevtesttrain set. Complex texts with a rhetorical structure are the most common way in which polar opinions areexpressed in the corpus. The simplest sentences are most common among the examples of the mixed class
Other, with this difference being particularly evident in the premise (argument) classification subtask(
Fig. 3b). It demonstrates that most examples in this class lack any argumentation typically expressed by causal, conditional, or any other meaningful discourse relations, which is consistent withthe definition of the Other in the premise classification subtask description. One interesting observationis that the examples in which the author expresses a positive stance or argument tend to have the mostcomplex structures in the train set.
1 2 3 4 5 6 7 8 9
Number of elementary discourse units0.0
10.0
20.0
30.0
Percentage of subset, %stance=forstance=otherstance=against(a) Stance detection1 2 3 4 5 6 7 8 9
Number of elementary discourse units0.0
10.0
20.0
30.0
Percentage of subset, %premise=forpremise=otherpremise=against(b) Premise classification
Chistova E., Smirnov I.5.2 Settings
In the data, the number of examples of various classes is unbalanced, with a major predominance ofthe Irrelevant. We choose to add class weights to the loss in both BER
T and RST-LST
M models toprevent unbalanced learning. These weights are adjusted to correspond to the overall class weights inthe train data. We use the Optuna optimization framework automated hyperparameter tuning inboth BER
T fine-tuning and RST-LST
M training. The optimal hidden size of Tree-LST
M for the threetopic-related models is found to be between 50 and 125 units. We use Py
Torch and AllenNL
P libraries for implementation and a single Nvidia Ge
Force RT
X 2080 Ti GP
U. In our experimental setup,RST-LST
M takes on average 2.4 times less time to run one training epoch than BER
T, with 2 to 5 epochstotal.
5.3 Evaluation Procedure
In our evaluation, we use the metric proposed by Ru
Arg-2022: macro F1 excluding the score for label
Irrelevant. We use a 5-fold cross-validation over the labeled train set to accurately compare approachesthat employ and do not employ rhetorical structure. For the official test and development sets, the finalpredictions are obtained by averaging predictions from five models trained on cross-validation. This issimilar to an ensemble, where each model is trained using 80% of the train data.
6 Results and Discussion
In Table 2 we compare the results of the model with Tree LST
M over RS
T structure with the baselineBER
T model.
Non-EDUclassification
Performance on non-ED
U Overall performance
Masks Vaccines Quar. Mean Masks Vaccines Quar. Mean
Stance detectionBER
T 59.8 ± 2.7 62.4 ± 3.4 54.5 ± 3.4 58.9 ± 2.3 60.6 ± 2.6 64.4 ± 2.2 56.4 ± 2.8 60.5 ± 1.9+ RST-LST
M 61.3 ± 2.7 63.4 ± 4.2 55.6 ± 2.7 60.1 ± 2.3 61.7 ± 2.6 65.1 ± 3.0 57.5 ± 2.4 61.4 ± 1.8
Premise classificationBER
T 66.4 ± 2.9 61.7 ± 4.3 56.4 ± 2.8 61.5 ± 2.2 66.0 ± 2.4 62.6 ± 2.7 57.0 ± 2.3 61.9 ± 1.6+ RST-LST
M 68.1 ± 2.1 60.4 ± 3.3 57.6 ± 2.0 62.0 ± 1.3 67.5 ± 1.9 61.5 ± 2.3 58.3 ± 2.1 62.4 ± 0.9against other forPredictionagainstotherfor
Ground truth53.0 28.3 16.9
13.5 75.1 24.1
24.0 53.540(a) BER
Tagainst other forPredictionagainstotherfor
Ground truth54.8 25.9 15.6
12.2 76.2 23.3
23.3 54.440(b) BER
T + RST-LSTM
Stance detection: Introducing discourse structure to this model leads to an improvement across alltopics. The method resulted in an averaged 1.2% mean F1 improvement in the classification of textswith discourse structure and a 0.9% mean F1 improvement for all texts. cross-validation confusion matrices for stance detection in the non-elementary samples of the train set,topics not separated, excluding label Irrelevant. The model with RST-LST
M shows improved ability todistinguish between For and Against polar stance labels and the mixed label Other. For the sequential
Discourse-aware text classification for argument miningclassifier, Other is the most challenging label. First, it can mean that the text has examples of both polarclasses; second, it is the most frequent. We include the examples where discourse structure helps todifferentiate the stance labels in Figures 5 and 6 in Appendix.
Premise classification: On average, the classification performance improved by 0.5% both for textswith identifiable rhetorical structure and for all texts. Classification performance improved significantlyfor the topics Masks (+1.6% F1) and Quarantine (+2.0% F1) and worsened for the Vaccines (-1.3%
F1), which may indicate a drawback of the approach in which a single structure representation for twotargets is trained simultaneously; it is also worth noting that the scores related to the Vaccines topic havethe highest deviation in both the sequential and structural classification methods on complex (non-ED
U)examples. The examples where discourse structure helps to differentiate the premise labels are illustratedin Figures 7 and 8 in Appendix.
Evaluation on the official dev and test sets: In Table 3 we compare the methods on the official dev andtest sets of Ru
Arg-2022. Both sets are treated as unseen, so the official development set was not used forthe parameters adjustment. The results confirm that the RST-LST
M is capable of capturing the overallpolarity of stance and arguments in a document based on the rhetorical structure. Vaccines-related textclassification continues to produce the least stable results. On both dev and test sets, the BER
T modelenhanced with the RST-LST
M module achieves the best performance for premise classification.
Non-EDUclassification
Dev Test
Masks Vaccines Quar. Mean Masks Vaccines Quar. MeanBER
T 66.0 / 66.1 66.8 / 58.5 58.4 / 58.8 63.7 / 61.1 70.0 / 76.4 68.3 / 63.4 61.0 / 71.6 66.5 / 70.6+ RST-LST
M 67.3 / 68.2 67.8 / 56.3 56.3 / 59.8 63.8 / 61.4 70.0 / 76.5 66.0 / 63.8 61.7 / 72.5 65.9 / 71.0
The public RuAR
G-2022 test leaderboard (
Table 4) shows only the results of the last model evaluated.
In our case, it is a model different from the one evaluated in Tables 2 and 3. Specifically, it is anadditional variant of RST-LST
M where nuclei of asymmetric relations are marked as Span (instead ofAttribution_
Nucleus, Coherence_
Nucleus, etc.). As our method aggregates the rhetorical relations withsimilar semantics and nuclearity discrepancies, such as the causal relations Purpose5 and Cause
Effect6,or contrastive Concession7 and Contrast8, this idea was later dismissed. However, according to Table 3,our final method described in this paper also ranks 4th in stance prediction (65.9%) and 3rd in argumentclassification (71.0%) in the competition leaderboard on the official test set.
Stance detection Premise classification1 camalibi 69.7 1 camalibi 74.0
2 sevastyanm 68.2 2 sevastyanm 72.4
3 iamdenay 66.8 3 ursdth (ours) 70.6
4 ursdth (ours) 65.7 4 iamdenay 65.6
5 sopilnyak 56.0 5 dr 60.4
6 kazzand 55.5 6 kazzand 56.0
7 morty 53.5 7 morty 54.5
8 invincible 58.9 8 invincible 54.3
9 dr 47.5 9 Baseline 43.6
10 Baseline 41.8
Ablation study: We inspect the importance of the rhetorical relation labels and nuclearities in ourmethod in 5
Satellite represents the intended result behind the situation described in the nucleus.
6
Nucleus represents the actual result after the situation described in the satellite.
7
Mononuclear relation in which additional information in satellite creates expectations that the situation in the nucleus
would be opposite.
8
Multinuclear relation in which nuclei describe alternative situations.
Chistova E., Smirnov I.only type in the trees with a structural type, marginally affects the classification performance on the discourse trees. However, the simultaneous substitution of all semantic relations for a multinuclear relation
Structural leads to a 0.2% F1 decrease in the stance identification performance and a 0.6% F1 decreasein argument classification, demonstrating the importance of the features related to the labeled rhetoricalstructure in argument mining. We note that although the RS
T parser is far from perfect in recognizingthe labeled trees, it is capable of identifying argumentative structures.
DiscourserelationsStancedetectionArgumentclassification
All 60.1 62.0Coherence 0.1 0.1- 0.1Contrastive 0.1 0.1- 0.1Causal 0.0 0.1- 0.1Condition 0.1 0.1- 0.1Attribution 0.1 0.0
Only structural 0.2 0.67 Conclusion
Sequential text classifiers typically perform well when applied to short texts, but their performance degrades for longer texts due to the complexity of discourse. In order to form an accurate hidden representation of a complex text, we propose a method leveraging both a pretrained language model and anend-to-end RS
T parser. Additionally, we improve the rhetorical parsing for Russian using a recent topdown algorithm for paragraph parsing and report fine-grained RS
T scores for different text granularities.
The improved RS
T parser is used to show the utility of rhetorical parsing in stance detection and premiseclassification on social media comments.
The architecture we propose shows the effectiveness of a two-step rhetorical-driven approach, wherethe base text classification method can be any advanced neural network or feature-based machine learningmodel. Future work should investigate the suitability of discourse parsing in Russian for other tasksrequiring argument extraction and processing.
Acknowledgements
This paper is supported by the Research Program of the National Center for Physics and Mathematics(project no. 9).
