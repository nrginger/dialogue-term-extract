Topic modeling is a statistical method for analyzing a corpus of documents. The result of the modeling is a set of topics. Each topic is usually represented as a discrete distribution over the set of all words in the corpus. Some applications of topic modeling are information search , , , analysis of text documents , , , , images and video data , , , audio data , problems of bioinformatics , .
The most popular algorithms for topic modeling solve the task of stochastic matrix factorization i. e. approximate representation of a stochastic matrix F as a product of two stochastic matrices F ≈ ΦΘ. Matrix F is obtained from the collection of texts by assigning F to the number of occurrences of i-th word in j-th document and column normalization. Matrix F ∈ ℝ|W|×|
D| is usually called word-document matrix, where |
W| is a number of words and |
D| is a number of documents in the corpus. Matrices Φ ∈ ℝ|W|×|
T| and Θ ∈ ℝ|T|×|
D| are called word-topic matrix and topic-document matrix, where |
T| is a number of topics that is usually fixed before run of the algorithm. If we fix some stochastic matrix factorization F ≈ ΦΘ we may interpret distributions in columns of the matrix Φ as topics.
Two most popular approaches to the topic modeling are Probabilistic Latent Semantic Analysis (PLS
A) Latent Dirichlet Allocation (LD
A) . The basic hypothesis of the PLS
A model is the conditional independence hypothesis: the probability of a word occurrence in a document is conditionally independent of the document given a topic. LD
A is a Bayesian version of PLS
A. The main assumption of the LD
A model is that ϕwt and θtd are generated from the Dirichlet distribution. Additive Regularization of Topic Models (ART
M) , , the formulation of PLS
A by adding different regularizers to the loss function. Some of them are described in 2.2.
Usually algorithms use random initialization and then converge to some local optimum. One of the main problems of topic modeling is instability i. e. convergence Stability of Topic Modeling via Modality Regularizationto different solutions from different initializations. Mathematical origins of this issue were studied in , , , authors research the problem of uniqueness of Nonnegative Matrix Factorization (NM
F). Another approach to the problem is customization of basic algorithms to achieve better stability.
In the paper authors proposed ensemble methods and compared their performance with standard LD
A and NM
F approaches. The idea of their K
Fold method is to train several base topic models, transform them into the intermediate representation and build the final topic model on the top of this representation. According to experiments performed on annotated text corpora, K
Fold ensemble strategy can produce more stable and accurate topic models.
The authors of a modification of the standard latent Dirichlet allocation (LD
A) model called granulated LD
A (GLD
A). The method is based on local density regularization that assigns the same topic with high probability to the words that meet together in the context. As for evaluation of model stability, the authors used Jaccard similarity and the number of stable topics based on Kullback
Leibler distance. The study shows that GLD
A seems to reduce instability while yielding the same topic quality as classical topic models.
There are several studies , , show positive influence of additional information about documents in the collection on topic modeling performance. In this paper, we propose a method of increasing the stability that uses multimodal topic modeling. We use words as a first modality and different types of tags as additional modalities. We show that even using a partially labeled corpus (5% or 20% of the whole corpus) may increase the stability of PLS
A model without significant loss of model quality.
2. Background
2.1. PLS
A
Let D be a collection of documents, and let W be its vocabulary. The idea of probabilistic topic modeling is to describe how a collection of documents D is generated by a finite set of topics T. According to PLS
A , the term distribution in each document d ∈ D can be decomposed as a mixture of term probabilities for topics and topic probabilities for documents: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (1)where ϕwt = p(w|t) is the distribution of words in topics and θtd = p(t|d) is the distribution of topics in documents. The parameters ϕwt and θtd form stochastic matrices Φ and Θ. The problem of finding these matrices can be considered as an approximate matrix factorization task F ≈ ΦΘ, F = (p̂wd)|W|×|
D|, where p̂wd = nwd/nd is a frequency estimate of the conditional probability p(w|d), nd is the length of the document d, nwd is the number of occurrences of the word w in the document d.
Parameters of the PLS
A model are estimated via maximizing log-likelihood function with linear constraints:
Derbanosov R., Bakhanova M. =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (2) 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (3)
The process of solving this problem consists of random initialization of the matrix Φ and application of E
M algorithm.
Most Bayesian approaches, such as LD
A, use a prior Dirichlet distribution as the main regularizer, thus complicate the combination with other regularizers. ART
M is a modern extension of PLS
A model proposed in is free from excess probabilistic assumptions. It does not require parameters to be generated from Dirichlet distribution and allows to use different regularizers that may have no probabilistic interpretation at all. Suppose Ri(Φ, Θ), i = 1, 2, …, n are n regularizers that we want to maximize along with the likelihood L(Φ, Θ). In ART
M, we solve multi-criteria problem via maximization of the linear combination of L and Ri with some nonnegative regularization coefficients τi: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (4)
Matrices Φ and Θ are estimated using E
M algorithm, which can be described by two iteratively repeated steps.
At the E-step, we estimate the condition probability p(t|d, w) for all words in documents (d, w) using Bayes formula: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (5)
These probabilities are used to calculate parameters nwt—the number of occurrences of the word w in the collection D with relation to the topic t and ntd—the number of words in the document d with relation to the topic t.
=∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (6)
At the M-step, we calculate parameters ϕwt and θtd as frequency estimates of the corresponding conditional probabilities: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (7)where the sign ∝ means that the distribution on the left is obtained after the normalization of the right expression, and (x)+ = max{x, 0}. Thus, we can add different regularizers to set necessary constrains to the topic model. In this work, we will use the following regularizers: smoothing, sparsing, decorrelation and modality.
Stability of Topic Modeling via Modality Regularization2.2. Additive regularization of topic models
Smoothing regularizer. If we want ϕwt and θtd to be close to some discrete distributions βw and αd in terms of Kullback–
Leibler divergence we can use a smoothing regularizer: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (8)where β0 and α0 are regularization coefficients. Hence, the M-th step of the algorithm gives equations: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (9)
It is recommended to use a prior Dirichlet distributions or Bayesian inference for distributions βw and αt. The effect of this regularizer is an increase in small values of ϕwt and θtd due to a slight decrease in their large values. As a result, generated topics may include general vocabulary words, stop words and rare words that are usually excluded from topics.
Sparsing regularizer. Usually we assume that each word and each document relate to a small number of topics. It means that matrices Φ and Θ should be sparse. We can achieve it using a sparsing regularizer. One can notice that sparsing is an inverse procedure to smoothing. Therefore, sparsing and smoothing differ only in the sign of parameters βw and αt.
Decorrelation regularizer. Decorrelation regularizer formalises the requirement that topics have to differ from each other. It can be satisfied via minimizing the sum of covariances between distributions ϕwt and ϕws for all pairs of topics t, s: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (10)where τ is a regularization coefficient. In this case, the formula for the regularized M-step takes the form: 𝑅𝑅(Φ,Θ) =∑︁𝑖𝑖𝜏𝜏𝑖𝑖𝑅𝑅𝑖𝑖(Φ,Θ),𝐿𝐿(Φ,Θ) +𝑅𝑅(Φ,Θ) → maxΦ,Θ.
(4)𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑) = 𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡∑︀𝑠𝑠∈𝑇𝑇𝜑𝜑𝑤𝑤𝑠𝑠𝜃𝜃𝑠𝑠𝑡𝑡. (5)𝑛𝑛𝑤𝑤𝑤𝑤 =∑︁𝑡𝑡∈𝐷𝐷𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑), 𝑛𝑛𝑤𝑤𝑡𝑡 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡𝑝𝑝(𝑡𝑡|𝑑𝑑, 𝑑𝑑). (6)𝜑𝜑𝑤𝑤𝑤𝑤 ∝(︂𝑛𝑛𝑤𝑤𝑤𝑤 + 𝜑𝜑𝑤𝑤𝑤𝑤𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜑𝜑𝑤𝑤𝑤𝑤)︂+, 𝜃𝜃𝑤𝑤𝑡𝑡 ∝(︂𝑛𝑛𝑤𝑤𝑡𝑡 + 𝜃𝜃𝑤𝑤𝑡𝑡𝜕𝜕𝑅𝑅(Φ,Θ)𝜕𝜕𝜃𝜃𝑤𝑤𝑡𝑡)︂+, (7)𝑅𝑅(Φ,Θ) = 𝛽𝛽0∑︁𝑤𝑤∈𝑇𝑇∑︁𝑤𝑤∈𝑊𝑊𝛽𝛽𝑤𝑤 ln𝜑𝜑𝑤𝑤𝑤𝑤 + 𝛼𝛼0∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑇𝑇𝛼𝛼𝑤𝑤 ln 𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ, (8)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 + 𝛽𝛽0𝛽𝛽𝑤𝑤)+ , 𝜃𝜃𝑤𝑤𝑡𝑡 ∝ (𝑛𝑛𝑤𝑤𝑡𝑡 + 𝛼𝛼0𝛼𝛼𝑤𝑤)+ . (9)𝑝𝑝(𝑑𝑑|𝑑𝑑) =∑︁𝑤𝑤∈𝑇𝑇𝑝𝑝(𝑑𝑑|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷,𝑑𝑑 ∈ 𝑊𝑊, (1)𝐿𝐿(Φ,Θ) =∑︁𝑡𝑡∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛𝑤𝑤𝑡𝑡 log∑︁𝑤𝑤∈𝑇𝑇𝜑𝜑𝑤𝑤𝑤𝑤𝜃𝜃𝑤𝑤𝑡𝑡 → maxΦ,Θ(2)∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤 = 1, 𝜑𝜑𝑤𝑤𝑤𝑤 ≥ 0;∑︁𝑤𝑤∈𝑇𝑇𝜃𝜃𝑤𝑤𝑡𝑡 = 1, 𝜃𝜃𝑤𝑤𝑡𝑡 ≥ 0. (3)𝑅𝑅(Φ,Θ) = −𝜏𝜏∑︁𝑤𝑤∈𝑇𝑇∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤∑︁𝑤𝑤∈𝑊𝑊𝜑𝜑𝑤𝑤𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠 → maxΦ,Θ, (10)𝜑𝜑𝑤𝑤𝑤𝑤 ∝ (𝑛𝑛𝑤𝑤𝑤𝑤 − 𝜏𝜏𝜑𝜑𝑤𝑤𝑤𝑤∑︁𝑠𝑠∈𝑇𝑇∖𝑤𝑤𝜑𝜑𝑤𝑤𝑠𝑠)+. (11) (11)2.3. Multimodal topic modeling
Usually documents can be described not only by words but also by terms of other modalities . For example, textual modalities are tags, n-grams, named entities and natural language words. The last one is what we used to deal with in topic modeling. Pictures and web-sites are non-textual modalities. We can consider documents as a set of tokens taken from different modalities. The diverse meta-data represented by modalities can be helpful for determining topics, and, vice-versa, topics may be used to predict missing meta-data.
Multimodal topic modeling occurred to be an effective approach for solving different problems. For example, for a given parallel collection of text translation we can model topics and then use them for the cross-language search. In this case, each language is considered as a modality. Experiments showed that the combination of parallel documents and bilingual dictionaries improves the quality of cross-language Derbanosov R., Bakhanova M. in comparison with models using only bilingual dictionaries . Also, multimodal topic model can be applied for constructing recommendations . In this study, the authors focused on the article recommendation in the online-platform, they used different modalities, such as words from text, user’s feedback, tags, authors and user-specified categories. According to the results, the combination of modalities reasonably improves recommendation ranking.
Multimodal topic model and the regularized E
M-algorithm for this case were firstly introduced in .
Let M be a set of modalities, and let Wm, m ∈ M be a vocabulary of modality m. These vocabularies do not intersect and can be united into the set ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) containing terms of all modalities. A model of p(w|d) is introduced each modality Wm, m ∈ M: ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (12)
The main concept of such modeling is that topics p(t|d) are the same for all modalities. As for the distribution of words in topics, the matrices Φm = (ϕwt)|Wm|×|
T| are normalized separately and stacked vertically into the matrix Φ = (ϕwt)|W|×|
T|.
If we consider the log-likelihood of each modality as a regularizer with coefficient τm, then the optimization problem has the following form: ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (13) ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (14)3. Metrics
3.1. Quality of topic modeling
There are several metrics for measuring quality of topic model. Most previous works have exlusively focused on perplexity measure that describes the speed and level of convergence of the model. Perplexity can be represented as an inverse function of the likelihood of model parameters. One of the drawbacks of this metric is that it depends on the data size, therefore, it is hard to compare results of this measure obtained from models trained on different datasets.
Some recent studies , their models by pairwise information based metric called coherence . The practical meaning of coherence follows a simple idea: if we describe the topic as a set of words then these words are likely to meet together in the context. In addition, coherence seems to reflect well the interpretability of topics . Let k be an adjustable parameter meaning the number of top words in the topic t ∈ T, and let Wt = {w1, …, wk} be the corresponding set of top words. Then coherence formula for topic t is defined as follows:
Stability of Topic Modeling via Modality Regularization ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (15)where probabilities can be estimated by frequencies: ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (16) ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (17)
There are different types of calculating co-occurrencies n(u, v). In this paper, we calculate in how many documents the pair (u, v) occurred at least once: ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (18)
We have described above how to compute coherency only for one topic. To obtain the coherence score for the topic model we simply average coherencies for all topics in the model. The higher coherency is, the better.
3.2. Stability of topic modeling
Let’s denote by {
M1, M2, …, Mr} the set of topic models generated as a result of r runs of the algorithm on the same data. Assume that these models are similar if their topics are similar. To measure similarity between two topics represented by t top words, we propose to calculate the measure that we call Stable Words (S
W), and describe it by the following formula: ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (19)where Ri is a list of top t tokens of topic i. S
W takes values in , and the value 1 corresponds to the identical top words. S
W can be interpreted as a modified Jaccard
Index, these two metrics differ only by the denominator: in Jaccard Index we divide by the set union size |
Ri ∪ Rj|. We consider S
W as more interpretable measure in terms of topic stability because it is simply the share of stable words.
We should find topic correspondence between two sets of topics in order to compute similarity of these two sets. The best topic matching between two models with |
T| topics can be found using S
W defined in Eq. 20. We construct a matrix S, where the element sij represents similarity between the i-th topic of the first model and the j-th topic of the second model. Then we find the optimal matching P by solving the minimal weight bipartite matching problem applying the Hungarian algorithm .
To obtain the score of similarity between the set of r models we compute Average Stable Words (AS
W): ∑︁𝑚𝑚∈𝑀𝑀∑︁𝑑𝑑∈𝐷𝐷∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜏𝜏𝑚𝑚𝑛𝑛𝑤𝑤𝑑𝑑 ln∑︁𝑡𝑡∈𝑇𝑇𝜑𝜑𝑤𝑤𝑡𝑡𝜃𝜃𝑡𝑡𝑑𝑑 +𝑅𝑅(Φ,Θ) → maxΦ,Θ, (13)∑︁𝑤𝑤∈𝑊𝑊𝑚𝑚𝜑𝜑𝑤𝑤𝑡𝑡 = 1, 𝜑𝜑𝑤𝑤𝑡𝑡 ≥ 0;∑︁𝑡𝑡∈𝑇𝑇𝜃𝜃𝑡𝑡𝑑𝑑 = 1, 𝜃𝜃𝑡𝑡𝑑𝑑 ≥ 0. (14)𝐶𝐶𝑡𝑡 =2
𝑘𝑘(𝑘𝑘 − 1)𝑘𝑘−1∑︁𝑖𝑖=1𝑘𝑘∑︁𝑗𝑗=𝑖𝑖+1(︂log𝑝𝑝(𝑤𝑤𝑖𝑖, 𝑤𝑤𝑗𝑗)𝑝𝑝(𝑤𝑤𝑖𝑖)𝑝𝑝(𝑤𝑤𝑗𝑗))︂+, (15)𝑝𝑝(𝑢𝑢, 𝑢𝑢) =𝑛𝑛(𝑢𝑢, 𝑢𝑢)𝑛𝑛, 𝑝𝑝(𝑢𝑢) =𝑛𝑛(𝑢𝑢)𝑛𝑛, (16)𝑛𝑛(𝑢𝑢) =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑢𝑢, 𝑤𝑤), 𝑛𝑛 =∑︁𝑤𝑤∈𝑊𝑊𝑛𝑛(𝑤𝑤). (17)𝑛𝑛(𝑢𝑢, 𝑢𝑢) =|𝐷𝐷|∑︁𝑑𝑑=1. (18)𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖, 𝑅𝑅𝑗𝑗) =|𝑅𝑅𝑖𝑖 ∩𝑅𝑅𝑗𝑗|𝑡𝑡, (19)𝐴𝐴𝑆𝑆𝑆𝑆 =2
𝑟𝑟(𝑟𝑟 − 1)∑︁𝑖𝑖≤𝑗𝑗,𝑖𝑖̸=𝑗𝑗|𝑇𝑇 ||𝑇𝑇 |∑︁𝑠𝑠=1𝑆𝑆𝑆𝑆 (𝑅𝑅𝑖𝑖𝑠𝑠, 𝑅𝑅𝑗𝑗𝑗𝑗(𝑠𝑠)), (20)𝑆𝑆 = ⊔𝑚𝑚∈𝑀𝑀𝑆𝑆𝑚𝑚𝑝𝑝(𝑤𝑤|𝑑𝑑) =∑︁𝑡𝑡∈𝑇𝑇𝑝𝑝(𝑤𝑤|𝑡𝑡)𝑝𝑝(𝑡𝑡|𝑑𝑑), 𝑑𝑑 ∈ 𝐷𝐷, 𝑤𝑤 ∈ 𝑆𝑆𝑚𝑚. (12) (20)where π(s) is a topic of the model j matched to the topic s of the model i.
Derbanosov R., Bakhanova M. Experiments
We performed experiments on five texts collections: 20News
Groups, Reuters52, Cade, WebK
B and Habr. 20News
Groups a set of documents classified in 20 newsgroups. Reuters52 a collection of articles of 1987 year from Reuters that was manually classified by Reuters Ltd. The documents in WebK
B1 are webpages collected by the World Wide Knowledge Base project of the CM
U text learning group. Cade is a subset of web pages extracted from the CA
DÊ Web Directory which points to Brazilian webpages labeled by human experts . Habr is a dataset of articles from I
T blogging platform http://habrahabr.ru 5 modalities: text of the blogpost, author, users that leave comments at the blogpost, hub that is a site section, tags that are generated by the author. We used preprocessing from 20News
Groups, Reuters52, Cade and WebK
B datasets. All datasets were splited by train and test sets in a ratio of 60 to 40. Coherence was measured on hold-out test dataset.
We take text labels as an additional modality for 20News
Groups, Reuters52, Cade and WebK
B datasets. Each document contains one token of such a modality. We use different percentage of labeled documents: 5%, 20%, 50% and 100% in order to simulate partially labeled collection. We tested tags, habs modalities and the combination of four modalities: authors, tags, hubs and users on Habr dataset.
All experiments were performed using an open source library for topic modeling BigART
M . Models were trained until convergence on the train part of a dataset.
Each topic can be described as a set of the most frequent words of this topic. Several descriptions in terms of top words for 20News
Groups are presented in % of labels Top 10 words0 topic 1: game, team, plai, player, win, season, hockei, last, score, leagu
topic 2: nasa, research, univers, gov, orbit, launch, program, center, systemtopic 3: car, price, sale, bui, want, mail, sell, speed, apr, engintopic 4: gun, state, israel, law, isra, govern, weapon, american, right, arabtopic 5: kei, encrypt, chip, govern, secur, clipper, system, presid, public, work50 topic 1: game, team, plai, player, win, season, hockei, last, leagu, score
topic 2: space, nasa, scsi, system, control, orbit, work, card, launch, datatopic 3: car, wire, ground, engin, power, work, water, back, want, lighttopic 4: gun, state, israel, isra, bike, weapon, kill, apr, law, arabtopic 5: kei, govern, encrypt, system, secur, chip, presid, clipper, public, program1 http://www.cs.cmu.edu/~webkb/
http://www.cs.cmu.edu/~webkb/
Stability of Topic Modeling via Modality Regularization% of labels Top 10 words100 topic 1: window, game, team, plai, win, hockei, file, player, season, nhl
topic 2: space, nasa, work, presid, govern, orbit, state, launch, system, programtopic 3: car, engin, want, come, back, work, speed, price, start, autotopic 4: gun, weapon, law, state, firearm, fire, govern, crime, control, armtopic 5: kei, armenian, encrypt, chip, govern, israel, secur, isra, system, turkish
To measure stability of the set of models generated over r = 100 runs, we used AS
W (
Eq. 21) with top t = 10 tokens for each topic. The estimation of the quality of the models was conducted using coherence score (
Eq. 16) based on top t = 10 terms for each topic. We tried several set of hyperparameters for each model and present results with the best coherence. We performed several experiments with usual decorrelation, sparcing and smoothing regularizations.
with number of topics |T|=10
Modality Regularizer AS
W Coherence
Words, 100% of labels Labels modality 0.86±0.01 0.16±0.01
Words, 50% of labels Labels modality 0.86±0.01 0.18±0.01
Words, 20% of labels Labels modality 0.83±0.01 0.21±0.01
Words, 5% of labels Labels modality 0.79±0.01 0.24±0.01
Words — 0.78±0.01 0.26±0.02
Words Decorrelation Φ 0.77±0.01 0.26±0.02
Words Sparcing Θ 0.75±0.01 0.28±0.02
Words Smoothing Φ 0.53±0.05 0.90±0.14
Words Sparcing Φ 0.53±0.01 0.40±0.02with number of topics |T|=60
Modality Regularizer AS
W Coherence
Words, 100% of labels Labels modality 0.76±0.00 0.35±0.01
Words, 50% of labels Labels modality 0.70±0.00 0.46±0.01
Words, 20% of labels Labels modality 0.63±0.01 0.55±0.01
Words, 5% of labels Labels modality 0.60±0.01 0.62±0.01
Words — 0.61±0.01 0.62±0.02
Words Decorrelation Φ 0.60±0.01 0.62±0.02
Words Sparcing Θ 0.12±0.00 0.10±0.02
Words Smoothing Φ 0.43±0.08 0.68±0.58
Words Sparcing Φ 0.25±0.00 0.74±0.01
Derbanosov R., Bakhanova M. number of topics |T|=60
Modality Regularizer AS
W Coherence
Words, 100% of labels Labels modality 0.65±0.01 0.72±0.01
Words, 50% of labels Labels modality 0.60±0.01 0.81±0.01
Words, 20% of labels Labels modality 0.56±0.00 0.87±0.01
Words, 5% of labels Labels modality 0.54±0.01 0.87±0.01
Words — 0.53±0.01 0.90±0.01
Words Decorrelation Φ 0.52±0.01 0.91±0.01
Words Sparcing Θ 0.08±0.00 0.07±0.01
Words Smoothing Φ 0.68±0.05 0.53±0.19
Words Sparcing Φ 0.20±0.00 0.83±0.02
Modality Regularizer AS
W Coherence
Words, 100% of labels Labels modality 0.75±0.02 1.30±0.03
Words, 50% of labels Labels modality 0.72±0.02 1.27±0.03
Words, 20% of labels Labels modality 0.71±0.02 1.31±0.02
Words, 5% of labels Labels modality 0.69±0.02 1.32±0.03
Words — 0.69±0.02 1.33±0.02
Words Decorrelation Φ 0.69±0.02 1.33±0.02
Words Sparcing Θ 0.71±0.02 1.37±0.02
Words Smoothing Φ 0.45±0.02 1.57±0.11
Words Sparcing Φ 0.50±0.01 1.43±0.04
Modality Regularizer AS
W Coherence
Words, 100% of labels Labels modality 0.70±0.02 0.37±0.02
Words, 50% of labels Labels modality 0.65±0.02 0.43±0.02
Words, 20% of labels Labels modality 0.66±0.02 0.44±0.01
Words, 5% of labels Labels modality 0.64±0.02 0.47±0.01
Words — 0.64±0.02 0.49±0.02
Words Decorrelation Φ 0.64±0.02 0.49±0.02
Words Sparcing Θ 0.54±0.02 0.46±0.03
Words Smoothing Φ 0.68±0.04 0.31±0.04
Words Sparcing Φ 0.40±0.01 0.53±0.02
Stability of Topic Modeling via Modality Regularization
Modality Regularizer AS
W Coherence
Words, authors, users, tags, hubs Combination of modalities0.73±0.00 0.40±0.01
Words, tags Tags modality 0.63±0.00 0.56±0.01
Words, hubs Hubs modality 0.54±0.01 0.73±0.02
Words Smoothing Φ 0.51±0.06 0.27±0.09
Words Decorrelation Φ 0.51±0.01 0.77±0.02
Words — 0.51±0.01 0.77±0.02
The results of topic modeling on 20News
Groups, Reuters52, Cade and WebK
B datasets (
Tables 2–6) indicate that increase in the percentage of labels leads to stability growth. Moreover, models with regularizers, such as sparcing and smoothing, yield very low values of AS
W compared to models with labels modality. Even 5% or 20% of labels may be enough to significantly increase model stability. However, we observe a drop in coherence score, especially in the models with high percentage of labels. Note, that models with labels modality trained on Reuters52 produce comparable and even higher coherence than models with other regularizers.
Experiments on Habr dataset show that the model combination of all five modalities outperforms all other models in terms of stability measure (
Table 7). We see that the use of one additional modality—hubs or tags—increases AS
W score but results in a slight dercease of quality in comparison with the use of other regularizers.
Overall, we conclude that models with different modalities, such as labels and additional meta-data, produce more stable topics. At the same time, the model with labels modality may yield low coherence score if the percentage of labels is high.
5. Conclusion
Modern topic modeling approaches suffer from instability of their results even with fixed dataset and hyperparameters. We have demonstrated that stability of topic modeling algorithm may be improved with the help of side information. Evaluation on several text corpora shows that regularization of the PLS
A model with additional modalities leads to less impact of random initialization and thus more stable modeling even if side information was provided only for some subset of documents.
While our experiments were conducted on five significantly different datasets, it is still an open question what combination of additional information is the best choice for improving stability with the smallest degradation of metrics of a model. The topic for further research is to find a combination of various regularizers with the best balance between modeling stability and the quality of topics.
Derbanosov R., Bakhanova M.