Words have a history; that is, they change and evolve over time. Types of changes include: orthographic,or changes in spelling or capitalization; register changes, in which a word becomes more respectable orless; and the semantic changes discussed in this paper, in which the word changes its relationships withother words in the vocabulary, perhaps by losing or gaining a sense or just changing the frequency of usefor a sense.
The task of finding word sense changes over time belongs to the field of natural language processing.
It is called diachronic Lexical Semantic Change (LS
C) and in recent years, it is getting more attention. There is also the synchronic LS
C task, which aims to identify domain-specific changesof word senses compared to general-language usage .
1.1 The RuShift
Eval Task
The goal of the RuShift
Eval task1 to rank a set of target words according to their degree oflexical semantic similarity between two corpora ùê∂ùê∂ùëñùëñ and ùê∂ùê∂ùëóùëó , each from a different time period ùë°ùë°ùëñùëñ and ùë°ùë°ùëóùëó ,respectively. A lower rank (score) for the test word means stronger semantic change. The organizers*
Equal contribution.
1
It is almost the same task as the Sem
Eval-2020 Task 1, sub-task 2, see Section 2.provided three corpora for different time periods ùê∂ùê∂1 pre
Soviet (1700-1916), ùê∂ùê∂2 Soviet (1918-1990) and
ùê∂ùê∂3 post
Soviet (1991-2016), see Section 3 for details. They also provided annotated training data (ranksfor list of words) for two corpora pairs ùê∂ùê∂1/ùê∂ùê∂2 (pre-Soviet/
Soviet) and ùê∂ùê∂2/ùê∂ùê∂3 (Soviet/post
Soviet) fromthe RuSem
Shift test set . In the test phase, the participants were asked to produce outputs for a list oftest words2 for all three pairs of the corpora, i.e., ùê∂ùê∂1/ùê∂ùê∂2 (pre-Soviet/
Soviet), ùê∂ùê∂2/ùê∂ùê∂3 (Soviet/post
Soviet)and ùê∂ùê∂1/ùê∂ùê∂3 (pre-Soviet/post
Soviet). In evaluation, a Spearman correlation coefficient with gold rankingsbased on human annotation is computed for each pair of corpora and subsequently all three coefficientsare averaged. The average is used as a final score for one submission.
Even though the organizers provided some annotated training data, we did not use the data and we relyon a completely unsupervised solution. We use a very similar approach to . The general idea ofour solution is that we consider each pair of different corpora ùê∂ùê∂ùëñùëñ and ùê∂ùê∂ùëóùëó as separate languages ùêøùêø1 andùêøùêø2 despite the fact that both of them are written in Russian. We assume that both of these languages aredeeply similar in all aspects, including semantic. We train a semantic space (word embedding) for eachcorpus and for each pair of corpora, we perform a cross-lingual mapping (i.e., we transform the spacesinto one common space) of their corresponding word embeddings. For the cross-lingual transformation,we use the Orthogonal transformation Canonical Correlation Analysis (CC
A) . Afterthe transformation for one corpora pair, we can measure cosine similarity for one target word between itstwo vectors because they are in the same space. The cosine similarity denotes their semantic similarity,all the target words are then ranked according to the cosine similarity.
2 Related Work
Word embeddings are vector spaces in which the location in the space reveals some relationships withnearby words. For example, the nearest-neighbor set of a word generally includes words which havesome semantic or syntactic relationship with it. The embedding is thus a concrete demonstration of
Harris‚Äô remark difference of meaning correlates with difference of distribution .
Alignment transformations or cross-lingual transformations are techniques for aligning two word embedding spaces, first described in Mikolov et al. translating single words from one language toanother. For example, to find English equivalents of Russian words, semantic word embedding vectorspaces ùëãùëãùê∏ùê∏ and ùëãùëãùëÖùëÖ with vector length ùëòùëò for each language are built from monolingual corpora ùê∂ùê∂ùê∏ùê∏ , ùê∂ùê∂ùëÖùëÖ. Asmall dictionary of cross-language synonyms, with ùëõùëõ pairs like e.g. (
Please, –ü—Ä–æ—Å–∏–º) is provided. Thesesynonyms are used to create two ùëõùëõ by ùëòùëò matrices ùê∑ùê∑ùê∏ùê∏ and ùê∑ùê∑ùëÖùëÖ, with rows in the ùê∑ùê∑ùê∏ùê∏ matrix consistingof semantic vectors from the English embedding, and corresponding rows in the ùê∑ùê∑ùëÖùëÖ matrix consistingof semantic vectors from the Russian embedding. Finally a linear transformation ùëäùëäùê∏ùê∏‚ÜíùëÖùëÖ is trained tochange the vectors for the English vectors to the Russian ones, by finding the following minimum:ùëäùëäùê∏ùê∏‚ÜíùëÖùëÖ = argminùëäùëä||ùê∑ùê∑ùëÖùëÖ ‚àíùëäùëäùê∑ùê∑ùëÖùëÖ||2
Then we hope to find that vector ùë£ùë£ùê∏ùê∏ùëûùëû for an English word ùëûùëû not in our dictionary will be mapped withùëäùëäùê∏ùê∏‚ÜíùëÖùëÖùë£ùë£ùê∏ùê∏ùëûùëû to be in the near neighborhood of vectors ùë£ùë£ùëÖùëÖùë§ùë§ùëóùëó, good Russian translations of ùëûùëû.
Kulkarni et al. have been the first to use the displacement of a word mapped by a linear transformation from one word-embedding semantic space to another as a measurement of semanticchange. Their transformations, between word embeddings for corpora ùê∂ùê∂ùë†ùë† and ùê∂ùê∂ùë°ùë° with large vocabularyintersections, were created based on nearest neighbors. For each test word ùëûùëû, if ùë£ùë£ùë†ùë†ùë§ùë§ùëñùëñis the vector forthe ùëñùëñ-th nearest neighbor to ùë£ùë£ùë†ùë†ùëûùëû in one space, and ùë£ùë£ùë°ùë°ùë§ùë§ùëñùëñis the vector for the same word in the other space,and each such vector is ùëòùëò elements long, then a ùëòùëò √ó ùëòùëò matrix ùëäùëä ùë†ùë†‚Üíùë°ùë° such that ùë£ùë£ùë°ùë°ùë§ùë§ùëñùëñ= ùëäùëä ùë†ùë†‚Üíùë°ùë°ùë£ùë£ùë†ùë†ùë§ùë§ùëñùëñis completely determined whenever we consider at least ùëòùëò nearest neighbors, that is |ùë£ùë£ùë†ùë†ùë§ùë§ùëñùëñ| > ùëòùëò. Thus, they builda piece-wise linear transformation from one space to another around each test word, and compare themapped position of the test word to the actual position for the same word in the second space.
Hamilton et al. the term orthogonal Procrustes3 to describe their method of building an2
The list was same for all three pairs.
3
Procrustes was a figure in Greek mythology who forced passing travellers to fit into his bed, either stretching them longer
or cutting them shorter if necessary.
P≈ôib√°≈à P., Pra≈æ√°k O., Taylor S. E.orthogonal linear mapping to align embeddings for two time periods. Like Mikolov unlike
Kulkarni, they use one transformation for all words in the vocabulary. Unlike Mikolov, they add theadditional constraint that the transformation be orthogonal, so that angles and thus cosine similaritiesbetween words are preserved by the transformation. With this added condition, the problem has a closedform using Singular Value Decomposition. Their experiments showed that the combination of wordembeddings and orthogonal Procrustes give excellent results when there is sufficient data to build goodword embeddings.
Tahmasebi et al. a comprehensive survey of techniques for the LS
C task. Schlechtweg etal. available approaches for LS
C detection using the DU
Rel dataset .
According to , there are three main types of approaches. (1) Semantic vector spaces approaches represent each word with two vectors for two different time periods. Thechange of meaning is then measured by some distance (usually by the cosine distance) between the twovectors. (2) Topic modeling approaches a probability distribution of wordsover their different senses, i.e., topics and (3) Clustering models .
The recent competitions focused on LS
C: Sem
Eval-2020 Task 1 DIACR
Ita onunsupervised approaches and did not provide any annotated training data. The goal of the SemEval-2020
Task 1 was to measure the binary (changed or not) and continuous change in words between two timeperiods in English, German, Latin and Swedish corpora. In the DIACR
Ita the participants were askedonly to measure the binary changes for a set of target words between two time periods in Italian.
3 Data
The data for the RuShift
Eval task is drawn from the Russian National Corpus4. Like the data for theother tasks shown below, each corpus consists of random sentences chosen from the literature of itstime-period. Thus, neighboring sentences provide no additional context for words in short sentences.
Task:
Language Period 1 Period 2 Period 3 # test wordsSem
Eval T1:
English 6.5
M 6.7
M ‚Äì 37Sem
Eval T1:
German 70.2
M 72.3
M ‚Äì 48Sem
Eval T1:
Latin 1.7
M 9.4
M ‚Äì 40Sem
Eval T1:
Swedish 71
M 110.7
M ‚Äì 31DIACR-Ita:
Italian 156.8
M 589.6
M ‚Äì 18RuShiftEval:
Russian 75.1
M 97.2
M 85.3
M 99
The RuShift
Eval corpora are not lemmatized; the tokens in the corpora are inflected wordforms usedcorrectly in the sentence context. The test words, however, are lemmas. Some of them may appear in thecorpora when an inflected word is the same as the lemma, but in general, many of the uses of a lemmaare different than the lemma itself.
For the Sem
Eval-2020 Task 1, the corpora were provided only in lemmatized form, in part becauseof copyright issues. Reading them, it is clear that some information has been lost, including numberand case for nouns and person and tense for verbs. These might not be important to the task, but thereis no choice to make; all the evaluations must use the lemmatized forms (which are not always theactual lemmas; the text processing has a few errors). For DIACR
Ita, the corpus was provided in verticalform, with the original token, the lemma, and the part of speech available for each token. Prazak etal. four of the seven possible choices: tokens; token_PO
S (Part-of
Speech); lemmas;and lemma_PO
S. The change measurements were most consistent for lemmas, and least consistent fortoken_PO
S, with the other two choices in the middle. For most of our RuShift
Eval submissions, wechose to use lemmas instead of raw tokens.
We use U
Dpipe the russian-syntagrus-ud-2.5-191206.udpipe model to lemmatize the corpora, that is, to convert every token to its lemma.
4https://rusvectores.org/static/corpora/UWB@RuShift
Eval Measuring Semantic Difference as per-word Variation in Aligned Semantic Spaces
4 System Description
Our method5 is fully unsupervised and language-independent. It consists of preparing a semantic vectorspace for each corpus, earlier and later; computing a linear transformation between earlier and laterspaces, using Canonical Correlation Analysis and Orthogonal Transformation; and measuring the cosinesbetween the transformed vector for the target word from the earlier corpus and the vector for the targetword in the later corpus.
First, we train semantic spaces from each corpus ùê∂ùê∂1, ùê∂ùê∂2 and ùê∂ùê∂3. We represent the semantic spacesby a matrix Xùë†ùë† (i.e., a source space ùë†ùë†) and a matrix Xùë°ùë° (i.e, a target space ùë°ùë°) using fast
Text with negative sampling. We perform a cross-lingual mapping of the two vectorspaces, getting two matrices XÃÇùë†ùë† and XÃÇùë°ùë° projected into a shared space. We select two methods forthe cross-lingual mapping Canonical Correlation Analysis (CC
A) using our own implementation anda modification of the Orthogonal Transformation from Vec
Map . Both of these methods are lineartransformations. In our case, the transformation can be written as follows:
XÃÇùë†ùë† = Wùë†ùë†‚Üíùë°ùë°
Xùë†ùë† (1)where Wùë†ùë†‚Üíùë°ùë° is a matrix that performs linear transformation from the source space ùë†ùë† (matrix Xùë†ùë†) into atarget space ùë°ùë° and XÃÇùë†ùë† is the source space transformed into the target space ùë°ùë° (the matrix Xùë°ùë° does not haveto be transformed because Xùë°ùë° is already in the target space ùë°ùë° and Xùë°ùë° = XÃÇùë°ùë°).
Generally, the CC
A transformation transforms both spaces Xùë†ùë† and Xùë°ùë° into a third shared space ùëúùëú(where Xùë†ùë† Ã∏= XÃÇùë†ùë† and Xùë°ùë° Ã∏= XÃÇùë°ùë°). Thus, CC
A computes two transformation matrices Wùë†ùë†‚Üíùëúùëú for thesource space and Wùë°ùë°‚Üíùëúùëú for the target space. The transformation matrices are computed by minimizingthe negative correlation between the vectors xùë†ùë†ùëñùëñ ‚àà Xùë†ùë† and xùë°ùë°ùëñùëñ ‚àà Xùë°ùë° that are projected into the sharedspace ùëúùëú. The negative correlation ùúåùúå is defined as follows:argminWùë†ùë†‚Üíùëúùëú,Wùë°ùë°‚Üíùëúùëú‚àíùëõùëõ‚àëÔ∏Åùëñùëñ=1ùúåùúå(
Wùë†ùë†‚Üíùëúùëúxùë†ùë†ùëñùëñ ,
Wùë°ùë°‚Üíùëúùëúxùë°ùë°ùëñùëñ) = ‚àíùëõùëõ‚àëÔ∏Åùëñùëñ=1ùëêùëêùëúùëúùëêùëê(
Wùë†ùë†‚Üíùëúùëúxùë†ùë†ùëñùëñ ,Wùë°ùë°‚Üíùëúùëúxùë°ùë°ùëñùëñ)‚àöÔ∏Äùëêùëêùë£ùë£ùë£ùë£(
Wùë†ùë†‚Üíùëúùëúxùë†ùë†ùëñùëñ )√ó ùëêùëêùë£ùë£ùë£ùë£(
Wùë°ùë°‚Üíùëúùëúxùë°ùë°ùëñùëñ)(2)where ùëêùëêùëúùëúùëêùëê the covariance, ùëêùëêùë£ùë£ùë£ùë£ is the variance and ùëõùëõ is a number of vectors. In our implementation ofCC
A, the matrix XÃÇùë°ùë° is equal to the matrix Xùë°ùë° because it transforms only the source space ùë†ùë† (matrix Xùë†ùë†)into the target space ùë°ùë° from the common shared space with a pseudo-inversion, and the target space doesnot change. The matrix Wùë†ùë†‚Üíùë°ùë° for this transformation is then given by:
Wùë†ùë†‚Üíùë°ùë° = Wùë†ùë†‚Üíùëúùëú(
Wùë°ùë°‚Üíùëúùëú)‚àí1 (3)
In the case of the Orthogonal Transformation, the submission is referred to as ort-200-O
T, see to both semantic spaces. The transformation matrix Wùë†ùë†‚Üíùë°ùë° is given by:argmin
Wùë†ùë†‚Üíùë°ùë°|ùëâùëâ |‚àëÔ∏Åùëñùëñ(
Wùë†ùë†‚Üíùë°ùë°xùë†ùë†ùëñùëñ ‚àí xùë°ùë°ùëñùëñ)2 (4)
under the hard condition that Wùë†ùë†‚Üíùë°ùë° needs to be orthogonal, where V is the vocabulary of correct wordtranslations from source to target space. The reason for the orthogonality constraint is that linear transformation with the orthogonal matrix does not squeeze or re-scale the transformed space. It only rotatesthe space, thus it preserves most of the relationships of its elements (in our case, it is important thatorthogonal transformation preserves angles between the words, so it preserves the cosine similarity).
Finally, in all transformation methods, for each word ùë§ùë§ùëñùëñ from the set of target words ùëáùëá , we select itscorresponding vectors vùë†ùë†ùë§ùë§ùëñùëñand vùë°ùë°ùë§ùë§ùëñùëñfrom matrices XÃÇùë†ùë† and XÃÇùë°ùë°, respectively (vùë†ùë†ùë§ùë§ùëñùëñ‚àà XÃÇùë†ùë† and vùë°ùë°ùë§ùë§ùëñùëñ‚àà XÃÇùë°ùë°),and we compute the cosine similarity between these two vectors. The cosine similarity is then used togenerate an output, i.e., the ranking.
5
The source code is available at https://github.com/pauli31/Sem
Eval2020-task1
P≈ôib√°≈à P., Pra≈æ√°k O., Taylor S. E.
Rank C1/
C2 C2/
C3 ùê∂ùê∂1/ùê∂ùê∂3 Avg. Description27 .367 .354 .533 .417 cca-150-O
T
49 .277 .273 .464 .338 cca-100to220-agg
50 .239 .307 .450 .332 ort-200-O
T
57 .220 .255 .446 .307 cca-100to500-agg
71 .096 .155 .317 .190 cca-300-pre-/post-trained
Aggregating runs (submissions ranked 49 and 57) were performed by giving each test word the average of its rank in the individual runs. Our best aggregating run is cca-100to220-agg, aggregating runsfrom 29 embeddings with vector lengths from 100 to 220. Cca-100to500-agg aggregates runs on 119embeddings with vector lengths from 100 to 500. It has a smaller Standard Error of the Mean, but worsescores.
Pre-training (submission ranked 71 in Table 2) was performed for one epoch on the entire vocabularyfor all corpora, and then for five epochs in one corpus. The pre-training was supposed to start all threeembeddings from the same random beginning. For training our embeddings, including pre-training weused the gensim module, which re-implements both the word2vec Skip-gram and the fast
Texttraining algorithms. Both aggregation and pre-training submissions use the CC
A transformation.
For the experiments with the orthogonal transformation, we use the Vec
Map tool . We use embeddings with the size of 200 and 300. We performed two versions of the experiments, the first withlemmatized text and the other with only the target word lemmatized and the rest of the text only lowercased (O
T in the name of the submissions).
The cca-150-O
T submission used the CC
A transformation with fast
Text embeddings with dimension150. We build the translation dictionary for the transformation of the two spaces by removing the target
words from the intersection of their vocabularies (we use all other words from the intersection). Theembeddings are trained on the same preprocessed text as the orthogonal submission.
4.1 Results
We made ten submissions. The most interesting submissions are shown in table 2, which is excerptedfrom the full leaderboard. Our team ranked near the median, with absolute correlations less than weexpected based on our scores with RuSem
Shift data.
As described above, we experimented with several different details within the broad strategy of aligning embeddings. Although single runs on each sub-strategy are not necessarily statistically significant,table 2 shows that the O
T (only targets lemmatized) sub-strategy and CC
A transformations were relatively successful, and that embeddings with more than 220-dimensional vectors were less successful,perhaps because more training should have been used for the extra parameters.
5 Conclusion
In the two earlier recent semantic change workshops , strategies like ours did very well .
In contrast, at RuShift
Eval, other strategies (probably supervised) have dramatically taken over the topspots. You will have to read those papers to find out how they did it. However, we can speculate on somedifferences peculiar to this situation.
Our approach is unsupervised. We did a couple of sanity runs using the RuSem
Shift data , butmade no effort to exploit it. So systems which found ways to incorporate supervision should do better.
It is possible that the human annotation of the gold data is capturing only a part of the total semanticchange in test words, and thus supervised systems which attempt to match those annotations fare betterthan those like ours which look at global usage. Against this theory, the annotation technique appears tobe very similar to that of Schlechtweg et al. , used in the other recent workshops.
None of the languages for the other workshops in Table 1 is from the Slavic language group. Slaviclanguages have more cases for nouns, and perhaps more tokens/lemma than other languages. They haveUWB@RuShift
Eval Measuring Semantic Difference as per-word Variation in Aligned Semantic Spacesa generally freer word-order, which may have an effect on a five-word context window. Latin has someof the same problems as the Slavic languages, and Latin results for the Sem
Eval task were generallymuch worse ‚Äì but the small corpora sizes for Latin could have aggravated those problems (see Table 1),and were certainly easy to blame. In contrast, the RuShift
Eval corpora are completely adequate.
Further, because it is easier to get a good Spearman correlation for a shorter sequence, our ‚Äôsanity‚Äôruns on the RuSem
Shift data may have misled us about our likely scores on the final evaluation. Theexpected absolute value of the Spearman correlation of two lists of random numbers is 0.27 for lists oflength 10, 0.11 for lists of length 50, and 0.08 for lists of length 100. The shorter lists also have largerstandard deviation. The lists of test words for previous workshops ranged from 18 to 48 words long, andthe two RuSem
Shift lists are 49 and 50 words. So higher scores are easier to achieve for those shorterlists. The highest score of ten submissions is most likely to be 1.3ùúéùúé above the mean. (
Another factoris averaging the three inter-corpora scores. Assuming that the standard deviation of each score is ùë†ùë†, thestandard deviation of the average is ùë†ùë†‚àö3
.) Everyone had this problem; but it may partly account for ourconfusion about how well we expected to do.
Finally, of course, we may have made a silly implementation error. If so, we were in good company.
6 Acknowledgements
This work has been partly supported by ERD
F ‚Äù
Research and Development of Intelligent Componentsof Advanced Technologies for the Pilsen Metropolitan Area (Inte
Com)‚Äù (no.: C
Z.02.1.01/0.0/0.0/17048/0007267); by the project L
O1506 of the Czech Ministry of Education, Youth and Sports; and by
Grant No. SG
S-2019-018 Processing of heterogeneous data and its specialized applications. Access tocomputing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure Meta
Centrum provided under the programme "
Projects of Large Research, Development, and
Innovations Infrastructures" (CESNE
T L
M2015042), is greatly appreciated.
